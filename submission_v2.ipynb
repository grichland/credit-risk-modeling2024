{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport polars as pl\nimport os, gc, warnings\nfrom glob import glob\nfrom pathlib import Path\nfrom typing import Any\n\nwarnings.filterwarnings(\"ignore\")\n\nROOT = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\nTRAIN_DIR = ROOT / \"parquet_files\" / \"train\"\nTEST_DIR = ROOT / \"parquet_files\" / \"test\"","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:41:08.899138Z","iopub.execute_input":"2024-05-15T03:41:08.899796Z","iopub.status.idle":"2024-05-15T03:41:09.998012Z","shell.execute_reply.started":"2024-05-15T03:41:08.899762Z","shell.execute_reply":"2024-05-15T03:41:09.996974Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# preprocessing","metadata":{}},{"cell_type":"code","source":"class Utility:\n    @staticmethod\n    def get_feat_defs(ending_with: str) -> None:\n        \"\"\"\n        Retrieves feature definitions from a CSV file based on the specified ending.\n\n        Args:\n        - ending_with (str): Ending to filter feature definitions.\n\n        Returns:\n        - pl.DataFrame: Filtered feature definitions.\n        \"\"\"\n        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n\n        filtered_feats: pl.DataFrame = feat_defs.filter(\n            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n        )\n\n        with pl.Config(fmt_str_lengths=200, tbl_rows=-1):\n            print(filtered_feats)\n\n        filtered_feats = None\n        feat_defs = None\n\n    @staticmethod\n    def find_index(lst: list[Any], item: Any) -> int | None:\n        \"\"\"\n        Finds the index of an item in a list.\n\n        Args:\n        - lst (list): List to search.\n        - item (Any): Item to find in the list.\n\n        Returns:\n        - int | None: Index of the item if found, otherwise None.\n        \"\"\"\n        try:\n            return lst.index(item)\n        except ValueError:\n            return None\n\n    @staticmethod\n    def dtype_to_str(dtype: pl.DataType) -> str:\n        \"\"\"\n        Converts Polars data type to string representation.\n\n        Args:\n        - dtype (pl.DataType): Polars data type.\n\n        Returns:\n        - str: String representation of the data type.\n        \"\"\"\n        dtype_map = {\n            pl.Decimal: \"Decimal\",\n            pl.Float32: \"Float32\",\n            pl.Float64: \"Float64\",\n            pl.UInt8: \"UInt8\",\n            pl.UInt16: \"UInt16\",\n            pl.UInt32: \"UInt32\",\n            pl.UInt64: \"UInt64\",\n            pl.Int8: \"Int8\",\n            pl.Int16: \"Int16\",\n            pl.Int32: \"Int32\",\n            pl.Int64: \"Int64\",\n            pl.Date: \"Date\",\n            pl.Datetime: \"Datetime\",\n            pl.Duration: \"Duration\",\n            pl.Time: \"Time\",\n            pl.Array: \"Array\",\n            pl.List: \"List\",\n            pl.Struct: \"Struct\",\n            pl.String: \"String\",\n            pl.Categorical: \"Categorical\",\n            pl.Enum: \"Enum\",\n            pl.Utf8: \"Utf8\",\n            pl.Binary: \"Binary\",\n            pl.Boolean: \"Boolean\",\n            pl.Null: \"Null\",\n            pl.Object: \"Object\",\n            pl.Unknown: \"Unknown\",\n        }\n\n        return dtype_map.get(dtype)\n\n    @staticmethod\n    def find_feat_occur(regex_path: str, ending_with: str) -> pl.DataFrame:\n        \"\"\"\n        Finds occurrences of features ending with a specific string in Parquet files.\n\n        Args:\n        - regex_path (str): Regular expression to match Parquet file paths.\n        - ending_with (str): Ending to filter feature names.\n\n        Returns:\n        - pl.DataFrame: DataFrame containing feature definitions, data types, and file locations.\n        \"\"\"\n        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\").filter(\n            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n        )\n        feat_defs.sort(by=[\"Variable\"])\n\n        feats: list[pl.String] = feat_defs[\"Variable\"].to_list()\n        feats.sort()\n\n        occurrences: list[list] = [[set(), set()] for _ in range(feat_defs.height)]\n\n        for path in glob(str(regex_path)):\n            df_schema: dict = pl.read_parquet_schema(path)\n\n            for feat, dtype in df_schema.items():\n                index: int = Utility.find_index(feats, feat)\n                if index != None:\n                    occurrences[index][0].add(Utility.dtype_to_str(dtype))\n                    occurrences[index][1].add(Path(path).stem)\n\n        data_types: list[str] = [None] * feat_defs.height\n        file_locs: list[str] = [None] * feat_defs.height\n\n        for i, feat in enumerate(feats):\n            data_types[i] = list(occurrences[i][0])\n            file_locs[i] = list(occurrences[i][1])\n\n        feat_defs = feat_defs.with_columns(pl.Series(data_types).alias(\"Data_Type(s)\"))\n        feat_defs = feat_defs.with_columns(pl.Series(file_locs).alias(\"File_Loc(s)\"))\n\n        return feat_defs\n\n    def reduce_memory_usage(df: pl.DataFrame, name) -> pl.DataFrame:\n        \"\"\"\n        Reduces memory usage of a DataFrame by converting column types.\n\n        Args:\n        - df (pl.DataFrame): DataFrame to optimize.\n        - name (str): Name of the DataFrame.\n\n        Returns:\n        - pl.DataFrame: Optimized DataFrame.\n        \"\"\"\n        print(\n            f\"Memory usage of dataframe \\\"{name}\\\" is {round(df.estimated_size('mb'), 4)} MB.\"\n        )\n\n        int_types = [\n            pl.Int8,\n            pl.Int16,\n            pl.Int32,\n            pl.Int64,\n            pl.UInt8,\n            pl.UInt16,\n            pl.UInt32,\n            pl.UInt64,\n        ]\n        float_types = [pl.Float32, pl.Float64]\n\n        for col in df.columns:\n            col_type = df[col].dtype\n            if col_type in int_types + float_types:\n                c_min = df[col].min()\n                c_max = df[col].max()\n\n                if c_min is not None and c_max is not None:\n                    if col_type in int_types:\n                        if c_min >= 0:\n                            if (\n                                c_min >= np.iinfo(np.uint8).min\n                                and c_max <= np.iinfo(np.uint8).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.UInt8))\n                            elif (\n                                c_min >= np.iinfo(np.uint16).min\n                                and c_max <= np.iinfo(np.uint16).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.UInt16))\n                            elif (\n                                c_min >= np.iinfo(np.uint32).min\n                                and c_max <= np.iinfo(np.uint32).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.UInt32))\n                            elif (\n                                c_min >= np.iinfo(np.uint64).min\n                                and c_max <= np.iinfo(np.uint64).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.UInt64))\n                        else:\n                            if (\n                                c_min >= np.iinfo(np.int8).min\n                                and c_max <= np.iinfo(np.int8).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.Int8))\n                            elif (\n                                c_min >= np.iinfo(np.int16).min\n                                and c_max <= np.iinfo(np.int16).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.Int16))\n                            elif (\n                                c_min >= np.iinfo(np.int32).min\n                                and c_max <= np.iinfo(np.int32).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.Int32))\n                            elif (\n                                c_min >= np.iinfo(np.int64).min\n                                and c_max <= np.iinfo(np.int64).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.Int64))\n                    elif col_type in float_types:\n                        if (\n                            c_min > np.finfo(np.float32).min\n                            and c_max < np.finfo(np.float32).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Float32))\n\n        print(\n            f\"Memory usage of dataframe \\\"{name}\\\" became {round(df.estimated_size('mb'), 4)} MB.\"\n        )\n\n        return df\n\n    def to_pandas(df: pl.DataFrame, cat_cols: list[str] = None) -> (pd.DataFrame, list[str]):  # type: ignore\n        \"\"\"\n        Converts a Polars DataFrame to a Pandas DataFrame.\n\n        Args:\n        - df (pl.DataFrame): Polars DataFrame to convert.\n        - cat_cols (list[str]): List of categorical columns. Default is None.\n\n        Returns:\n        - (pd.DataFrame, list[str]): Tuple containing the converted Pandas DataFrame and categorical columns.\n        \"\"\"\n        df: pd.DataFrame = df.to_pandas()\n\n        if cat_cols is None:\n            cat_cols = list(df.select_dtypes(\"object\").columns)\n\n        df[cat_cols] = df[cat_cols].astype(\"category\")\n\n        return df, cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:41:10.000226Z","iopub.execute_input":"2024-05-15T03:41:10.000756Z","iopub.status.idle":"2024-05-15T03:41:10.036822Z","shell.execute_reply.started":"2024-05-15T03:41:10.000720Z","shell.execute_reply":"2024-05-15T03:41:10.035730Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Aggregator:\n    @staticmethod\n    def max_expr(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Generates expressions for calculating maximum values for specific columns.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of expressions for maximum values.\n        \"\"\"\n        cols: list[str] = [\n            col\n            for col in df.columns\n            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n        ]\n\n        expr_max: list[pl.Series] = [\n            pl.col(col).max().alias(f\"{col}_MAX\") for col in cols\n        ]\n\n        return expr_max\n\n    @staticmethod\n    def min_expr(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Generates expressions for calculating minimum values for specific columns.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of expressions for minimum values.\n        \"\"\"\n        cols: list[str] = [\n            col\n            for col in df.columns\n            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n        ]\n\n        expr_min: list[pl.Series] = [\n            pl.col(col).min().alias(f\"{col}_MIN\") for col in cols\n        ]\n\n        return expr_min\n\n    @staticmethod\n    def mean_expr(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Generates expressions for calculating mean values for specific columns.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of expressions for mean values.\n        \"\"\"\n        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n\n        expr_mean: list[pl.Series] = [\n            pl.col(col).mean().alias(f\"{col}_MEAN\") for col in cols\n        ]\n\n        return expr_mean\n\n    @staticmethod\n    def var_expr(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Generates expressions for calculating variance for specific columns.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of expressions for variance.\n        \"\"\"\n        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n\n        expr_mean: list[pl.Series] = [\n            pl.col(col).var().alias(f\"{col}_VAR\") for col in cols\n        ]\n\n        return expr_mean\n\n    @staticmethod\n    def mode_expr(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Generates expressions for calculating mode values for specific columns.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of expressions for mode values.\n        \"\"\"\n        cols: list[str] = [col for col in df.columns if col.endswith(\"M\")]\n\n        expr_mode: list[pl.Series] = [\n            pl.col(col).drop_nulls().mode().first().alias(f\"{col}_MODE\") for col in cols\n        ]\n\n        return expr_mode\n\n    @staticmethod\n    def get_exprs(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Combines expressions for maximum, mean, and variance calculations.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of combined expressions.\n        \"\"\"\n        exprs = (\n            Aggregator.max_expr(df) + Aggregator.mean_expr(df) + Aggregator.var_expr(df)\n        )\n\n        return exprs","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:41:10.037766Z","iopub.execute_input":"2024-05-15T03:41:10.038028Z","iopub.status.idle":"2024-05-15T03:41:10.056543Z","shell.execute_reply.started":"2024-05-15T03:41:10.038006Z","shell.execute_reply":"2024-05-15T03:41:10.055237Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class SchemaGen:\n    @staticmethod\n    def change_dtypes(df: pl.LazyFrame) -> pl.LazyFrame:\n        \"\"\"\n        Changes the data types of columns in the DataFrame.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - pl.LazyFrame: LazyFrame with modified data types.\n        \"\"\"\n        for col in df.columns:\n            if col == \"case_id\":\n                df = df.with_columns(pl.col(col).cast(pl.UInt32).alias(col))\n            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.UInt16).alias(col))\n            elif col == \"date_decision\" or col[-1] == \"D\":\n                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n            elif col[-1] in [\"P\", \"A\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n        return df\n\n    @staticmethod\n    def scan_files(glob_path: str, depth: int = None):\n        chunks = []\n        for path in glob(str(glob_path)):\n            df = pl.read_parquet(path, low_memory=True, rechunk=True)\n            df = df.pipe(SchemaGen.change_dtypes)\n            if depth in [1, 2]:\n                df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n            chunks.append(df)\n        df = pl.concat(chunks, how=\"vertical_relaxed\")\n        del chunks\n        gc.collect()\n\n        df = df.unique(subset=[\"case_id\"]) \n        \n        return df\n\n    @staticmethod\n    def join_dataframes(df_base, depth_0, depth_1, depth_2):\n        for i, df in enumerate(depth_0 + depth_1 + depth_2):\n            df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n        return df_base\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:41:10.059530Z","iopub.execute_input":"2024-05-15T03:41:10.060051Z","iopub.status.idle":"2024-05-15T03:41:10.072356Z","shell.execute_reply.started":"2024-05-15T03:41:10.060007Z","shell.execute_reply":"2024-05-15T03:41:10.071632Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def filter_cols(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Filters columns in the DataFrame based on null percentage and unique values for string columns.\n\n    Args:\n    - df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n    - pl.DataFrame: DataFrame with filtered columns.\n    \"\"\"\n    for col in df.columns:\n        if col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]:\n            null_pct = df[col].is_null().mean()\n\n            if null_pct > 0.95:\n                df = df.drop(col)\n\n    for col in df.columns:\n        if (col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]) & (\n            df[col].dtype == pl.String\n        ):\n            freq = df[col].n_unique()\n\n            if (freq > 200) | (freq == 1):\n                df = df.drop(col)\n\n    return df\n\n\ndef transform_cols(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Transforms columns in the DataFrame according to predefined rules.\n\n    Args:\n    - df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n    - pl.DataFrame: DataFrame with transformed columns.\n    \"\"\"\n    if \"riskassesment_302T\" in df.columns:\n        if df[\"riskassesment_302T\"].dtype == pl.Null:\n            df = df.with_columns(\n                [\n                    pl.Series(\n                        \"riskassesment_302T_rng\", df[\"riskassesment_302T\"], pl.UInt8\n                    ),\n                    pl.Series(\n                        \"riskassesment_302T_mean\", df[\"riskassesment_302T\"], pl.UInt8\n                    ),\n                ]\n            )\n        else:\n            pct_low: pl.Series = (\n                df[\"riskassesment_302T\"]\n                .str.split(\" - \")\n                .apply(lambda x: x[0].replace(\"%\", \"\"))\n                .cast(pl.UInt8)\n            )\n            pct_high: pl.Series = (\n                df[\"riskassesment_302T\"]\n                .str.split(\" - \")\n                .apply(lambda x: x[1].replace(\"%\", \"\"))\n                .cast(pl.UInt8)\n            )\n\n            diff: pl.Series = pct_high - pct_low\n            avg: pl.Series = ((pct_low + pct_high) / 2).cast(pl.Float32)\n\n            del pct_high, pct_low\n            gc.collect()\n\n            df = df.with_columns(\n                [\n                    diff.alias(\"riskassesment_302T_rng\"),\n                    avg.alias(\"riskassesment_302T_mean\"),\n                ]\n            )\n\n        df.drop(\"riskassesment_302T\")\n\n    return df\n\n\ndef handle_dates(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Handles date columns in the DataFrame.\n\n    Args:\n    - df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n    - pl.DataFrame: DataFrame with transformed date columns.\n    \"\"\"\n    for col in df.columns:\n        if (col[-1] == 'D') or ('D_' in col):\n            df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n            df = df.with_columns(pl.col(col).dt.total_days().cast(pl.Int32))\n\n    df = df.rename(\n        {\n            \"MONTH\": \"month\",\n            \"WEEK_NUM\": \"week_num\"\n        }\n    )\n            \n    df = df.with_columns(\n        [\n            pl.col(\"date_decision\").dt.year().alias(\"year\").cast(pl.Int16),\n            pl.col(\"date_decision\").dt.day().alias(\"day\").cast(pl.UInt8),\n        ]\n    )\n\n    return df.drop(\"date_decision\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:41:10.073499Z","iopub.execute_input":"2024-05-15T03:41:10.074159Z","iopub.status.idle":"2024-05-15T03:41:10.090839Z","shell.execute_reply.started":"2024-05-15T03:41:10.074131Z","shell.execute_reply":"2024-05-15T03:41:10.090164Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data_store: dict = {\n    \"df_base\": SchemaGen.scan_files(TRAIN_DIR / \"train_base.parquet\"),\n    \"depth_0\": [\n        SchemaGen.scan_files(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        SchemaGen.scan_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n    ],\n}\n\ndf_train: pl.DataFrame = (\n    SchemaGen.join_dataframes(**data_store)\n    .pipe(filter_cols)\n    .pipe(transform_cols)\n    .pipe(handle_dates)\n    .pipe(Utility.reduce_memory_usage, \"df_train\")\n)\n\ndel data_store\ngc.collect()\n\nprint(f\"Train data shape: {df_train.shape}\")\ndisplay(df_train.head(10))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T04:11:44.386897Z","iopub.execute_input":"2024-05-15T04:11:44.387310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_store: dict = {\n    \"df_base\": SchemaGen.scan_files(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        SchemaGen.scan_files(TEST_DIR / \"test_static_cb_0.parquet\"),\n        SchemaGen.scan_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        SchemaGen.scan_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_other_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_person_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n    ],\n}\n\ndf_test: pl.DataFrame = (\n    SchemaGen.join_dataframes(**data_store)\n    .pipe(transform_cols)\n    .pipe(handle_dates)\n    .select([col for col in df_train.columns if col != \"target\"])\n    .pipe(Utility.reduce_memory_usage, \"df_test\")\n)\n\ndel data_store\ngc.collect()\n\nprint(f\"Test data shape: {df_test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 'target' not in df_test.columns:\n    df_test = df_test.with_columns(pl.lit(0).alias('target').cast(pl.Int8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, cat_cols = Utility.to_pandas(\n                        pl.concat([\n                                 df_train.with_columns(pl.lit('train').alias('partition')),\n                                 df_test.select(df_train.columns).with_columns(pl.lit('test').alias('partition'))\n                                    ],how='vertical_relaxed')\n                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df[df['partition']=='train'].reset_index(drop=True)\ndf_test  = df[df['partition']=='test'].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = df_train.columns[5:-3]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:44:32.237307Z","iopub.execute_input":"2024-05-15T03:44:32.237605Z","iopub.status.idle":"2024-05-15T03:44:32.245675Z","shell.execute_reply.started":"2024-05-15T03:44:32.237580Z","shell.execute_reply":"2024-05-15T03:44:32.244685Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# exploration","metadata":{}},{"cell_type":"code","source":"# placeholder for exploration code ","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:44:32.246922Z","iopub.execute_input":"2024-05-15T03:44:32.247243Z","iopub.status.idle":"2024-05-15T03:44:32.254673Z","shell.execute_reply.started":"2024-05-15T03:44:32.247211Z","shell.execute_reply":"2024-05-15T03:44:32.253640Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# training","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split,StratifiedGroupKFold\nimport lightgbm as lgb \nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\nfrom hyperopt.pyll import scope\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:52:04.988218Z","iopub.execute_input":"2024-05-15T03:52:04.988791Z","iopub.status.idle":"2024-05-15T03:52:04.995746Z","shell.execute_reply.started":"2024-05-15T03:52:04.988754Z","shell.execute_reply":"2024-05-15T03:52:04.994664Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:52:05.286123Z","iopub.execute_input":"2024-05-15T03:52:05.286604Z","iopub.status.idle":"2024-05-15T03:52:05.297514Z","shell.execute_reply.started":"2024-05-15T03:52:05.286569Z","shell.execute_reply":"2024-05-15T03:52:05.296380Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def get_base_params():\n    base_params = {\n        'boosting_type':'gbdt',\n        'random_state': 117,\n        'objective': 'binary',\n        'metric': 'auc',\n        'extra_trees':True,\n        'verbose': -1,\n        'max_bin': 64,\n#         'device_type': 'gpu'\n        \n    }\n    return base_params","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:52:05.502546Z","iopub.execute_input":"2024-05-15T03:52:05.504946Z","iopub.status.idle":"2024-05-15T03:52:05.510164Z","shell.execute_reply.started":"2024-05-15T03:52:05.504910Z","shell.execute_reply":"2024-05-15T03:52:05.509139Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# set up search space\nsearch_space_setup = {\n    'feature_fraction': hp.uniform('colsample_bynode', 0.3, 0.8),\n    'max_depth': scope.int(hp.uniform('max_depth', 5, 20)),\n    'l1_regularization': hp.loguniform('l1_regularization', np.log(.001), np.log(100)),\n    'l2_regularization':hp.loguniform('l2_regularization',np.log(.001), np.log(100)),\n    'cat_l2': hp.loguniform('cat_l2', np.log(.001), np.log(100)),\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.3, 0.8),\n    'bagging_freq': scope.int(hp.uniform('bagging_freq', 0, 5)),\n    'learning_rate' : hp.loguniform('learning_rate', np.log(0.01), np.log(.5)),\n    'n_estimators':scope.int(hp.uniform('n_estimators', 500, 1500)),\n    'num_leaves': scope.int(hp.uniform('num_leaves', 50, 250)),\n}\nsearch_space = get_base_params()\nfor k,v in search_space_setup.items():\n    search_space[k] = v\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:52:06.511823Z","iopub.execute_input":"2024-05-15T03:52:06.512635Z","iopub.status.idle":"2024-05-15T03:52:06.522126Z","shell.execute_reply.started":"2024-05-15T03:52:06.512582Z","shell.execute_reply":"2024-05-15T03:52:06.521185Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def trial_fn(params,\n             splits = None,\n             dataset = None):\n    \n    num_boost_round = params.pop('n_estimators')\n    cv_results = lgb.cv(\n        params,\n        dataset,\n        num_boost_round=50,\n        folds=splits,\n        seed = 117\n    ) \n    \n    score = cv_results['valid auc-mean'][-1] \n    return {\"status\": STATUS_OK, \"loss\": -score} # always minimizes","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:44:35.923865Z","iopub.execute_input":"2024-05-15T03:44:35.924364Z","iopub.status.idle":"2024-05-15T03:44:35.933677Z","shell.execute_reply.started":"2024-05-15T03:44:35.924326Z","shell.execute_reply":"2024-05-15T03:44:35.932675Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"lgbtrain = lgb.Dataset(df_train[features], label=df_train['target'])\ntest_X   = df_test[features].copy()\nsubmission = df_test[['case_id']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_test\ndel df_train\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do splits ahead of time to improve trial speed\n# k = 5\n# splits   = [(train_idx,valid_idx) for train_idx,valid_idx in \n#           StratifiedGroupKFold(n_splits=k).split(np.arange(df_train.shape[0]),\n#                                                  df_train['target'],\n#                                                  groups = df_train['week_num'])]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_params = fmin(fn=partial(trial_fn, splits = splits, dataset = lgbtrain),\n#                     space=search_space,\n#                     algo=tpe.suggest,\n#                     max_evals=10,\n#                     timeout=60*60*2 # seconds\n#                   )\n# int_params = ['max_depth','n_estimators','bagging_freq','num_leaves']\n# bestp = get_base_params()\n# for k,v in best_params.items():\n#     if k in int_params:\n#         bestp[k] = int(v)\n#     else:\n#         bestp[k] = v\n# bestp","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:55:07.834909Z","iopub.execute_input":"2024-05-15T03:55:07.835360Z","iopub.status.idle":"2024-05-15T03:55:07.839883Z","shell.execute_reply.started":"2024-05-15T03:55:07.835331Z","shell.execute_reply":"2024-05-15T03:55:07.839213Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"bestp = {\n         'boosting_type': 'gbdt',\n#          'device_type': 'gpu',\n         'random_state': 117,\n         'objective': 'binary',\n         'metric': 'auc',\n         'extra_trees': True,\n         'verbose': -1,\n         'max_bin': 64,\n         'bagging_fraction': 0.6615111203742043,\n         'bagging_freq': 4,\n         'cat_l2': 0.4303012850161522,\n         'colsample_bynode': 0.30799275380454566,\n         'l1_regularization': 0.09818609605701412,\n         'l2_regularization': 45.88388390697673,\n         'learning_rate': 0.06583892942324936,\n         'max_depth': 15,\n         'n_estimators': 849,\n         'num_leaves': 100,\n         'verbose': 1\n        }","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:55:10.588324Z","iopub.execute_input":"2024-05-15T03:55:10.589180Z","iopub.status.idle":"2024-05-15T03:55:10.595182Z","shell.execute_reply.started":"2024-05-15T03:55:10.589146Z","shell.execute_reply":"2024-05-15T03:55:10.594020Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"gbm = lgb.train(\n    bestp,\n    lgbtrain,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:55:13.228522Z","iopub.execute_input":"2024-05-15T03:55:13.229737Z","iopub.status.idle":"2024-05-15T04:10:28.251407Z","shell.execute_reply.started":"2024-05-15T03:55:13.229690Z","shell.execute_reply":"2024-05-15T04:10:28.250234Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 47994, number of negative: 1478675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.155825 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 21384\n[LightGBM] [Info] Number of data points in the train set: 1526669, number of used features: 463\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031437 -> initscore=-3.427826\n[LightGBM] [Info] Start training from score -3.427826\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# submission","metadata":{}},{"cell_type":"code","source":"submission['score'] = gbm.predict(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}