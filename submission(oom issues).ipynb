{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport polars as pl\nimport os, gc\nfrom sklearn.preprocessing import LabelEncoder\n\n\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 1000)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:54:39.730407Z","iopub.execute_input":"2024-05-14T03:54:39.730785Z","iopub.status.idle":"2024-05-14T03:54:42.784625Z","shell.execute_reply.started":"2024-05-14T03:54:39.730755Z","shell.execute_reply":"2024-05-14T03:54:42.783333Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def calculate_woe_iv_categorical(feature, response):\n    # Calculate the total number of events (positive responses) and non-events (negative responses)\n    total_events = response.sum()\n    total_non_events = response.count() - total_events\n    \n    # Create a new DataFrame with the feature and response values\n    df = pd.DataFrame({'bin': feature.fillna('missing'), 'response': response})\n    \n    # Calculate the percentage of events and non-events for each bin of the feature\n    bin_summary = df.groupby('bin')['response'].agg(['sum', 'count']).reset_index()\n    bin_summary.columns = ['bin', 'events', 'total']\n    bin_summary['non-events'] = (bin_summary['total'] - bin_summary['events']) \n    bin_summary['event_rate'] = (bin_summary['events'] / total_events)\n    bin_summary['non-event_rate'] = (bin_summary['non-events'] / total_non_events) + 1e-10 # epsilon so that that the non event rate is not 0\n\n    # Calculate the Weight of Evidence (WOE) and Information Value (IV) for each bin\n    bin_summary['WOE'] = np.log1p(bin_summary['event_rate'] / bin_summary['non-event_rate'])\n    bin_summary['IV'] = (bin_summary['event_rate'] - bin_summary['non-event_rate']) * bin_summary['WOE']\n\n    # # Calculate the total Information Value (IV) for the feature\n    total_IV = bin_summary['IV'].sum()\n    \n    return total_IV\n\ndef calculate_woe_iv_numeric(feature, response,quantiles = 25):\n    # Calculate the total number of events (positive responses) and non-events (negative responses)\n    total_events = response.sum()\n    total_non_events = response.count() - total_events\n    \n    # Create a new DataFrame with the feature and response values\n    df = pd.DataFrame({'feature': feature, 'response': response})\n    \n    # we want to support missing values\n    df['bin'] = -1\n    df.loc[df['feature'].notnull(),'bin'] = pd.qcut(df.loc[df['feature'].notnull(),'feature'], q=quantiles,duplicates='drop',labels=False)\n\n    del df['feature']\n    # Calculate the percentage of events and non-events for each bin of the feature\n    bin_summary = df.groupby('bin')['response'].agg(['sum', 'count']).reset_index()\n    bin_summary.columns = ['bin', 'events', 'total']\n    bin_summary['non-events'] = (bin_summary['total'] - bin_summary['events']) \n    bin_summary['event_rate'] = (bin_summary['events'] / total_events)\n    bin_summary['non-event_rate'] = (bin_summary['non-events'] / total_non_events) + 1e-10 # epsilon so that that the non event rate is not 0\n\n    # Calculate the Weight of Evidence (WOE) and Information Value (IV) for each bin\n    bin_summary['WOE'] = np.log1p(bin_summary['event_rate'] / bin_summary['non-event_rate'])\n    bin_summary['IV'] = (bin_summary['event_rate'] - bin_summary['non-event_rate']) * bin_summary['WOE']\n\n    # # Calculate the total Information Value (IV) for the feature\n    total_IV = bin_summary['IV'].sum()\n    \n    return total_IV\n\ndef calculate_psi_categorical(old,new): \n    old = old.to_frame().fillna('missing')\n    old.columns = ['bin']\n    new = new.to_frame().fillna('missing')\n    new.columns = ['bin']    \n    \n    old = old.groupby('bin').agg(count_old=('bin','count'))\n    new = new.groupby('bin').agg(count_new=('bin','count'))\n    \n    bins = list(set(old['bin'].tolist()+new['bin'].tolist())) \n    bin_summary = pd.DataFrame(bins,columns=['bin'])\n    bin_summary = bin_summary.merge(old[['bin','count_old']],on='bin',how='left')\n    bin_summary = bin_summary.merge(new[['bin','count_new']],on='bin',how='left')\n    bin_summary['prop_old'] = (bin_summary['count_old'].fillna(0) / len(old)) + 1e-10 # epsilon\n    bin_summary['prop_new'] = (bin_summary['count_new'].fillna(0) / len(new)) + 1e-10 # epsilon\n\n    return np.sum((bin_summary['prop_old'] - bin_summary['prop_new']) * np.log(bin_summary['prop_old']/bin_summary['prop_new']))\n\ndef calculate_psi_numeric(old,new,q=10): \n\n    old = old.to_frame()\n    old.columns = ['val']\n    new = new.to_frame()\n    new.columns = ['val']\n    \n    # set up initial bins for missing values\n    old['bin'] = -1\n    new['bin'] = -1\n    \n    \n    # return 0 in the event that theres less than 3 unique bin across both\n    if (old['val'].fillna(-9999).nunique() + new['val'].fillna(-9999).nunique()) <= 2:\n        return np.nan\n    else: \n        # assign each value to a quantile \n        old.loc[old['val'].notnull(),'bin'] = pd.qcut(old.loc[old['val'].notnull(),'val'], q=q,duplicates='drop',labels=False)\n        new.loc[new['val'].notnull(),'bin'] = pd.qcut(new.loc[new['val'].notnull(),'val'], q=q,duplicates='drop',labels=False)\n        \n        old = old.groupby('bin').agg(count_old=('bin','count'))\n        new = new.groupby('bin').agg(count_new=('bin','count'))\n        \n        \n        bins = list(set(old['bin'].tolist()+new['bin'].tolist())) \n        bin_summary = pd.DataFrame(bins,columns=['bin'])\n        bin_summary = bin_summary.merge(old[['bin','count_old']],on='bin',how='left')\n        bin_summary = bin_summary.merge(new[['bin','count_new']],on='bin',how='left')\n        bin_summary['prop_old'] = (bin_summary['count_old'].fillna(0) / len(old)) + 1e-10 # epsilon\n        bin_summary['prop_new'] = (bin_summary['count_new'].fillna(0) / len(new)) + 1e-10 # epsilon\n    \n        return np.sum((bin_summary['prop_old'] - bin_summary['prop_new']) * np.log(bin_summary['prop_old']/bin_summary['prop_new']))","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:54:42.787055Z","iopub.execute_input":"2024-05-14T03:54:42.787737Z","iopub.status.idle":"2024-05-14T03:54:42.831256Z","shell.execute_reply.started":"2024-05-14T03:54:42.787692Z","shell.execute_reply":"2024-05-14T03:54:42.829664Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"[Data Info](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data) <br>\n[Discussion on how the data is setup](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/473950) <br>\n[Starter Notebook](https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook)\n* depth=0 - These are static features directly tied to a specific case_id.\n* depth=1 - Each case_id has an associated historical record, indexed by num_group1.\n* depth=2 - Each case_id has an associated historical record, indexed by both num_group1 and num_group2.","metadata":{}},{"cell_type":"code","source":"class Aggregator:\n    def __init__(self,numeric_cols,string_cols,date_cols,criteria):\n        self.numeric_cols = numeric_cols\n        self.string_cols  = string_cols\n        self.date_cols    = date_cols\n        self.criteria = criteria\n        \n    def num_expr(self,col):\n        \n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n        expr_min    = [pl.min(col).alias(f\"{col}_MIN_{self.criteria}\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST_{self.criteria}\")]\n        expr_mean   = [pl.mean(col).alias(f\"{col}_MEAN_{self.criteria}\")]\n        expr_median = [pl.median(col).alias(f\"{col}_MEDIAN_{self.criteria}\")]\n        expr_var    = [pl.var(col).alias(f\"{col}_VAR_{self.criteria}\")]\n\n        return expr_max + expr_last + expr_mean + expr_median + expr_var + expr_min\n\n    def date_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST_{self.criteria}\")]\n        expr_mean   = [pl.mean(col).alias(f\"{col}_MEAN_{self.criteria}\")]\n\n        return expr_max + expr_last + expr_mean \n\n    def str_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST_{self.criteria}\")]\n        return expr_max + expr_last \n\n    def count_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n\n        return expr_max\n\n    def get_exprs(self,df):\n        expr = []\n        new_date_cols = []\n        new_str_cols = []\n        for col in df.columns:\n            if 'num_group' in col:\n                expr.extend(self.count_expr(col))\n            elif col in self.numeric_cols:\n                expr.extend(self.num_expr(col))\n            elif col in self.string_cols:\n                new_str_cols.extend([f\"{col}_MAX_{self.criteria}\",f\"{col}_LAST_{self.criteria}\"])\n                expr.extend(self.str_expr(col))\n            elif col in self.date_cols:\n                new_date_cols.extend([f\"{col}_MAX_{self.criteria}\",f\"{col}_LAST_{self.criteria}\",f\"{col}_MEAN_{self.criteria}\"])\n                expr.extend(self.date_expr(col))\n        \n        return expr, new_date_cols, new_str_cols","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:54:42.833290Z","iopub.execute_input":"2024-05-14T03:54:42.833841Z","iopub.status.idle":"2024-05-14T03:54:42.856942Z","shell.execute_reply.started":"2024-05-14T03:54:42.833799Z","shell.execute_reply":"2024-05-14T03:54:42.855306Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def filter_cols(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Filters columns in the DataFrame based on null percentage and unique values for string columns.\n\n    Args:\n    - df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n    - pl.DataFrame: DataFrame with filtered columns.\n    \"\"\"\n    for col in df.columns:\n        if col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]:\n            null_pct = df[col].is_null().mean()\n\n            if null_pct > 0.97:\n                df = df.drop(col)\n                print(f\"dropped column {col} because too many nulls\")\n    for col in df.columns:\n        if (col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]) & (\n            df[col].dtype == pl.String\n        ):\n            freq = df[col].n_unique()\n\n            if (freq > 200) | (freq == 1):\n                df = df.drop(col)\n                print(f\"dropped column {col} because of category size\")\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:54:42.860180Z","iopub.execute_input":"2024-05-14T03:54:42.861214Z","iopub.status.idle":"2024-05-14T03:54:42.883150Z","shell.execute_reply.started":"2024-05-14T03:54:42.861161Z","shell.execute_reply":"2024-05-14T03:54:42.881405Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def reduce_polars_memory_usage(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Reduces memory usage of a DataFrame by converting column types.\n\n    Args:\n    - df (pl.DataFrame): DataFrame to optimize.\n    - name (str): Name of the DataFrame.\n\n    Returns:\n    - pl.DataFrame: Optimized DataFrame.\n    \"\"\"\n    og_mem = round(df.estimated_size('mb'), 4)\n\n    int_types = [\n        pl.Int8,\n        pl.Int16,\n        pl.Int32,\n        pl.Int64,\n        pl.UInt8,\n        pl.UInt16,\n        pl.UInt32,\n        pl.UInt64,\n    ]\n    float_types = [pl.Float32, pl.Float64]\n\n    for col in df.columns:\n        if col == 'case_id':\n            continue\n        col_type = df[col].dtype\n        if col_type in int_types + float_types:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if c_min is not None and c_max is not None:\n                if col_type in int_types:\n                    if c_min >= 0:\n                        if (\n                            c_min >= np.iinfo(np.uint8).min\n                            and c_max <= np.iinfo(np.uint8).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt8))\n                        elif (\n                            c_min >= np.iinfo(np.uint16).min\n                            and c_max <= np.iinfo(np.uint16).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt16))\n                        elif (\n                            c_min >= np.iinfo(np.uint32).min\n                            and c_max <= np.iinfo(np.uint32).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt32))\n                        elif (\n                            c_min >= np.iinfo(np.uint64).min\n                            and c_max <= np.iinfo(np.uint64).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt64))\n                    else:\n                        if (\n                            c_min >= np.iinfo(np.int8).min\n                            and c_max <= np.iinfo(np.int8).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int8))\n                        elif (\n                            c_min >= np.iinfo(np.int16).min\n                            and c_max <= np.iinfo(np.int16).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int16))\n                        elif (\n                            c_min >= np.iinfo(np.int32).min\n                            and c_max <= np.iinfo(np.int32).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int32))\n                        elif (\n                            c_min >= np.iinfo(np.int64).min\n                            and c_max <= np.iinfo(np.int64).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int64))\n                elif col_type in float_types:\n                    if (\n                        c_min > np.finfo(np.float32).min\n                        and c_max < np.finfo(np.float32).max\n                    ):\n                        df = df.with_columns(df[col].cast(pl.Float32))\n\n    print(\n        f\"Memory of polars dataframe went from {og_mem}MB to {round(df.estimated_size('mb'), 4)}MB.\"\n    )\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:54:42.885320Z","iopub.execute_input":"2024-05-14T03:54:42.885935Z","iopub.status.idle":"2024-05-14T03:54:42.910198Z","shell.execute_reply.started":"2024-05-14T03:54:42.885882Z","shell.execute_reply":"2024-05-14T03:54:42.908854Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def extract_lowercase(s):\n    # Initialize an empty result string\n    result = \"\"\n\n    # Loop through each character in the string\n    for char in s:\n        # Check if the character is lowercase\n        if char.islower() or char == '_' or char.isnumeric():\n            result += char\n        # Break the loop if a non-lowercase character is encountered (if desired)\n        elif result:\n            break\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:54:42.911749Z","iopub.execute_input":"2024-05-14T03:54:42.912107Z","iopub.status.idle":"2024-05-14T03:54:42.930548Z","shell.execute_reply.started":"2024-05-14T03:54:42.912077Z","shell.execute_reply":"2024-05-14T03:54:42.929318Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class DatasetBuilder:\n    \"\"\" This class is used to create the dataset \"\"\"\n    def __init__(self, \n                 n_samples   = None, \n                 parent_path = \"/kaggle/input/home-credit-credit-risk-model-stability\",\n                ):\n        \n\n\n        self.parent_path = parent_path\n        self.n_samples = n_samples\n\n        self.feat_info = pd.read_csv(f\"{parent_path}/feature_definitions.csv\")\n        self.date_cols = []\n        self.string_cols = []\n        \n        self.run()\n\n    def explain_feat(self,feat_name:str):\n        assert feat_name in self.feat_info['Variable'].unique(), \"feature not found in feature info dataframe\"\n        return self.feat_info[self.feat_info['Variable']==feat_name]['Description'].values[0]\n\n    def set_table_dtypes(self,df):\n        for col in df.columns:\n                    \n            if col in [\"case_id\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int32))\n            elif col in  [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"target\"]:\n                df = df.with_columns(pl.col(col).cast(pl.UInt16))            \n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))                \n            elif (col[-1] in (\"M\",)) or (col in self.string_cols):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n                if col not in self.string_cols:\n                    self.string_cols.append(col)\n            elif col[-1] in (\"L\",\"T\"): # we dont know the transform needed, just going to assume its either float and if not, then string\n                try:\n                    df = df.with_columns(pl.col(col).cast(pl.Float64))\n                except:\n                    df = df.with_columns(pl.col(col).cast(pl.String))\n                    if col not in self.string_cols:\n                        self.string_cols.append(col) \n                    continue\n                \n            elif col[-1] in (\"D\",) or (col in self.date_cols):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n                if col not in self.date_cols:\n                    self.date_cols.append(col)\n        return df\n\n    def feature_engineer_dates(self,df,date_cols=None):\n        if date_cols is None:\n            date_cols = self.date_cols\n        if 'date_decision' not in df.columns:\n            df = df.join(self.df[['case_id','date_decision']],on='case_id')\n        for col in date_cols:\n            if col in df.columns:\n                df = df.with_columns((pl.col(\"date_decision\") - pl.col(col)).dt.total_days().alias(f'{col}_DAYS_SINCE'))\n                df = df.drop(col)\n\n        if 'date_decision' in df.columns:\n            df = df.drop('date_decision')\n            \n        return df\n  \n    \n    def create_base_dataset(self):\n        \n        # load in the training dataset \n        if self.n_samples is not None:\n            train = pl.read_parquet(f\"{self.parent_path}/parquet_files/train/train_base.parquet\") \\\n            .sample(n=self.n_samples).with_columns(pl.lit('train').alias('partition'))\n        else:\n            train = pl.read_parquet(f\"{self.parent_path}/parquet_files/train/train_base.parquet\") \\\n            .with_columns(pl.lit('train').alias('partition'))\n        \n        # load in the test dataset\n        test =  pl.read_parquet(f\"{self.parent_path}/parquet_files/test/test_base.parquet\")\\\n                .with_columns(pl.lit(0).alias('target'))\\\n                .with_columns(pl.lit('test').alias('partition'))        \n        \n        \n        \n        \n        \n        # concat train and test\n        self.df = reduce_polars_memory_usage(pl.concat([train,test],how='vertical_relaxed').pipe(self.set_table_dtypes))\n        \n        # get all case_ids\n        self.train_case_ids = train.get_column('case_id').to_list()\n        self.test_case_ids  = test.get_column('case_id').to_list()\n        \n        # store base cols\n        self.base_df_cols = self.df.columns\n        \n        del train\n        del test\n        gc.collect()\n\n    def read_in_files_with_criteria(self, criteria:str):\n        print(f\"processing criteria {criteria}...\")\n        train_df  = pl.concat([pl.scan_parquet(f\"{self.parent_path}/parquet_files/train/{x}\", low_memory=True, rechunk=True)\n                       for x in os.listdir(f\"{self.parent_path}/parquet_files/train\") if (criteria in x)],how='vertical_relaxed')\n        test_df  =  pl.concat([pl.scan_parquet(f\"{self.parent_path}/parquet_files/test/{x}\", low_memory=True, rechunk=True)\n                       for x in os.listdir(f\"{self.parent_path}/parquet_files/test\") if (criteria in x)],how='vertical_relaxed')\n        \n\n        # being in train partition doesnt gaurentee it is in the test partition, so we have to ensure it \n        columns_in_common = list(set(train_df.columns).intersection(set(test_df.columns)))\n\n        df = pl.concat([train_df.select(columns_in_common),\n                         test_df.select(columns_in_common)],how='vertical_relaxed') \n            \n        del train_df\n        del test_df \n        gc.collect()\n        \n        df = df.collect().pipe(self.set_table_dtypes).filter(pl.col('case_id').is_in(self.train_case_ids+self.test_case_ids))\n\n        return df\n        \n    def optimize_polars_df(self,df):\n        return reduce_polars_memory_usage(filter_cols(df))\n       \n        \n\n    def evaluate_features(self,df:pl.DataFrame,\n                          stability_scoring=False):\n        \"\"\"\n        1) calculates weight of evidence * information value for measuring predictive power\n        \n        \"\"\"\n        feats = [x for x in df.columns if x not in self.base_df_cols]\n\n        df = df.filter(pl.col(\"case_id\").is_in(self.train_case_ids))\n        n_row = len(self.df)\n        if 'target' not in df.columns:\n            df = df.join(self.df[['case_id','target']],on='case_id')\n        \n        # predictive power - woe*iv\n        woeivs  = []\n        for col in feats:\n            if df[col].dtype == pl.String:\n                woeiv = calculate_woe_iv_categorical(df[col].to_pandas(), df['target'].to_pandas())\n                woeivs.append(woeiv)\n            else:\n                woeiv = calculate_woe_iv_numeric(df[col].to_pandas(), df['target'].to_pandas())\n                woeivs.append(woeiv)\n        \n\n        feature_scores = pd.DataFrame(feats,columns=['feature'])\n        feature_scores['categorical'] = feature_scores['feature'].isin(self.string_cols)\n        feature_scores['prop_null'] = feature_scores['feature'].apply(lambda feat: df[feat].to_pandas().isna().sum()) / n_row\n        feature_scores['woe_iv'] = woeivs       \n    \n        return feature_scores\n        \n    \n    def select_features(self,df,score=\"woe_iv\",threshold=0.02,dedup_agg=False):\n        feature_scores = self.evaluate_features(df)\n        start_n = len(feature_scores)\n        \n        if dedup_agg:\n            feature_scores['feature_base_col'] = feature_scores['feature'].apply(extract_lowercase)\n            feature_scores = feature_scores.sort_values(['feature_base_col',score],ascending=[True,False]).drop_duplicates(subset=['feature_base_col'])\n        \n        chosen_features = feature_scores[feature_scores[score]>=threshold]['feature'].unique().tolist()\n        print(f\"selected {len(chosen_features)}/{start_n} features for the model dataset\")\n        del feature_scores\n        return chosen_features\n\n    \n    def to_pandas(self,df_data):\n        df_data = df_data.to_pandas()\n        cat_cols = [x for x in df_data.columns if (x in self.string_cols) and (x not in self.base_df_cols)]\n        df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n        df_data = self.reduce_pandas_mem_usage(df_data)\n        return df_data, cat_cols\n\n    def reduce_pandas_mem_usage(self,df):\n        \"\"\" iterate through all the columns of a dataframe and modify the data type\n            to reduce memory usage.        \n        \"\"\"\n        start_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage of pandas dataframe is {:.2f} MB'.format(start_mem))\n\n        for col in [x for x in df.columns if x not in self.base_df_cols]:\n            col_type = df[col].dtype\n            if str(col_type)==\"category\":\n                continue\n                \n            else:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n\n\n        end_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n        return df\n     \n    def process_depth0(self):\n        \"\"\"\n        These files can be used as is except for the dates, so just collect them, do feature engineering on the dates, then \n        throw out the date columns, grab top k features, join back to base\n        \"\"\"\n        depth0_criterias = [\"static_0\",\"static_cb_0\"]\n\n        for criteria in depth0_criterias:\n            df = self.read_in_files_with_criteria(criteria)\n            df = self.optimize_polars_df(df)\n            df = self.feature_engineer_dates(df)\n            depth0_feats = self.select_features(df,score=\"woe_iv\",threshold=0.01)\n            self.df = self.df.join(df[['case_id']+depth0_feats], on='case_id', how='left')   \n        \n        del df\n        gc.collect()\n\n    def process_depth1(self):\n        \"\"\"\n        These files have one group; collect them, auto aggregate, do feature engineering on the dates,\n        throw out the date columns, grab top k features, join back to base\n        \"\"\"\n        depth1_criterias = [\"applprev_1\",\"other_1\",\n                            \"tax_registry_a_1\",\"tax_registry_b_1\",\"tax_registry_c_1\",\n                            \"credit_bureau_a_1\",\"credit_bureau_b_1\",\n                            \"deposit_1\",\"person_1\",\"debitcard_1\"]\n        \n        # all groups\n        for criteria in depth1_criterias:\n            df = self.df[['case_id','target','date_decision']]\n            \n            criteria_df = self.read_in_files_with_criteria(criteria)\n            criteria_df = self.optimize_polars_df(criteria_df)\n            aggr = Aggregator([x for x in criteria_df.columns if x not in self.string_cols+self.date_cols+self.base_df_cols],\n                              self.string_cols,self.date_cols,\n                              f\"{criteria.upper()}_DEPTH1_ALL\")\n            agg_expr, agg_dt_cols, agg_str_cols = aggr.get_exprs(criteria_df)\n \n            criteria_df = criteria_df.group_by(\"case_id\").agg(agg_expr)\n            df = df.join(criteria_df, on=['case_id'], how='left')\n            df = self.feature_engineer_dates(df,date_cols=agg_dt_cols)    \n\n            feats = self.select_features(df,score=\"woe_iv\",dedup_agg=True,threshold=0.01)\n            \n            if len(feats)>0:\n                self.string_cols.extend([x for x in agg_str_cols if x in feats])\n                self.df = self.df.join(df[['case_id']+feats], on='case_id', how='left') \n \n            del criteria_df\n            del df\n            gc.collect()\n\n    def process_depth2(self):\n        \"\"\"\n        For now, just approach it like depth 2\n        \"\"\"\n        depth2_criterias = [\"applprev_2\",\"person_2\"] # \"credit_bureau_b_2\",\"credit_bureau_a_2\",\n        \n\n        for criteria in depth2_criterias:\n            df = self.df[['case_id','target','date_decision']]\n            # all groups\n            criteria_df = self.read_in_files_with_criteria(criteria)\n            criteria_df = self.optimize_polars_df(criteria_df)\n            aggr = Aggregator([x for x in criteria_df.columns if x not in self.string_cols+self.date_cols+self.base_df_cols],\n                              self.string_cols,self.date_cols,\n                              f\"{criteria.upper()}_DEPTH2_ALL\")\n            agg_expr, agg_dt_cols, agg_str_cols = aggr.get_exprs(criteria_df)\n \n            criteria_df = criteria_df.group_by(\"case_id\").agg(agg_expr)\n            df = df.join(criteria_df, on=['case_id'], how='left')\n            df = self.feature_engineer_dates(df,date_cols=agg_dt_cols)    \n            feats = self.select_features(df,score=\"woe_iv\",dedup_agg=True)\n            if len(feats)>0:\n                self.string_cols.extend([x for x in agg_str_cols if x in feats])\n                self.df = self.df.join(df[['case_id']+feats], on='case_id', how='left') \n \n            del criteria_df\n            del df\n            gc.collect()            \n            \n\n    def run(self):\n        self.create_base_dataset()\n        self.process_depth0()\n        self.process_depth1()\n        self.process_depth2()        \n        \n    def get_datasets(self):\n        df,cat_cols = self.to_pandas(self.df)\n\n        del self.df\n        gc.collect()\n        \n        return {\"train\":df[df['partition']=='train'].reset_index(drop=True), \n                \"test\": df[df['partition']=='test'].reset_index(drop=True), \n                \"features\": [x for x in df.columns if x not in self.base_df_cols],\n                \"cat_features\": cat_cols}\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:54:42.936126Z","iopub.execute_input":"2024-05-14T03:54:42.936703Z","iopub.status.idle":"2024-05-14T03:54:43.020664Z","shell.execute_reply.started":"2024-05-14T03:54:42.936641Z","shell.execute_reply":"2024-05-14T03:54:43.018710Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"ds = DatasetBuilder().get_datasets()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:54:43.022341Z","iopub.execute_input":"2024-05-14T03:54:43.023356Z","iopub.status.idle":"2024-05-14T04:04:01.500001Z","shell.execute_reply.started":"2024-05-14T03:54:43.023317Z","shell.execute_reply":"2024-05-14T04:04:01.498478Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Memory of polars dataframe went from 36.3986MB to 27.6629MB.\nprocessing criteria static_0...\ndropped column lastrepayingdate_696D because too many nulls\ndropped column clientscnt_136L because too many nulls\ndropped column isbidproductrequest_292L because too many nulls\ndropped column equalityempfrom_62L because too many nulls\ndropped column lastdependentsnum_448L because too many nulls\ndropped column payvacationpostpone_4187118D because too many nulls\ndropped column lastotherinc_902A because too many nulls\ndropped column interestrategrace_34L because too many nulls\ndropped column lastotherlnsexpense_631A because too many nulls\ndropped column lastapprcommoditytypec_5251766M because of category size\ndropped column previouscontdistrict_112M because of category size\nMemory of polars dataframe went from 1689.7873MB to 950.1673MB.\nselected 83/156 features for the model dataset\nprocessing criteria static_cb_0...\ndropped column for3years_128L because too many nulls\ndropped column formonth_118L because too many nulls\ndropped column foryear_818L because too many nulls\ndropped column foryear_618L because too many nulls\ndropped column fortoday_1092L because too many nulls\ndropped column forquarter_462L because too many nulls\ndropped column forweek_601L because too many nulls\ndropped column dateofbirth_342D because too many nulls\ndropped column forweek_528L because too many nulls\ndropped column formonth_535L because too many nulls\ndropped column formonth_206L because too many nulls\ndropped column for3years_504L because too many nulls\ndropped column foryear_850L because too many nulls\ndropped column forweek_1077L because too many nulls\ndropped column forquarter_634L because too many nulls\ndropped column for3years_584L because too many nulls\ndropped column forquarter_1017L because too many nulls\nMemory of polars dataframe went from 349.9409MB to 235.4629MB.\nselected 24/35 features for the model dataset\nprocessing criteria applprev_1...\ndropped column district_544M because of category size\ndropped column profession_152M because of category size\nMemory of polars dataframe went from 1580.954MB to 1076.8361MB.\nselected 31/162 features for the model dataset\nprocessing criteria other_1...\nMemory of polars dataframe went from 2.2422MB to 1.2186MB.\nselected 0/31 features for the model dataset\nprocessing criteria tax_registry_a_1...\ndropped column name_4527232M because of category size\nMemory of polars dataframe went from 56.2323MB to 40.6122MB.\nselected 3/10 features for the model dataset\nprocessing criteria tax_registry_b_1...\ndropped column name_4917606M because of category size\nMemory of polars dataframe went from 19.0191MB to 13.736MB.\nselected 0/10 features for the model dataset\nprocessing criteria tax_registry_c_1...\ndropped column employername_160M because of category size\nMemory of polars dataframe went from 57.4001MB to 41.4557MB.\nselected 2/10 features for the model dataset\nprocessing criteria credit_bureau_a_1...\ndropped column annualeffectiverate_63L because too many nulls\ndropped column contractsum_5085717L because too many nulls\ndropped column prolongationcount_599L because too many nulls\ndropped column interestrate_508L because too many nulls\ndropped column classificationofcontr_400M because of category size\ndropped column financialinstitution_382M because of category size\ndropped column contractst_964M because of category size\ndropped column financialinstitution_591M because of category size\nMemory of polars dataframe went from 7870.922MB to 4830.4982MB.\nselected 45/351 features for the model dataset\nprocessing criteria credit_bureau_b_1...\ndropped column periodicityofpmts_997L because too many nulls\nMemory of polars dataframe went from 27.3313MB to 17.4315MB.\nselected 29/208 features for the model dataset\nprocessing criteria deposit_1...\nMemory of polars dataframe went from 3.0613MB to 2.3695MB.\nselected 0/13 features for the model dataset\nprocessing criteria person_1...\ndropped column role_993L because too many nulls\ndropped column childnum_185L because too many nulls\ndropped column gender_992L because too many nulls\ndropped column maritalst_703L because too many nulls\ndropped column birthdate_87D because too many nulls\ndropped column housingtype_772L because too many nulls\ndropped column isreference_387L because too many nulls\ndropped column contaddr_zipcode_807M because of category size\ndropped column registaddr_district_1083M because of category size\ndropped column contaddr_district_15M because of category size\ndropped column empladdr_district_926M because of category size\ndropped column registaddr_zipcode_184M because of category size\ndropped column empladdr_zipcode_114M because of category size\nMemory of polars dataframe went from 360.0842MB to 266.4887MB.\nselected 13/79 features for the model dataset\nprocessing criteria debitcard_1...\nMemory of polars dataframe went from 5.1755MB to 3.2253MB.\nselected 0/22 features for the model dataset\nprocessing criteria applprev_2...\ndropped column credacc_cards_status_52L because too many nulls\nMemory of polars dataframe went from 355.6499MB to 328.8031MB.\nselected 1/6 features for the model dataset\nprocessing criteria person_2...\ndropped column relatedpersons_role_762T because too many nulls\ndropped column empls_employedfrom_796D because too many nulls\ndropped column addres_district_368M because of category size\ndropped column empls_employer_name_740M because of category size\ndropped column addres_zip_823M because of category size\nMemory of polars dataframe went from 38.3421MB to 35.2075MB.\nselected 0/8 features for the model dataset\nMemory usage of pandas dataframe is 1498.19 MB\nMemory usage after optimization is: 799.33 MB\nDecreased by 46.6%\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ds['train'].shape)\nds['train']['target'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:01.501848Z","iopub.execute_input":"2024-05-14T04:04:01.502264Z","iopub.status.idle":"2024-05-14T04:04:01.527563Z","shell.execute_reply.started":"2024-05-14T04:04:01.502204Z","shell.execute_reply":"2024-05-14T04:04:01.526239Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(1526659, 237)\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"target\n0    0.968563\n1    0.031437\nName: proportion, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# del DSBuilder\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:01.532890Z","iopub.execute_input":"2024-05-14T04:04:01.533360Z","iopub.status.idle":"2024-05-14T04:04:01.540319Z","shell.execute_reply.started":"2024-05-14T04:04:01.533328Z","shell.execute_reply":"2024-05-14T04:04:01.539261Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"ds['train']","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:01.541727Z","iopub.execute_input":"2024-05-14T04:04:01.542222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = ds['test'][['case_id']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training LGBM","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split,StratifiedGroupKFold\nimport lightgbm as lgb \nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\nfrom hyperopt.pyll import scope\nfrom functools import partial","metadata":{"execution":{"iopub.status.idle":"2024-05-14T04:04:04.955722Z","shell.execute_reply.started":"2024-05-14T04:04:01.929859Z","shell.execute_reply":"2024-05-14T04:04:04.954374Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:04.957378Z","iopub.execute_input":"2024-05-14T04:04:04.958086Z","iopub.status.idle":"2024-05-14T04:04:04.968900Z","shell.execute_reply.started":"2024-05-14T04:04:04.958027Z","shell.execute_reply":"2024-05-14T04:04:04.967249Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def get_base_params():\n    base_params = {\n        'boosting_type':'gbdt',\n        'random_state': 117,\n        'objective': 'binary',\n        'metric': 'auc',\n        'extra_trees':True,\n        'verbose': -1,\n        'max_bin': 64,\n        'device_type': 'gpu'\n        \n    }\n    return base_params","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:04.970394Z","iopub.execute_input":"2024-05-14T04:04:04.970822Z","iopub.status.idle":"2024-05-14T04:04:04.984556Z","shell.execute_reply.started":"2024-05-14T04:04:04.970788Z","shell.execute_reply":"2024-05-14T04:04:04.983026Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# # do splits ahead of time to improve trial speed\nk = 5\n\nlgbtrain = lgb.Dataset(ds['train'].loc[:,ds['features']], label=ds['train'].loc[:,'target'])\ntest_X = ds['test'][ds['features']]\nsplits = [(train_idx,valid_idx) for train_idx,valid_idx in \n          StratifiedGroupKFold(n_splits=k).split(np.arange(ds['train'].shape[0]),\n                                                 ds['train']['target'],\n                                                 groups = ds['train']['WEEK_NUM'])]","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:04.988400Z","iopub.execute_input":"2024-05-14T04:04:04.988845Z","iopub.status.idle":"2024-05-14T04:04:06.051090Z","shell.execute_reply.started":"2024-05-14T04:04:04.988810Z","shell.execute_reply":"2024-05-14T04:04:06.049102Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m lgbtrain \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[:,ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]], label\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[:,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m test_X \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      6\u001b[0m splits \u001b[38;5;241m=\u001b[39m [(train_idx,valid_idx) \u001b[38;5;28;01mfor\u001b[39;00m train_idx,valid_idx \u001b[38;5;129;01min\u001b[39;00m \n\u001b[0;32m----> 7\u001b[0m           StratifiedGroupKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[43mk\u001b[49m)\u001b[38;5;241m.\u001b[39msplit(np\u001b[38;5;241m.\u001b[39marange(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m      8\u001b[0m                                                  ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m                                                  groups \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWEEK_NUM\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n","\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"],"ename":"NameError","evalue":"name 'k' is not defined","output_type":"error"}]},{"cell_type":"code","source":"## I AM DISABLING THIS BECAUSE I AM GOING TO DO TUNING ELSEWHERE\n\n\n# set up search space\n# search_space_setup = {\n#     'feature_fraction': hp.uniform('colsample_bynode', 0.3, 0.8),\n#     'max_depth': scope.int(hp.uniform('max_depth', 5, 20)),\n#     'l1_regularization': hp.loguniform('l1_regularization', np.log(.001), np.log(100)),\n#     'l2_regularization':hp.loguniform('l2_regularization',np.log(.001), np.log(100)),\n#     'cat_l2': hp.loguniform('cat_l2', np.log(.001), np.log(100)),\n#     'bagging_fraction': hp.uniform('bagging_fraction', 0.3, 0.8),\n#     'bagging_freq': scope.int(hp.uniform('bagging_freq', 0, 5)),\n#     'learning_rate' : hp.loguniform('learning_rate', np.log(0.01), np.log(.5)),\n# #     'n_estimators':scope.int(hp.uniform('n_estimators', 500, 1500)),\n    \n\n# }\n# search_space = get_base_params()\n# for k,v in search_space_setup.items():\n#     search_space[k] = v\n\n\n\n\n# def trial_fn(params,\n#              splits = None,\n#              dataset = None):\n    \n#     num_boost_round = params.pop('n_estimators')\n#     cv_results = lgb.cv(\n#         params,\n#         dataset,\n#         num_boost_round=50,\n#         folds=splits,\n#         seed = 117\n#     ) \n    \n#     score = cv_results['valid auc-mean'][-1] \n#     return {\"status\": STATUS_OK, \"loss\": -score} # always minimizes\n\n\n# best_params = fmin(fn=partial(trial_fn, splits = splits, dataset = lgbtrain),\n#                     space=search_space,\n#                     algo=tpe.suggest,\n#                     max_evals=10,\n#                     timeout=60*60*2 # seconds\n#                   )\n# int_params = ['max_depth','n_estimators','bagging_freq']\n# bestp = get_base_params()\n# for k,v in best_params.items():\n#     if k in int_params:\n#         bestp[k] = int(v)\n#     else:\n#         bestp[k] = v\n# bestp","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:06.052383Z","iopub.status.idle":"2024-05-14T04:04:06.052831Z","shell.execute_reply.started":"2024-05-14T04:04:06.052634Z","shell.execute_reply":"2024-05-14T04:04:06.052652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bestp = {\n         'boosting_type': 'gbdt',\n         'device_type': 'gpu',\n         'random_state': 117,\n         'objective': 'binary',\n         'metric': 'auc',\n         'extra_trees': True,\n         'verbose': -1,\n         'max_bin': 64,\n         'bagging_fraction': 0.6615111203742043,\n         'bagging_freq': 4,\n         'cat_l2': 0.4303012850161522,\n         'colsample_bynode': 0.30799275380454566,\n         'l1_regularization': 0.09818609605701412,\n         'l2_regularization': 45.88388390697673,\n         'learning_rate': 0.06583892942324936,\n         'max_depth': 15,\n         'n_estimators': 849\n        }","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:06.054771Z","iopub.status.idle":"2024-05-14T04:04:06.055344Z","shell.execute_reply.started":"2024-05-14T04:04:06.055046Z","shell.execute_reply":"2024-05-14T04:04:06.055069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbm = lgb.train(\n    bestp,\n    lgbtrain \n)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:06.057352Z","iopub.status.idle":"2024-05-14T04:04:06.057951Z","shell.execute_reply.started":"2024-05-14T04:04:06.057658Z","shell.execute_reply":"2024-05-14T04:04:06.057683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del ds\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:06.059944Z","iopub.status.idle":"2024-05-14T04:04:06.060510Z","shell.execute_reply.started":"2024-05-14T04:04:06.060258Z","shell.execute_reply":"2024-05-14T04:04:06.060283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n","metadata":{}},{"cell_type":"code","source":"submission['score'] = gbm.predict(test_X)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:06.061960Z","iopub.status.idle":"2024-05-14T04:04:06.062444Z","shell.execute_reply.started":"2024-05-14T04:04:06.062227Z","shell.execute_reply":"2024-05-14T04:04:06.062246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:04:06.064076Z","iopub.status.idle":"2024-05-14T04:04:06.064538Z","shell.execute_reply.started":"2024-05-14T04:04:06.064319Z","shell.execute_reply":"2024-05-14T04:04:06.064337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
