{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport polars as pl\nimport os, gc\nfrom sklearn.preprocessing import LabelEncoder\n\n\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 1000)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:25:42.650576Z","iopub.execute_input":"2024-05-11T07:25:42.651346Z","iopub.status.idle":"2024-05-11T07:25:42.656732Z","shell.execute_reply.started":"2024-05-11T07:25:42.651314Z","shell.execute_reply":"2024-05-11T07:25:42.655719Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def calculate_woe_iv_categorical(feature, response):\n    # Calculate the total number of events (positive responses) and non-events (negative responses)\n    total_events = response.sum()\n    total_non_events = response.count() - total_events\n    \n    # Create a new DataFrame with the feature and response values\n    df = pd.DataFrame({'bin': feature.fillna('missing'), 'response': response})\n    \n    # Calculate the percentage of events and non-events for each bin of the feature\n    bin_summary = df.groupby('bin')['response'].agg(['sum', 'count']).reset_index()\n    bin_summary.columns = ['bin', 'events', 'total']\n    bin_summary['non-events'] = (bin_summary['total'] - bin_summary['events']) \n    bin_summary['event_rate'] = (bin_summary['events'] / total_events)\n    bin_summary['non-event_rate'] = (bin_summary['non-events'] / total_non_events) + 1e-10 # epsilon so that that the non event rate is not 0\n\n    # Calculate the Weight of Evidence (WOE) and Information Value (IV) for each bin\n    bin_summary['WOE'] = np.log1p(bin_summary['event_rate'] / bin_summary['non-event_rate'])\n    bin_summary['IV'] = (bin_summary['event_rate'] - bin_summary['non-event_rate']) * bin_summary['WOE']\n\n    # # Calculate the total Information Value (IV) for the feature\n    total_IV = bin_summary['IV'].sum()\n    \n    return total_IV\n\ndef calculate_woe_iv_numeric(feature, response,quantiles = 50):\n    # Calculate the total number of events (positive responses) and non-events (negative responses)\n    total_events = response.sum()\n    total_non_events = response.count() - total_events\n    \n    # Create a new DataFrame with the feature and response values\n    df = pd.DataFrame({'feature': feature, 'response': response})\n    \n    # we want to support missing values\n    df['bin'] = -1\n    df.loc[df['feature'].notnull(),'bin'] = pd.qcut(df.loc[df['feature'].notnull(),'feature'], q=quantiles,duplicates='drop',labels=False)\n\n    del df['feature']\n    # Calculate the percentage of events and non-events for each bin of the feature\n    bin_summary = df.groupby('bin')['response'].agg(['sum', 'count']).reset_index()\n    bin_summary.columns = ['bin', 'events', 'total']\n    bin_summary['non-events'] = (bin_summary['total'] - bin_summary['events']) \n    bin_summary['event_rate'] = (bin_summary['events'] / total_events)\n    bin_summary['non-event_rate'] = (bin_summary['non-events'] / total_non_events) + 1e-10 # epsilon so that that the non event rate is not 0\n\n    # Calculate the Weight of Evidence (WOE) and Information Value (IV) for each bin\n    bin_summary['WOE'] = np.log1p(bin_summary['event_rate'] / bin_summary['non-event_rate'])\n    bin_summary['IV'] = (bin_summary['event_rate'] - bin_summary['non-event_rate']) * bin_summary['WOE']\n\n    # # Calculate the total Information Value (IV) for the feature\n    total_IV = bin_summary['IV'].sum()\n    \n    return total_IV\n\ndef calculate_psi_categorical(old,new): \n    old = old.fillna(\"missing\")\n    new = new.fillna(\"missing\")    \n    \n    bins = list(set(old.tolist()+new.tolist())) \n    bin_summary = pd.DataFrame(bins,columns=['bin'])\n    bin_summary['prop_old'] = (bin_summary['bin'].apply(lambda x: (old==x).sum()) / len(old)) + 1e-10 # epsilon\n    bin_summary['prop_new'] = (bin_summary['bin'].apply(lambda x: (new==x).sum()) / len(new)) + 1e-10 # epsilon\n\n    \n    return np.sum((bin_summary['prop_old'] - bin_summary['prop_new']) * np.log(bin_summary['prop_old']/bin_summary['prop_new']))\n\ndef calculate_psi_numeric(old,new,q=10): \n\n    old = pd.DataFrame(old,columns=['val'])\n    new = pd.DataFrame(new,columns=['val'])\n    \n    # set up initial bins for missing values\n    old['bin'] = -1\n    new['bin'] = -1\n    \n    \n    # return 0 in the event that theres only unique bin across both\n    if (old['bin'].fillna(-9999).nunique() + new['bin'].fillna(-9999).nunique()) == 0:\n        return 0\n    else: \n        # assign each value to a quantile \n        old.loc[old.notnull(),'bin'] = pd.qcut(old.loc[old.notnull(),'val'], q=quantiles,duplicates='drop',labels=False)\n        new.loc[new.notnull(),'bin'] = pd.qcut(new.loc[old.notnull(),'val'], q=quantiles,duplicates='drop',labels=False)\n        \n    \n        bins = list(set(old['bin'].tolist()+new['bin'].tolist())) \n        bin_summary = pd.DataFrame(bins,columns=['bin'])\n        bin_summary['prop_old'] = (bin_summary['bin'].apply(lambda x: (old['bin']==x).sum()) / len(old)) + 1e-10 # epsilon\n        bin_summary['prop_new'] = (bin_summary['bin'].apply(lambda x: (new['bin']==x).sum()) / len(new)) + 1e-10 # epsilon\n    \n        return np.sum((bin_summary['prop_old'] - bin_summary['prop_new']) * np.log(bin_summary['prop_old']/bin_summary['prop_new']))","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:25:42.683680Z","iopub.execute_input":"2024-05-11T07:25:42.684650Z","iopub.status.idle":"2024-05-11T07:25:42.707988Z","shell.execute_reply.started":"2024-05-11T07:25:42.684618Z","shell.execute_reply":"2024-05-11T07:25:42.707085Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"[Data Info](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data) <br>\n[Discussion on how the data is setup](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/473950) <br>\n[Starter Notebook](https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook)\n* depth=0 - These are static features directly tied to a specific case_id.\n* depth=1 - Each case_id has an associated historical record, indexed by num_group1.\n* depth=2 - Each case_id has an associated historical record, indexed by both num_group1 and num_group2.","metadata":{}},{"cell_type":"code","source":"class Aggregator:\n    def __init__(self,numeric_cols,string_cols,date_cols,criteria):\n        self.numeric_cols = numeric_cols\n        self.string_cols  = string_cols\n        self.date_cols    = date_cols\n        self.criteria = criteria\n        \n    def num_expr(self,col):\n        \n        expr_max    = [pl.max(col).cast(pl.Float32).alias(f\"{col}_MAX\")]\n        expr_min    = [pl.min(col).cast(pl.Float32).alias(f\"{col}_MIN\")]\n        expr_last   = [pl.last(col).cast(pl.Float32).alias(f\"{col}_LAST\")]\n        expr_mean   = [pl.mean(col).cast(pl.Float32).alias(f\"{col}_MEAN\")]\n        expr_median = [pl.median(col).cast(pl.Float32).alias(f\"{col}_MEDIAN\")]\n        expr_var    = [pl.var(col).cast(pl.Float32).alias(f\"{col}_VAR\")]\n\n        return expr_max + expr_last + expr_mean + expr_median + expr_var + expr_min\n\n    def date_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST\")]\n        expr_mean   = [pl.mean(col).alias(f\"{col}_MEAN\")]\n\n        return expr_max + expr_last + expr_mean \n\n    def str_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST\")]\n        return expr_max + expr_last \n\n    def count_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n\n        return expr_max\n\n    def get_exprs(self,df):\n        expr = []\n        new_date_cols = []\n        new_str_cols = []\n        for col in df.columns:\n            if 'num_group' in col:\n                expr.extend(self.count_expr(col))\n            elif col in self.numeric_cols:\n                expr.extend(self.num_expr(col))\n            elif col in self.string_cols:\n                new_str_cols.extend([f\"{col}_MAX\",f\"{col}_LAST\"])\n                expr.extend(self.str_expr(col))\n            elif col in self.date_cols:\n                new_date_cols.extend([f\"{col}_MAX\",f\"{col}_LAST\",f\"{col}_MEAN\"])\n                expr.extend(self.date_expr(col))\n        \n        return expr, new_date_cols, new_str_cols","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:25:42.709787Z","iopub.execute_input":"2024-05-11T07:25:42.710795Z","iopub.status.idle":"2024-05-11T07:25:42.725354Z","shell.execute_reply.started":"2024-05-11T07:25:42.710759Z","shell.execute_reply":"2024-05-11T07:25:42.724337Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class DatasetBuilder:\n    \"\"\" This class is used to create the dataset \"\"\"\n    def __init__(self, \n                 n_samples   = None, \n                 parent_path = \"/kaggle/input/home-credit-credit-risk-model-stability\"):\n        \n\n\n        self.parent_path = parent_path\n        self.n_samples = n_samples\n\n        self.feat_info = pd.read_csv(f\"{parent_path}/feature_definitions.csv\")\n        self.date_cols = []\n        self.string_cols = []\n        self.numeric_cols = []\n        \n        self.run()\n\n    def explain_feat(self,feat_name:str):\n        assert feat_name in self.feat_info['Variable'].unique(), \"feature not found in feature info dataframe\"\n        return self.feat_info[self.feat_info['Variable']==feat_name]['Description'].values[0]\n\n    def set_table_dtypes(self,df):\n        for col in df.columns:\n            \n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int32))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float32))\n                if col not in self.numeric_cols:\n                    self.numeric_cols.append(col)                \n            elif (col[-1] in (\"M\",)) or (col in self.string_cols):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n                if col not in self.string_cols:\n                    self.string_cols.append(col)\n            elif col[-1] in (\"L\",\"T\"): # we dont know the transform needed, just going to assume its either float and if not, then string\n                try:\n                    df = df.with_columns(pl.col(col).cast(pl.Float32))\n                    if col not in self.numeric_cols:\n                        self.numeric_cols.append(col) \n                except:\n                    df = df.with_columns(pl.col(col).cast(pl.String))\n                    if col not in self.string_cols:\n                        self.string_cols.append(col) \n                    continue\n                \n            elif col[-1] in (\"D\",) or (col in self.date_cols):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n                if col not in self.date_cols:\n                    self.date_cols.append(col)\n                \n        return df\n\n    def feature_engineer_dates(self,df,date_cols=None):\n        if date_cols is None:\n            date_cols = self.date_cols\n        for col in date_cols:\n            if col in df.columns:\n                df = df.with_columns((pl.col(\"date_decision\") - pl.col(col)).dt.total_days().alias(f'{col}_DAYS_SINCE'))  # days since\n                df = df.drop(col)\n        \n        return df\n  \n\n    \n    def create_base_dataset(self):\n        \n        # load in the training dataset \n        if self.n_samples:\n            train = pl.read_parquet(f\"{self.parent_path}/parquet_files/train/train_base.parquet\") \\\n            .pipe(self.set_table_dtypes).sample(n=self.n_samples).with_columns(pl.lit('train').alias('partition'))\n        else:\n            train = pl.read_parquet(f\"{self.parent_path}/parquet_files/train/train_base.parquet\") \\\n            .pipe(self.set_table_dtypes).with_columns(pl.lit('train').alias('partition'))\n        \n        # load in the test dataset\n        test =  pl.read_parquet(f\"{self.parent_path}/parquet_files/test/test_base.parquet\")\\\n                .pipe(self.set_table_dtypes).with_columns(pl.lit('test').alias('partition'))\n        \n        # concat train and test\n        self.df = pl.concat([train,test],how='diagonal_relaxed')\n        \n        # get all case_ids\n        self.case_ids = self.df.get_column('case_id').to_list()\n        \n        # store base cols\n        self.base_df_cols = self.df.columns\n        \n        # features\n        self.df = self.df.with_columns(self.df[\"date_decision\"].dt.month().alias(\"dd_month\"))\n        self.df = self.df.with_columns(self.df[\"date_decision\"].dt.year().alias(\"dd_year\"))\n\n    def read_in_files_with_criteria(self, criteria:str):\n        train_df  = pl.concat([pl.read_parquet(f\"{self.parent_path}/parquet_files/train/{x}\").pipe(self.set_table_dtypes).filter(pl.col('case_id').is_in(self.case_ids))\n                       for x in os.listdir(f\"{self.parent_path}/parquet_files/train\") if (criteria in x)],how='diagonal_relaxed')\n        test_df  =  pl.concat([pl.read_parquet(f\"{self.parent_path}/parquet_files/test/{x}\").pipe(self.set_table_dtypes)\n                       for x in os.listdir(f\"{self.parent_path}/parquet_files/test\") if (criteria in x)],how='diagonal_relaxed')\n        \n        # being in train partition doesnt gaurentee it is in the test partition, so we have to ensure it \n        columns_in_common = list(set(train_df.columns).intersection(test_df.columns))\n        \n        df = pl.concat([train_df.select(columns_in_common),\n                         test_df.select(columns_in_common)],how='diagonal_relaxed')\n        \n        \n        return df\n       \n        \n\n    def evaluate_features(self,df):\n        \"\"\"\n        1) calculates weight of evidence * information value for measuring predictive power\n        2) iterates through months to get psi (currently trying to figure out why so many nan out)\n        \n        \"\"\"\n        feats = [x for x in df.columns if x not in self.base_df_cols]\n        \n        # predictive power - woe*iv\n        woeivs  = []\n        for col in feats:\n            if col in self.string_cols:\n                woeiv = calculate_woe_iv_categorical(df[col].to_pandas(), df['target'].to_pandas())\n                woeivs.append(woeiv)\n            else:\n                woeiv = calculate_woe_iv_numeric(df[col].to_pandas(), df['target'].to_pandas())\n                woeivs.append(woeiv)\n\n        \n#         # stability - psi and woe*iv\n#         psi_res = {x:[] for x in feats}\n#         woe_res = {x:[] for x in feats}\n#         for i in range(len(year_months)-1):\n#             psis = []\n#             old = df.filter((pl.col(\"ym_decision\") == year_months[i]))\n#             new = df.filter((pl.col(\"ym_decision\") == year_months[i+1]))\n#             for col in feats:\n#                 if col in self.string_cols:\n#                     psi = calculate_psi_categorical(old[col].to_pandas(),new[col].to_pandas())\n#                 else:\n#                     psi = calculate_psi_numeric(old[col].to_pandas(),new[col].to_pandas())\n                    \n#                 psi_res[col].append(psi)\n                    \n\n\n            \n        feature_scores = pd.DataFrame(feats,columns=['feature'])\n        feature_scores['prop_null'] = feature_scores['feature'].apply(lambda feat: df[feat].to_pandas().isna().sum()) / len(self.df)\n        feature_scores['woe_iv'] = woeivs\n        \n        # lots of these ended up as nulls, will deal with later\n#         feature_scores['eligible_psi'] = feature_scores['feature'].apply(lambda feat: sum([0 if np.isnan(x) else 1 for x in psi_res[feat]]))\n#         feature_scores['avg_psi'] = feature_scores['feature'].apply(lambda feat: np.nanmean(psi_res[feat]))\n#         feature_scores['std_psi'] = feature_scores['feature'].apply(lambda feat: np.nanstd(psi_res[feat]))\n#         feature_scores['max_psi'] = feature_scores['feature'].apply(lambda feat: np.max(psi_res[feat]))\n\n        return feature_scores\n        \n    \n    def select_features(self,df,score=\"woe_iv\",top_k=150):\n        feature_scores = self.evaluate_features(df)\n        top_k = min(top_k,len(feature_scores)-1)\n        chosen_features = feature_scores.sort_values(score,ascending=False).reset_index(drop=True).loc[:top_k,'feature'].to_list()\n        print(f\"selected {top_k}/{len(feature_scores)} features for the model dataset\")\n        return chosen_features\n\n    \n    def to_pandas(self,df_data):\n        df_data = df_data.to_pandas()\n        cat_cols = [x for x in df_data.columns if (x in self.string_cols) and (x not in self.base_df_cols)]\n        df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n        \n        return df_data, cat_cols\n\n    def reduce_mem_usage(self,df):\n        \"\"\" iterate through all the columns of a dataframe and modify the data type\n            to reduce memory usage.        \n        \"\"\"\n        start_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n        for col in df.columns[7:]:\n            col_type = df[col].dtype\n            if str(col_type)==\"category\":\n                continue\n\n            if col_type not in self.string_cols:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n#                 else:\n#                     if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                         df[col] = df[col].astype(np.float16)\n#                     elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n#                         df[col] = df[col].astype(np.float32)\n#                     else:\n#                         df[col] = df[col].astype(np.float32)\n#                 df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\n            else:\n                continue\n        end_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n        return df\n     \n    def process_depth0(self):\n        \"\"\"\n        These files can be used as is except for the dates, so just collect them, do feature engineering on the dates, then \n        throw out the date columns, grab top k features, join back to base\n        \"\"\"\n        depth0_criterias = [\"static_0\",\"static_cb_0\"]\n        df = self.df[['case_id','target','date_decision']]\n        for criteria in depth0_criterias:\n            df = df.join(self.read_in_files_with_criteria(criteria), on=['case_id'], how='left')\n            \n        df = self.feature_engineer_dates(df)\n        depth0_feats = self.select_features(df,score=\"woe_iv\",top_k=75)\n        self.df = self.df.join(df[['case_id']+depth0_feats], on='case_id', how='left')   \n    \n\n    def process_depth1(self):\n        \"\"\"\n        These files have one group; collect them, auto aggregate, do feature engineering on the dates,\n        throw out the date columns, grab top k features, join back to base\n        \"\"\"\n        depth1_criterias = [\"applprev_1\",\"other_1\",\n                            \"tax_registry_a_1\",\"tax_registry_b_1\",\"tax_registry_c_1\",\n                            \"credit_bureau_a_1\",\"credit_bureau_b_1\",\n                            \"deposit_1\",\"person_1\",\"debitcard_1\"]\n        df = self.df[['case_id','target','date_decision']]\n        agg_dt_cols_coll = []\n        for criteria in depth1_criterias:\n            criteria_df = self.read_in_files_with_criteria(criteria)\n            aggr = Aggregator(self.numeric_cols,self.string_cols,self.date_cols,f\"{criteria.upper()}_DEPTH1\")\n            agg_expr, agg_dt_cols,agg_str_cols = aggr.get_exprs(criteria_df)\n            agg_dt_cols_coll.extend(agg_dt_cols)\n            self.string_cols.extend(agg_str_cols)\n            criteria_df = criteria_df.group_by(\"case_id\").agg(agg_expr)\n            df = df.join(criteria_df, on=['case_id'], how='inner')\n        \n        df = self.feature_engineer_dates(df,date_cols=agg_dt_cols_coll)\n        depth1_feats = self.select_features(df,score=\"woe_iv\",top_k=75)\n        self.df = self.df.join(df[['case_id']+depth1_feats], on='case_id', how='left') \n    \n    def run(self):\n        self.create_base_dataset()\n        self.process_depth0()\n        self.process_depth1()\n        \n    def get_datasets(self):\n        ds,cat_cols = self.to_pandas(self.df)\n        ds = self.reduce_mem_usage(ds)\n        return {\"train\":ds[ds['partition']=='train'].reset_index(drop=True), \n                \"test\": ds[ds['partition']=='test'].reset_index(drop=True), \n                \"features\": [x for x in ds.columns if x not in self.base_df_cols],\n                \"cat_features\": cat_cols}","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:52:57.735138Z","iopub.execute_input":"2024-05-11T07:52:57.735534Z","iopub.status.idle":"2024-05-11T07:52:57.786923Z","shell.execute_reply.started":"2024-05-11T07:52:57.735501Z","shell.execute_reply":"2024-05-11T07:52:57.785804Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"ds = DatasetBuilder().get_datasets()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:52:58.114711Z","iopub.execute_input":"2024-05-11T07:52:58.115097Z","iopub.status.idle":"2024-05-11T07:56:46.767708Z","shell.execute_reply.started":"2024-05-11T07:52:58.115066Z","shell.execute_reply":"2024-05-11T07:56:46.766788Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"selected 75/219 features for the model dataset\nselected 75/975 features for the model dataset\nMemory usage of dataframe is 1151.66 MB\nMemory usage after optimization is: 1148.75 MB\nDecreased by 0.3%\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ds['train'].shape)\nds['train']['target'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:56:46.770052Z","iopub.execute_input":"2024-05-11T07:56:46.770780Z","iopub.status.idle":"2024-05-11T07:56:46.790310Z","shell.execute_reply.started":"2024-05-11T07:56:46.770740Z","shell.execute_reply":"2024-05-11T07:56:46.789373Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"(1526659, 160)\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"target\n0.0    0.968563\n1.0    0.031437\nName: proportion, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# del DSBuilder\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:56:46.791931Z","iopub.execute_input":"2024-05-11T07:56:46.792231Z","iopub.status.idle":"2024-05-11T07:56:46.798700Z","shell.execute_reply.started":"2024-05-11T07:56:46.792190Z","shell.execute_reply":"2024-05-11T07:56:46.797812Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# Training XGBoost","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split,StratifiedGroupKFold\nimport xgboost as xgb\nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\nfrom hyperopt.pyll import scope\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:56:46.800722Z","iopub.execute_input":"2024-05-11T07:56:46.801004Z","iopub.status.idle":"2024-05-11T07:56:46.808640Z","shell.execute_reply.started":"2024-05-11T07:56:46.800981Z","shell.execute_reply":"2024-05-11T07:56:46.807893Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:56:46.809824Z","iopub.execute_input":"2024-05-11T07:56:46.810083Z","iopub.status.idle":"2024-05-11T07:56:46.819338Z","shell.execute_reply.started":"2024-05-11T07:56:46.810060Z","shell.execute_reply":"2024-05-11T07:56:46.818442Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def get_base_params():\n    base_params = {\n        'max_cat_to_onehot': 4,\n        'max_delta_step':0.7,\n        'random_state': 117,\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n\n        # turn on when gpu \n        'device': 'cuda',\n        'sampling_method':'gradient_based',\n    }\n    return base_params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_space = {\n    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.2, 0.6), # col_sample reduces correlation b/c cols to a limit and reduces computation\n    'colsample_bynode': hp.uniform('colsample_bynode', 0.2, 0.6),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.2, 0.6),\n    'gamma': hp.loguniform('gamma',np.log(0.00001), np.log(100)),\n    'max_depth': scope.int(hp.uniform('max_depth', 2, 15)),\n    'min_child_weight': hp.loguniform('min_child_weight', np.log(0.00001), np.log(100)),\n#     'reg_alpha': hp.loguniform('reg_alpha', np.log(.00001), np.log(100)),\n#     'reg_lambda':hp.loguniform('reg_lambda',np.log(.00001), np.log(100)),\n    'scale_pos_weight': hp.uniform('scale_pos_weight',1, 20),\n    'subsample': hp.uniform('subsample', 0.2, 0.5),\n    'learning_rate' : hp.loguniform('learning_rate', np.log(0.00001), np.log(.5)),\n    'n_estimators':scope.int(hp.uniform('n_estimators', 100, 1000)),\n    \n    \n   \n    \n    # turn off when gpu\n#     'n_jobs': 10,\n#     'tree_method':'hist',\n\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:56:46.820516Z","iopub.execute_input":"2024-05-11T07:56:46.821078Z","iopub.status.idle":"2024-05-11T07:56:46.830561Z","shell.execute_reply.started":"2024-05-11T07:56:46.821053Z","shell.execute_reply":"2024-05-11T07:56:46.829493Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# do splits ahead of time to improve trial speed\nk = 4\nskf = StratifiedGroupKFold(n_splits=k)\nidx = np.arange(len(ds['train']))\nds_train_dmatrix_splits = []\nfor train_idx, valid_idx in skf.split(idx,ds['train']['target'],groups = ds['train']['WEEK_NUM']): \n    dtrain = xgb.DMatrix(ds['train'].loc[train_idx,ds['features']], label=ds['train'].loc[train_idx,'target'],enable_categorical=True)\n    dvalid = xgb.DMatrix(ds['train'].loc[valid_idx,ds['features']], label=ds['train'].loc[valid_idx,'target'],enable_categorical=True)\n    ds_train_dmatrix_splits.append((dtrain,dvalid))","metadata":{"execution":{"iopub.status.busy":"2024-05-11T07:56:46.831844Z","iopub.execute_input":"2024-05-11T07:56:46.832166Z","iopub.status.idle":"2024-05-11T07:57:09.061142Z","shell.execute_reply.started":"2024-05-11T07:56:46.832140Z","shell.execute_reply":"2024-05-11T07:57:09.060194Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def trial_fn(params,\n             splits = []):\n\n\n    n_estimators = params.pop('n_estimators')\n    scores = [] \n    for dtrain, dvalid in splits: \n        mod = xgb.train(params,dtrain, n_estimators)\n        score = roc_auc_score(dvalid.get_label(),mod.predict(dvalid))\n        scores.append(score)\n    \n    score = np.mean(scores) \n\n    return {\"status\": STATUS_OK, \"loss\": -score} # always minimizes","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:05:48.417621Z","iopub.execute_input":"2024-05-11T08:05:48.418691Z","iopub.status.idle":"2024-05-11T08:05:48.424768Z","shell.execute_reply.started":"2024-05-11T08:05:48.418653Z","shell.execute_reply":"2024-05-11T08:05:48.423846Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"best_params = fmin(fn=partial(trial_fn, splits = ds_train_dmatrix_splits),\n                    space=search_space,\n                    algo=tpe.suggest,\n                    max_evals=100,\n                    timeout=60*60 # seconds\n                  )\nint_params = ['max_depth','n_estimators','max_cat_to_onehot']\nbestp = get_base_params()\nfor k,v in best_params.items():\n    if k in int params:\n        bestp[k] = int(v)\n    else:\n        bestp[k] = v\nbestp","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:05:49.169332Z","iopub.execute_input":"2024-05-11T08:05:49.170229Z","iopub.status.idle":"2024-05-11T08:41:52.404212Z","shell.execute_reply.started":"2024-05-11T08:05:49.170197Z","shell.execute_reply":"2024-05-11T08:41:52.403296Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"100%|██████████| 10/10 [36:03<00:00, 216.32s/trial, best loss: -0.8005964928837767]\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"{'colsample_bylevel': 0,\n 'colsample_bynode': 0,\n 'colsample_bytree': 0,\n 'gamma': 0,\n 'learning_rate': 0,\n 'max_depth': 7,\n 'min_child_weight': 10,\n 'n_estimators': 436,\n 'scale_pos_weight': 1,\n 'subsample': 0}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_estimators = best_params.pop('n_estimators')\ndtrain = xgb.DMatrix(ds['train'][ds['features']], label=ds['train']['target'],enable_categorical=True)\nmod = xgb.train(best_params,dtrain, n_estimators)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:41:52.406166Z","iopub.execute_input":"2024-05-11T08:41:52.406558Z","iopub.status.idle":"2024-05-11T08:42:27.459260Z","shell.execute_reply.started":"2024-05-11T08:41:52.406507Z","shell.execute_reply":"2024-05-11T08:42:27.458308Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n","metadata":{}},{"cell_type":"code","source":"del ds['train']\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:42:27.460595Z","iopub.execute_input":"2024-05-11T08:42:27.463827Z","iopub.status.idle":"2024-05-11T08:42:27.801768Z","shell.execute_reply.started":"2024-05-11T08:42:27.463792Z","shell.execute_reply":"2024-05-11T08:42:27.800792Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"2546"},"metadata":{}}]},{"cell_type":"code","source":"dtest = xgb.DMatrix(ds['test'][ds['features']], enable_categorical=True)\nds['test']['score'] = mod.predict(dtest)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:42:27.803402Z","iopub.execute_input":"2024-05-11T08:42:27.803706Z","iopub.status.idle":"2024-05-11T08:42:27.839401Z","shell.execute_reply.started":"2024-05-11T08:42:27.803680Z","shell.execute_reply":"2024-05-11T08:42:27.838607Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"submission = ds['test'][['case_id','score']]\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:42:27.840464Z","iopub.execute_input":"2024-05-11T08:42:27.841162Z","iopub.status.idle":"2024-05-11T08:42:27.855715Z","shell.execute_reply.started":"2024-05-11T08:42:27.841133Z","shell.execute_reply":"2024-05-11T08:42:27.854880Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"   case_id     score\n0    57543  0.031437\n1    57549  0.031437\n2    57551  0.031437\n3    57552  0.031437\n4    57569  0.031437","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>57543</td>\n      <td>0.031437</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>57549</td>\n      <td>0.031437</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57551</td>\n      <td>0.031437</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>57552</td>\n      <td>0.031437</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57569</td>\n      <td>0.031437</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}