{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport polars as pl\nimport os, gc\nfrom sklearn.preprocessing import LabelEncoder\n\n\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 1000)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T01:16:06.314119Z","iopub.execute_input":"2024-05-14T01:16:06.314519Z","iopub.status.idle":"2024-05-14T01:16:08.966227Z","shell.execute_reply.started":"2024-05-14T01:16:06.314485Z","shell.execute_reply":"2024-05-14T01:16:08.964935Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def calculate_woe_iv_categorical(feature, response):\n    # Calculate the total number of events (positive responses) and non-events (negative responses)\n    total_events = response.sum()\n    total_non_events = response.count() - total_events\n    \n    # Create a new DataFrame with the feature and response values\n    df = pd.DataFrame({'bin': feature.fillna('missing'), 'response': response})\n    \n    # Calculate the percentage of events and non-events for each bin of the feature\n    bin_summary = df.groupby('bin')['response'].agg(['sum', 'count']).reset_index()\n    bin_summary.columns = ['bin', 'events', 'total']\n    bin_summary['non-events'] = (bin_summary['total'] - bin_summary['events']) \n    bin_summary['event_rate'] = (bin_summary['events'] / total_events)\n    bin_summary['non-event_rate'] = (bin_summary['non-events'] / total_non_events) + 1e-10 # epsilon so that that the non event rate is not 0\n\n    # Calculate the Weight of Evidence (WOE) and Information Value (IV) for each bin\n    bin_summary['WOE'] = np.log1p(bin_summary['event_rate'] / bin_summary['non-event_rate'])\n    bin_summary['IV'] = (bin_summary['event_rate'] - bin_summary['non-event_rate']) * bin_summary['WOE']\n\n    # # Calculate the total Information Value (IV) for the feature\n    total_IV = bin_summary['IV'].sum()\n    \n    return total_IV\n\ndef calculate_woe_iv_numeric(feature, response,quantiles = 25):\n    # Calculate the total number of events (positive responses) and non-events (negative responses)\n    total_events = response.sum()\n    total_non_events = response.count() - total_events\n    \n    # Create a new DataFrame with the feature and response values\n    df = pd.DataFrame({'feature': feature, 'response': response})\n    \n    # we want to support missing values\n    df['bin'] = -1\n    df.loc[df['feature'].notnull(),'bin'] = pd.qcut(df.loc[df['feature'].notnull(),'feature'], q=quantiles,duplicates='drop',labels=False)\n\n    del df['feature']\n    # Calculate the percentage of events and non-events for each bin of the feature\n    bin_summary = df.groupby('bin')['response'].agg(['sum', 'count']).reset_index()\n    bin_summary.columns = ['bin', 'events', 'total']\n    bin_summary['non-events'] = (bin_summary['total'] - bin_summary['events']) \n    bin_summary['event_rate'] = (bin_summary['events'] / total_events)\n    bin_summary['non-event_rate'] = (bin_summary['non-events'] / total_non_events) + 1e-10 # epsilon so that that the non event rate is not 0\n\n    # Calculate the Weight of Evidence (WOE) and Information Value (IV) for each bin\n    bin_summary['WOE'] = np.log1p(bin_summary['event_rate'] / bin_summary['non-event_rate'])\n    bin_summary['IV'] = (bin_summary['event_rate'] - bin_summary['non-event_rate']) * bin_summary['WOE']\n\n    # # Calculate the total Information Value (IV) for the feature\n    total_IV = bin_summary['IV'].sum()\n    \n    return total_IV\n\ndef calculate_psi_categorical(old,new): \n    old = old.to_frame().fillna('missing')\n    old.columns = ['bin']\n    new = new.to_frame().fillna('missing')\n    new.columns = ['bin']    \n    \n    old = old.groupby('bin').agg(count_old=('bin','count'))\n    new = new.groupby('bin').agg(count_new=('bin','count'))\n    \n    bins = list(set(old['bin'].tolist()+new['bin'].tolist())) \n    bin_summary = pd.DataFrame(bins,columns=['bin'])\n    bin_summary = bin_summary.merge(old[['bin','count_old']],on='bin',how='left')\n    bin_summary = bin_summary.merge(new[['bin','count_new']],on='bin',how='left')\n    bin_summary['prop_old'] = (bin_summary['count_old'].fillna(0) / len(old)) + 1e-10 # epsilon\n    bin_summary['prop_new'] = (bin_summary['count_new'].fillna(0) / len(new)) + 1e-10 # epsilon\n\n    return np.sum((bin_summary['prop_old'] - bin_summary['prop_new']) * np.log(bin_summary['prop_old']/bin_summary['prop_new']))\n\ndef calculate_psi_numeric(old,new,q=10): \n\n    old = old.to_frame()\n    old.columns = ['val']\n    new = new.to_frame()\n    new.columns = ['val']\n    \n    # set up initial bins for missing values\n    old['bin'] = -1\n    new['bin'] = -1\n    \n    \n    # return 0 in the event that theres less than 3 unique bin across both\n    if (old['val'].fillna(-9999).nunique() + new['val'].fillna(-9999).nunique()) <= 2:\n        return np.nan\n    else: \n        # assign each value to a quantile \n        old.loc[old['val'].notnull(),'bin'] = pd.qcut(old.loc[old['val'].notnull(),'val'], q=q,duplicates='drop',labels=False)\n        new.loc[new['val'].notnull(),'bin'] = pd.qcut(new.loc[new['val'].notnull(),'val'], q=q,duplicates='drop',labels=False)\n        \n        old = old.groupby('bin').agg(count_old=('bin','count'))\n        new = new.groupby('bin').agg(count_new=('bin','count'))\n        \n        \n        bins = list(set(old['bin'].tolist()+new['bin'].tolist())) \n        bin_summary = pd.DataFrame(bins,columns=['bin'])\n        bin_summary = bin_summary.merge(old[['bin','count_old']],on='bin',how='left')\n        bin_summary = bin_summary.merge(new[['bin','count_new']],on='bin',how='left')\n        bin_summary['prop_old'] = (bin_summary['count_old'].fillna(0) / len(old)) + 1e-10 # epsilon\n        bin_summary['prop_new'] = (bin_summary['count_new'].fillna(0) / len(new)) + 1e-10 # epsilon\n    \n        return np.sum((bin_summary['prop_old'] - bin_summary['prop_new']) * np.log(bin_summary['prop_old']/bin_summary['prop_new']))","metadata":{"execution":{"iopub.status.busy":"2024-05-14T01:16:08.968483Z","iopub.execute_input":"2024-05-14T01:16:08.969104Z","iopub.status.idle":"2024-05-14T01:16:09.012887Z","shell.execute_reply.started":"2024-05-14T01:16:08.969060Z","shell.execute_reply":"2024-05-14T01:16:09.011570Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"[Data Info](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data) <br>\n[Discussion on how the data is setup](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/473950) <br>\n[Starter Notebook](https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook)\n* depth=0 - These are static features directly tied to a specific case_id.\n* depth=1 - Each case_id has an associated historical record, indexed by num_group1.\n* depth=2 - Each case_id has an associated historical record, indexed by both num_group1 and num_group2.","metadata":{}},{"cell_type":"code","source":"class Aggregator:\n    def __init__(self,numeric_cols,string_cols,date_cols,criteria):\n        self.numeric_cols = numeric_cols\n        self.string_cols  = string_cols\n        self.date_cols    = date_cols\n        self.criteria = criteria\n        \n    def num_expr(self,col):\n        \n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n        expr_min    = [pl.min(col).alias(f\"{col}_MIN_{self.criteria}\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST_{self.criteria}\")]\n        expr_mean   = [pl.mean(col).alias(f\"{col}_MEAN_{self.criteria}\")]\n        expr_median = [pl.median(col).alias(f\"{col}_MEDIAN_{self.criteria}\")]\n        expr_var    = [pl.var(col).alias(f\"{col}_VAR_{self.criteria}\")]\n\n        return expr_max + expr_last + expr_mean + expr_median + expr_var + expr_min\n\n    def date_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST_{self.criteria}\")]\n        expr_mean   = [pl.mean(col).alias(f\"{col}_MEAN_{self.criteria}\")]\n\n        return expr_max + expr_last + expr_mean \n\n    def str_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST_{self.criteria}\")]\n        return expr_max + expr_last \n\n    def count_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n\n        return expr_max\n\n    def get_exprs(self,df):\n        expr = []\n        new_date_cols = []\n        new_str_cols = []\n        for col in df.columns:\n            if 'num_group' in col:\n                expr.extend(self.count_expr(col))\n            elif col in self.numeric_cols:\n                expr.extend(self.num_expr(col))\n            elif col in self.string_cols:\n                new_str_cols.extend([f\"{col}_MAX_{self.criteria}\",f\"{col}_LAST_{self.criteria}\"])\n                expr.extend(self.str_expr(col))\n            elif col in self.date_cols:\n                new_date_cols.extend([f\"{col}_MAX_{self.criteria}\",f\"{col}_LAST_{self.criteria}\",f\"{col}_MEAN_{self.criteria}\"])\n                expr.extend(self.date_expr(col))\n        \n        return expr, new_date_cols, new_str_cols","metadata":{"execution":{"iopub.status.busy":"2024-05-14T01:16:09.015005Z","iopub.execute_input":"2024-05-14T01:16:09.015488Z","iopub.status.idle":"2024-05-14T01:16:09.038628Z","shell.execute_reply.started":"2024-05-14T01:16:09.015444Z","shell.execute_reply":"2024-05-14T01:16:09.037323Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def filter_cols(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Filters columns in the DataFrame based on null percentage and unique values for string columns.\n\n    Args:\n    - df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n    - pl.DataFrame: DataFrame with filtered columns.\n    \"\"\"\n    for col in df.columns:\n        if col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]:\n            null_pct = df[col].is_null().mean()\n\n            if null_pct > 0.97:\n                df = df.drop(col)\n                print(f\"dropped column {col} because too many nulls\")\n#     for col in df.columns:\n#         if (col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]) & (\n#             df[col].dtype == pl.String\n#         ):\n#             freq = df[col].n_unique()\n\n#             if (freq > 200) | (freq == 1):\n#                 df = df.drop(col)\n#                 print(f\"dropped column {col} because of category size\")\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-14T01:16:09.042180Z","iopub.execute_input":"2024-05-14T01:16:09.042621Z","iopub.status.idle":"2024-05-14T01:16:09.053493Z","shell.execute_reply.started":"2024-05-14T01:16:09.042579Z","shell.execute_reply":"2024-05-14T01:16:09.052224Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def reduce_polars_memory_usage(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Reduces memory usage of a DataFrame by converting column types.\n\n    Args:\n    - df (pl.DataFrame): DataFrame to optimize.\n    - name (str): Name of the DataFrame.\n\n    Returns:\n    - pl.DataFrame: Optimized DataFrame.\n    \"\"\"\n    og_mem = round(df.estimated_size('mb'), 4)\n\n    int_types = [\n        pl.Int8,\n        pl.Int16,\n        pl.Int32,\n        pl.Int64,\n        pl.UInt8,\n        pl.UInt16,\n        pl.UInt32,\n        pl.UInt64,\n    ]\n    float_types = [pl.Float32, pl.Float64]\n\n    for col in df.columns:\n        if col == 'case_id':\n            continue\n        col_type = df[col].dtype\n        if col_type in int_types + float_types:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if c_min is not None and c_max is not None:\n                if col_type in int_types:\n                    if c_min >= 0:\n                        if (\n                            c_min >= np.iinfo(np.uint8).min\n                            and c_max <= np.iinfo(np.uint8).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt8))\n                        elif (\n                            c_min >= np.iinfo(np.uint16).min\n                            and c_max <= np.iinfo(np.uint16).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt16))\n                        elif (\n                            c_min >= np.iinfo(np.uint32).min\n                            and c_max <= np.iinfo(np.uint32).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt32))\n                        elif (\n                            c_min >= np.iinfo(np.uint64).min\n                            and c_max <= np.iinfo(np.uint64).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt64))\n                    else:\n                        if (\n                            c_min >= np.iinfo(np.int8).min\n                            and c_max <= np.iinfo(np.int8).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int8))\n                        elif (\n                            c_min >= np.iinfo(np.int16).min\n                            and c_max <= np.iinfo(np.int16).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int16))\n                        elif (\n                            c_min >= np.iinfo(np.int32).min\n                            and c_max <= np.iinfo(np.int32).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int32))\n                        elif (\n                            c_min >= np.iinfo(np.int64).min\n                            and c_max <= np.iinfo(np.int64).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int64))\n                elif col_type in float_types:\n                    if (\n                        c_min > np.finfo(np.float32).min\n                        and c_max < np.finfo(np.float32).max\n                    ):\n                        df = df.with_columns(df[col].cast(pl.Float32))\n\n    print(\n        f\"Memory of polars dataframe went from {og_mem}MB to {round(df.estimated_size('mb'), 4)}MB.\"\n    )\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-14T01:16:09.056722Z","iopub.execute_input":"2024-05-14T01:16:09.057357Z","iopub.status.idle":"2024-05-14T01:16:09.079597Z","shell.execute_reply.started":"2024-05-14T01:16:09.057313Z","shell.execute_reply":"2024-05-14T01:16:09.078371Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def extract_lowercase(s):\n    # Initialize an empty result string\n    result = \"\"\n\n    # Loop through each character in the string\n    for char in s:\n        # Check if the character is lowercase\n        if char.islower() or char == '_' or char.isnumeric():\n            result += char\n        # Break the loop if a non-lowercase character is encountered (if desired)\n        elif result:\n            break\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-05-14T01:16:09.080978Z","iopub.execute_input":"2024-05-14T01:16:09.081406Z","iopub.status.idle":"2024-05-14T01:16:09.101792Z","shell.execute_reply.started":"2024-05-14T01:16:09.081369Z","shell.execute_reply":"2024-05-14T01:16:09.099995Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class DatasetBuilder:\n    \"\"\" This class is used to create the dataset \"\"\"\n    def __init__(self, \n                 n_samples   = None, \n                 parent_path = \"/kaggle/input/home-credit-credit-risk-model-stability\",\n                ):\n        \n\n\n        self.parent_path = parent_path\n        self.n_samples = n_samples\n\n        self.feat_info = pd.read_csv(f\"{parent_path}/feature_definitions.csv\")\n        self.date_cols = []\n        self.string_cols = []\n        \n        self.run()\n\n    def explain_feat(self,feat_name:str):\n        assert feat_name in self.feat_info['Variable'].unique(), \"feature not found in feature info dataframe\"\n        return self.feat_info[self.feat_info['Variable']==feat_name]['Description'].values[0]\n\n    def set_table_dtypes(self,df):\n        for col in df.columns:\n                    \n            if col in [\"case_id\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int32))\n            elif col in  [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"target\"]:\n                df = df.with_columns(pl.col(col).cast(pl.UInt16))            \n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))                \n            elif (col[-1] in (\"M\",)) or (col in self.string_cols):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n                if col not in self.string_cols:\n                    self.string_cols.append(col)\n            elif col[-1] in (\"L\",\"T\"): # we dont know the transform needed, just going to assume its either float and if not, then string\n                try:\n                    df = df.with_columns(pl.col(col).cast(pl.Float64))\n                except:\n                    df = df.with_columns(pl.col(col).cast(pl.String))\n                    if col not in self.string_cols:\n                        self.string_cols.append(col) \n                    continue\n                \n            elif col[-1] in (\"D\",) or (col in self.date_cols):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n                if col not in self.date_cols:\n                    self.date_cols.append(col)\n        return df\n\n    def feature_engineer_dates(self,df,date_cols=None):\n        if date_cols is None:\n            date_cols = self.date_cols\n        if 'date_decision' not in df.columns:\n            df = df.join(self.df[['case_id','date_decision']],on='case_id')\n        for col in date_cols:\n            if col in df.columns:\n                df = df.with_columns((pl.col(\"date_decision\") - pl.col(col)).dt.total_days().alias(f'{col}_DAYS_SINCE'))\n                df = df.drop(col)\n\n        if 'date_decision' in df.columns:\n            df = df.drop('date_decision')\n            \n        return df\n  \n    \n    def create_base_dataset(self):\n        \n        # load in the training dataset \n        if self.n_samples is not None:\n            train = pl.read_parquet(f\"{self.parent_path}/parquet_files/train/train_base.parquet\") \\\n            .sample(n=self.n_samples).with_columns(pl.lit('train').alias('partition'))\n        else:\n            train = pl.read_parquet(f\"{self.parent_path}/parquet_files/train/train_base.parquet\") \\\n            .with_columns(pl.lit('train').alias('partition'))\n        \n        # load in the test dataset\n        test =  pl.read_parquet(f\"{self.parent_path}/parquet_files/test/test_base.parquet\")\\\n                .with_columns(pl.lit(0).alias('target'))\\\n                .with_columns(pl.lit('test').alias('partition'))        \n        \n        \n        \n        \n        \n        # concat train and test\n        self.df = reduce_polars_memory_usage(pl.concat([train,test],how='vertical_relaxed').pipe(self.set_table_dtypes))\n        \n        # get all case_ids\n        self.train_case_ids = train.get_column('case_id').to_list()\n        self.test_case_ids  = test.get_column('case_id').to_list()\n        \n        # store base cols\n        self.base_df_cols = self.df.columns\n        \n        del train\n        del test\n        gc.collect()\n\n    def read_in_files_with_criteria(self, criteria:str):\n        print(f\"processing criteria {criteria}...\")\n        train_df  = pl.concat([pl.scan_parquet(f\"{self.parent_path}/parquet_files/train/{x}\", low_memory=True, rechunk=True)\n                       for x in os.listdir(f\"{self.parent_path}/parquet_files/train\") if (criteria in x)],how='vertical_relaxed')\n        test_df  =  pl.concat([pl.scan_parquet(f\"{self.parent_path}/parquet_files/test/{x}\", low_memory=True, rechunk=True)\n                       for x in os.listdir(f\"{self.parent_path}/parquet_files/test\") if (criteria in x)],how='vertical_relaxed')\n        \n\n        # being in train partition doesnt gaurentee it is in the test partition, so we have to ensure it \n        columns_in_common = list(set(train_df.columns).intersection(set(test_df.columns)))\n\n        df = pl.concat([train_df.select(columns_in_common),\n                         test_df.select(columns_in_common)],how='vertical_relaxed') \n            \n        del train_df\n        del test_df \n        gc.collect()\n        \n        df = df.collect().pipe(self.set_table_dtypes).filter(pl.col('case_id').is_in(self.train_case_ids+self.test_case_ids))\n\n        return df\n        \n    def optimize_polars_df(self,df):\n        return reduce_polars_memory_usage(filter_cols(df))\n       \n        \n\n    def evaluate_features(self,df:pl.DataFrame,\n                          stability_scoring=False):\n        \"\"\"\n        1) calculates weight of evidence * information value for measuring predictive power\n        \n        \"\"\"\n        feats = [x for x in df.columns if x not in self.base_df_cols]\n\n        df = df.filter(pl.col(\"case_id\").is_in(self.train_case_ids))\n        n_row = len(self.df)\n        if 'target' not in df.columns:\n            df = df.join(self.df[['case_id','target']],on='case_id')\n        \n        # predictive power - woe*iv\n        woeivs  = []\n        for col in feats:\n            if df[col].dtype == pl.String:\n                woeiv = calculate_woe_iv_categorical(df[col].to_pandas(), df['target'].to_pandas())\n                woeivs.append(woeiv)\n            else:\n                woeiv = calculate_woe_iv_numeric(df[col].to_pandas(), df['target'].to_pandas())\n                woeivs.append(woeiv)\n        \n\n        feature_scores = pd.DataFrame(feats,columns=['feature'])\n        feature_scores['categorical'] = feature_scores['feature'].isin(self.string_cols)\n        feature_scores['prop_null'] = feature_scores['feature'].apply(lambda feat: df[feat].to_pandas().isna().sum()) / n_row\n        feature_scores['woe_iv'] = woeivs       \n    \n        return feature_scores\n        \n    \n    def select_features(self,df,score=\"woe_iv\",threshold=0.05,dedup_agg=False):\n        feature_scores = self.evaluate_features(df)\n        start_n = len(feature_scores)\n        \n        if dedup_agg:\n            feature_scores['feature_base_col'] = feature_scores['feature'].apply(extract_lowercase)\n            feature_scores = feature_scores.sort_values(['feature_base_col',score],ascending=[True,False]).drop_duplicates(subset=['feature_base_col'])\n        \n        chosen_features = feature_scores[feature_scores[score]>=threshold]['feature'].unique().tolist()\n        print(f\"selected {len(chosen_features)}/{start_n} features for the model dataset\")\n        del feature_scores\n        return chosen_features\n\n    \n    def to_pandas(self,df_data):\n        df_data = df_data.to_pandas()\n        cat_cols = [x for x in df_data.columns if (x in self.string_cols) and (x not in self.base_df_cols)]\n        df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n        df_data = self.reduce_pandas_mem_usage(df_data)\n        return df_data, cat_cols\n\n    def reduce_pandas_mem_usage(self,df):\n        \"\"\" iterate through all the columns of a dataframe and modify the data type\n            to reduce memory usage.        \n        \"\"\"\n        start_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage of pandas dataframe is {:.2f} MB'.format(start_mem))\n\n        for col in [x for x in df.columns if x not in self.base_df_cols]:\n            col_type = df[col].dtype\n            if str(col_type)==\"category\":\n                continue\n                \n            else:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n\n\n        end_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n        return df\n     \n    def process_depth0(self):\n        \"\"\"\n        These files can be used as is except for the dates, so just collect them, do feature engineering on the dates, then \n        throw out the date columns, grab top k features, join back to base\n        \"\"\"\n        depth0_criterias = [\"static_0\",\"static_cb_0\"]\n\n        for criteria in depth0_criterias:\n            df = self.read_in_files_with_criteria(criteria)\n            df = self.optimize_polars_df(df)\n            df = self.feature_engineer_dates(df)\n            depth0_feats = self.select_features(df,score=\"woe_iv\")\n            self.df = self.df.join(df[['case_id']+depth0_feats], on='case_id', how='left')   \n        \n        del df\n        gc.collect()\n\n    def process_depth1(self):\n        \"\"\"\n        These files have one group; collect them, auto aggregate, do feature engineering on the dates,\n        throw out the date columns, grab top k features, join back to base\n        \"\"\"\n        depth1_criterias = [\"applprev_1\",\"other_1\",\n                            \"tax_registry_a_1\",\"tax_registry_b_1\",\"tax_registry_c_1\",\n                            \"credit_bureau_a_1\",\"credit_bureau_b_1\",\n                            \"deposit_1\",\"person_1\",\"debitcard_1\"]\n        \n        # all groups\n        for criteria in depth1_criterias:\n            df = self.df[['case_id','target','date_decision']]\n            \n            criteria_df = self.read_in_files_with_criteria(criteria)\n            criteria_df = self.optimize_polars_df(criteria_df)\n            aggr = Aggregator([x for x in criteria_df.columns if x not in self.string_cols+self.date_cols+self.base_df_cols],\n                              self.string_cols,self.date_cols,\n                              f\"{criteria.upper()}_DEPTH1_ALL\")\n            agg_expr, agg_dt_cols, agg_str_cols = aggr.get_exprs(criteria_df)\n \n            criteria_df = criteria_df.group_by(\"case_id\").agg(agg_expr)\n            df = df.join(criteria_df, on=['case_id'], how='left')\n            df = self.feature_engineer_dates(df,date_cols=agg_dt_cols)    \n\n            feats = self.select_features(df,score=\"woe_iv\",dedup_agg=True)\n            \n            if len(feats)>0:\n                self.string_cols.extend([x for x in agg_str_cols if x in feats])\n                self.df = self.df.join(df[['case_id']+feats], on='case_id', how='left') \n \n            del criteria_df\n            del df\n            gc.collect()\n\n    def process_depth2(self):\n        \"\"\"\n        For now, just approach it like depth 2\n        \"\"\"\n        depth2_criterias = [\"applprev_2\",\"person_2\",\"credit_bureau_b_2\"] # \"credit_bureau_a_2\",\n        \n\n        for criteria in depth2_criterias:\n            df = self.df[['case_id','target','date_decision']]\n            # all groups\n            criteria_df = self.read_in_files_with_criteria(criteria)\n            criteria_df = self.optimize_polars_df(criteria_df)\n            aggr = Aggregator([x for x in criteria_df.columns if x not in self.string_cols+self.date_cols+self.base_df_cols],\n                              self.string_cols,self.date_cols,\n                              f\"{criteria.upper()}_DEPTH2_ALL\")\n            agg_expr, agg_dt_cols, agg_str_cols = aggr.get_exprs(criteria_df)\n \n            criteria_df = criteria_df.group_by(\"case_id\").agg(agg_expr)\n            df = df.join(criteria_df, on=['case_id'], how='left')\n            df = self.feature_engineer_dates(df,date_cols=agg_dt_cols)    \n            feats = self.select_features(df,score=\"woe_iv\",dedup_agg=True)\n            if len(feats)>0:\n                self.string_cols.extend([x for x in agg_str_cols if x in feats])\n                self.df = self.df.join(df[['case_id']+feats], on='case_id', how='left') \n \n            del criteria_df\n            del df\n            gc.collect()            \n            \n\n    def run(self):\n        self.create_base_dataset()\n        self.process_depth0()\n        self.process_depth1()\n        self.process_depth2()        \n        \n    def get_datasets(self):\n        df,cat_cols = self.to_pandas(self.df)\n\n        del self.df\n        gc.collect()\n        \n        return {\"train\":df[df['partition']=='train'].reset_index(drop=True), \n                \"test\": df[df['partition']=='test'].reset_index(drop=True), \n                \"features\": [x for x in df.columns if x not in self.base_df_cols],\n                \"cat_features\": cat_cols}\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T01:16:09.103999Z","iopub.execute_input":"2024-05-14T01:16:09.104448Z","iopub.status.idle":"2024-05-14T01:16:09.187709Z","shell.execute_reply.started":"2024-05-14T01:16:09.104414Z","shell.execute_reply":"2024-05-14T01:16:09.186542Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"ds = DatasetBuilder().get_datasets()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T01:16:09.189228Z","iopub.execute_input":"2024-05-14T01:16:09.189624Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Memory of polars dataframe went from 36.3986MB to 27.6629MB.\nprocessing criteria static_0...\ndropped column clientscnt_136L because too many nulls\ndropped column lastotherlnsexpense_631A because too many nulls\ndropped column lastdependentsnum_448L because too many nulls\ndropped column lastrepayingdate_696D because too many nulls\ndropped column payvacationpostpone_4187118D because too many nulls\ndropped column lastotherinc_902A because too many nulls\ndropped column equalityempfrom_62L because too many nulls\ndropped column isbidproductrequest_292L because too many nulls\ndropped column interestrategrace_34L because too many nulls\nMemory of polars dataframe went from 1715.8248MB to 976.2047MB.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ds['train'].shape)\nds['train']['target'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del DSBuilder\n# gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds['train']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training LGBM","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split,StratifiedGroupKFold\nimport lightgbm as lgb \nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\nfrom hyperopt.pyll import scope\nfrom functools import partial","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_base_params():\n    base_params = {\n        'boosting_type':'gbdt',\n        'random_state': 117,\n        'objective': 'binary',\n        'metric': 'auc',\n        'extra_trees':True,\n        'verbose': -1,\n        'max_bin': 64,\n        'device_type': 'gpu'\n        \n    }\n    return base_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_space_setup = {\n    'feature_fraction': hp.uniform('colsample_bynode', 0.3, 0.8),\n    'max_depth': scope.int(hp.uniform('max_depth', 5, 20)),\n    'l1_regularization': hp.loguniform('l1_regularization', np.log(.001), np.log(100)),\n    'l2_regularization':hp.loguniform('l2_regularization',np.log(.001), np.log(100)),\n    'cat_l2': hp.loguniform('cat_l2', np.log(.001), np.log(100)),\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.3, 0.8),\n    'bagging_freq': scope.int(hp.uniform('bagging_freq', 0, 5)),\n    'learning_rate' : hp.loguniform('learning_rate', np.log(0.001), np.log(.5)),\n    'n_estimators':scope.int(hp.uniform('n_estimators', 500, 1500)),\n    \n\n}\nsearch_space = get_base_params()\nfor k,v in search_space_setup.items():\n    search_space[k] = v","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do splits ahead of time to improve trial speed\nk = 5\n\nlgbtrain = lgb.Dataset(ds['train'].loc[:,ds['features']], label=ds['train'].loc[:,'target'])\ntest_X = ds['test'][ds['features']]\nskf = StratifiedGroupKFold(n_splits=k)\nsplits = skf.split(ds['train'].loc[:,ds['features']],ds['train']['target'],groups = ds['train']['WEEK_NUM'])\n\nsubmission = ds['test'][['case_id']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del ds\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trial_fn(params,\n             splits = None,\n             dataset = None):\n    \n    num_boost_round = params.pop('n_estimators')\n\n    cv_results = lgb.cv(\n        params,\n        dataset,\n        num_boost_round=num_boost_round,\n        folds=splits,\n        seed = 117,\n        callbacks = [lgb.early_stopping(20)]\n    ) \n    \n    print(cv_results)\n    score = np.mean(cv_results['auc-mean'])\n    return {\"status\": STATUS_OK, \"loss\": -score} # always minimizes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = fmin(fn=partial(trial_fn, splits = splits, dataset = lgbtrain),\n                    space=search_space,\n                    algo=tpe.suggest,\n                    max_evals=100,\n                    timeout=60*60*2 # seconds\n                  )\nint_params = ['max_depth','n_estimators','bagging_freq']\nbestp = get_base_params()\nfor k,v in best_params.items():\n    if k in int_params:\n        bestp[k] = int(v)\n    else:\n        bestp[k] = v\nbestp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbm = lgb.train(\n    n_estimators,\n    lgbtrain \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n","metadata":{}},{"cell_type":"code","source":"submission['score'] = mod.predict(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}