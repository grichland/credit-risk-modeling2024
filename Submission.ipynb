{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport polars as pl\nimport os, gc\nfrom sklearn.preprocessing import LabelEncoder\n\n\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 1000)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:32:12.251924Z","iopub.execute_input":"2024-05-10T21:32:12.252361Z","iopub.status.idle":"2024-05-10T21:32:13.291402Z","shell.execute_reply.started":"2024-05-10T21:32:12.252308Z","shell.execute_reply":"2024-05-10T21:32:13.290346Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def calculate_woe_iv_categorical(feature, response):\n    # Calculate the total number of events (positive responses) and non-events (negative responses)\n    total_events = response.sum()\n    total_non_events = response.count() - total_events\n    \n    # Create a new DataFrame with the feature and response values\n    df = pd.DataFrame({'bin': feature.fillna('missing'), 'response': response})\n    \n    # Calculate the percentage of events and non-events for each bin of the feature\n    bin_summary = df.groupby('bin')['response'].agg(['sum', 'count']).reset_index()\n    bin_summary.columns = ['bin', 'events', 'total']\n    bin_summary['non-events'] = (bin_summary['total'] - bin_summary['events']) \n    bin_summary['event_rate'] = (bin_summary['events'] / total_events)\n    bin_summary['non-event_rate'] = (bin_summary['non-events'] / total_non_events) + 1e-10 # epsilon so that that the non event rate is not 0\n\n    # Calculate the Weight of Evidence (WOE) and Information Value (IV) for each bin\n    bin_summary['WOE'] = np.log1p(bin_summary['event_rate'] / bin_summary['non-event_rate'])\n    bin_summary['IV'] = (bin_summary['event_rate'] - bin_summary['non-event_rate']) * bin_summary['WOE']\n\n    # # Calculate the total Information Value (IV) for the feature\n    total_IV = bin_summary['IV'].sum()\n    \n    return total_IV\n\ndef calculate_woe_iv_numeric(feature, response,quantiles = 50):\n    # Calculate the total number of events (positive responses) and non-events (negative responses)\n    total_events = response.sum()\n    total_non_events = response.count() - total_events\n    \n    # Create a new DataFrame with the feature and response values\n    df = pd.DataFrame({'feature': feature, 'response': response})\n    \n    # we want to support missing values\n    df['bin'] = -1\n    df.loc[df['feature'].notnull(),'bin'] = pd.qcut(df.loc[df['feature'].notnull(),'feature'], q=quantiles,duplicates='drop',labels=False)\n\n    del df['feature']\n    # Calculate the percentage of events and non-events for each bin of the feature\n    bin_summary = df.groupby('bin')['response'].agg(['sum', 'count']).reset_index()\n    bin_summary.columns = ['bin', 'events', 'total']\n    bin_summary['non-events'] = (bin_summary['total'] - bin_summary['events']) \n    bin_summary['event_rate'] = (bin_summary['events'] / total_events)\n    bin_summary['non-event_rate'] = (bin_summary['non-events'] / total_non_events) + 1e-10 # epsilon so that that the non event rate is not 0\n\n    # Calculate the Weight of Evidence (WOE) and Information Value (IV) for each bin\n    bin_summary['WOE'] = np.log1p(bin_summary['event_rate'] / bin_summary['non-event_rate'])\n    bin_summary['IV'] = (bin_summary['event_rate'] - bin_summary['non-event_rate']) * bin_summary['WOE']\n\n    # # Calculate the total Information Value (IV) for the feature\n    total_IV = bin_summary['IV'].sum()\n    \n    return total_IV\n\ndef calculate_psi_categorical(old,new): \n    # series 1 = old, series 2 = new\n    old = old.fillna(\"missing\")\n    new = new.fillna(\"missing\")    \n    \n    bins = list(set(old.tolist()+new.tolist())) \n    bin_summary = pd.DataFrame(bins,columns=['bin'])\n    bin_summary['prop_old'] = (bin_summary['bin'].apply(lambda x: (old==x).sum()) / len(old)) + 1e-10 # epsilon\n    bin_summary['prop_new'] = (bin_summary['bin'].apply(lambda x: (new==x).sum()) / len(new)) + 1e-10 # epsilon\n\n    \n    return np.sum((bin_summary['prop_old'] - bin_summary['prop_new']) * np.log(bin_summary['prop_old']/bin_summary['prop_new']))\n\ndef calculate_psi_numeric(old,new,q=10): \n\n    old = pd.DataFrame(old,columns=['val'])\n    new = pd.DataFrame(new,columns=['val'])\n    \n    # set up initial bins for missing values\n    old['bin'] = -1\n    new['bin'] = -1\n    \n    \n    # we will only generate a score if we have enough unique values\n    if (old['val'].dropna().nunique() > 1) and (new['val'].dropna().nunique() > 1):\n        # assign each value to a quantile \n        old.loc[old.notnull(),'bin'] = pd.qcut(old.loc[old.notnull(),'val'], q=quantiles,duplicates='drop',labels=False)\n        new.loc[new.notnull(),'bin'] = pd.qcut(new.loc[old.notnull(),'val'], q=quantiles,duplicates='drop',labels=False)\n        \n    \n        bins = list(set(old['bin'].tolist()+new['bin'].tolist())) \n        bin_summary = pd.DataFrame(bins,columns=['bin'])\n        bin_summary['prop_old'] = (bin_summary['bin'].apply(lambda x: (old['bin']==x).sum()) / len(old)) + 1e-10 # epsilon\n        bin_summary['prop_new'] = (bin_summary['bin'].apply(lambda x: (new['bin']==x).sum()) / len(new)) + 1e-10 # epsilon\n    \n        return np.sum((bin_summary['prop_old'] - bin_summary['prop_new']) * np.log(bin_summary['prop_old']/bin_summary['prop_new']))\n    \n    else:\n        return np.nan","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:32:13.293473Z","iopub.execute_input":"2024-05-10T21:32:13.293940Z","iopub.status.idle":"2024-05-10T21:32:13.313788Z","shell.execute_reply.started":"2024-05-10T21:32:13.293902Z","shell.execute_reply":"2024-05-10T21:32:13.312794Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"[Data Info](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data) <br>\n[Discussion on how the data is setup](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/473950) <br>\n[Starter Notebook](https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook)\n* depth=0 - These are static features directly tied to a specific case_id.\n* depth=1 - Each case_id has an associated historical record, indexed by num_group1.\n* depth=2 - Each case_id has an associated historical record, indexed by both num_group1 and num_group2.","metadata":{}},{"cell_type":"code","source":"class Aggregator:\n    # Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def __init__(self,numeric_cols,string_cols,date_cols,criteria):\n        self.numeric_cols = numeric_cols\n        self.string_cols  = string_cols\n        self.date_cols    = date_cols\n        self.criteria = criteria\n        \n    def num_expr(self,col):\n        \n        expr_max    = [pl.max(col).alias(f\"{col}_MAX\")]\n        expr_min    = [pl.min(col).alias(f\"{col}_MIN\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST\")]\n        expr_mean   = [pl.mean(col).alias(f\"{col}_MEAN\")]\n        expr_median = [pl.median(col).alias(f\"{col}_MEDIAN\")]\n        expr_var    = [pl.var(col).alias(f\"{col}_VAR\")]\n\n        return expr_max + expr_last + expr_mean + expr_median + expr_var + expr_min\n\n    def date_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST\")]\n        expr_mean   = [pl.mean(col).alias(f\"{col}_MEAN\")]\n\n        return expr_max + expr_last + expr_mean \n\n    def str_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST\")]\n        return expr_max + expr_last \n\n    def count_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n\n        return expr_max\n\n    def get_exprs(self,df):\n        expr = []\n        new_date_cols = []\n        for col in df.columns:\n            if 'num_group' in col:\n                expr.extend(self.count_expr(col))\n            elif col in self.numeric_cols:\n                expr.extend(self.num_expr(col))\n            elif col in self.string_cols:\n                expr.extend(self.str_expr(col))\n            elif col in self.date_cols:\n                new_date_cols.extend([f\"{col}_MAX\",f\"{col}_LAST\",f\"{col}_MEAN\"])\n                expr.extend(self.date_expr(col))\n\n        return expr, new_date_cols","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:32:13.315490Z","iopub.execute_input":"2024-05-10T21:32:13.316114Z","iopub.status.idle":"2024-05-10T21:32:13.330866Z","shell.execute_reply.started":"2024-05-10T21:32:13.316064Z","shell.execute_reply":"2024-05-10T21:32:13.329385Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns[7:]:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:32:13.333774Z","iopub.execute_input":"2024-05-10T21:32:13.334941Z","iopub.status.idle":"2024-05-10T21:32:13.348733Z","shell.execute_reply.started":"2024-05-10T21:32:13.334901Z","shell.execute_reply":"2024-05-10T21:32:13.347384Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class DatasetBuilder:\n    \"\"\" This class is used to create the dataset \"\"\"\n    def __init__(self, \n                 n_samples   = None, \n                 parent_path = \"/kaggle/input/home-credit-credit-risk-model-stability\"):\n        \n\n\n        self.parent_path = parent_path\n        self.n_samples = n_samples\n\n        self.feat_info = pd.read_csv(f\"{parent_path}/feature_definitions.csv\")\n        self.date_cols = []\n        self.string_cols = []\n        self.numeric_cols = []\n        \n        self.run()\n\n    def explain_feat(self,feat_name:str):\n        assert feat_name in self.feat_info['Variable'].unique(), \"feature not found in feature info dataframe\"\n        return self.feat_info[self.feat_info['Variable']==feat_name]['Description'].values[0]\n\n    def set_table_dtypes(self,df):\n        for col in df.columns:\n            \n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int32))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float32))\n                if col not in self.numeric_cols:\n                    self.numeric_cols.append(col)                \n            elif (col[-1] in (\"M\",)) or (col in self.string_cols):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n                if col not in self.string_cols:\n                    self.string_cols.append(col)\n            elif col[-1] in (\"L\",\"T\"): # we dont know the transform needed, just going to assume its either float and if not, then string\n                try:\n                    df = df.with_columns(pl.col(col).cast(pl.Float32))\n                    if col not in self.numeric_cols:\n                        self.numeric_cols.append(col) \n                except:\n                    df = df.with_columns(pl.col(col).cast(pl.String))\n                    if col not in self.string_cols:\n                        self.string_cols.append(col) \n                    continue\n                \n            elif col[-1] in (\"D\",) or (col in self.date_cols):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n                if col not in self.date_cols:\n                    self.date_cols.append(col)\n                \n        return df\n\n    def feature_engineer_dates(self,df,date_cols=None):\n        if date_cols is None:\n            date_cols = self.date_cols\n        for col in date_cols:\n            if col in df.columns:\n                df = df.with_columns((pl.col(\"date_decision\") - pl.col(col)).dt.total_days().alias(f'{col}_DAYS_SINCE'))  # days since\n                df = df.drop(col)\n        \n        return df\n    \n    def create_base_dataset(self):\n        \n        # load in the training dataset \n        if self.n_samples:\n            train = pl.read_parquet(f\"{self.parent_path}/parquet_files/train/train_base.parquet\") \\\n            .pipe(self.set_table_dtypes).sample(n=self.n_samples).with_columns(pl.lit('train').alias('partition'))\n        else:\n            train = pl.read_parquet(f\"{self.parent_path}/parquet_files/train/train_base.parquet\") \\\n            .pipe(self.set_table_dtypes).with_columns(pl.lit('train').alias('partition'))\n        \n        # load in the test dataset\n        test =  pl.read_parquet(f\"{self.parent_path}/parquet_files/test/test_base.parquet\")\\\n                .pipe(self.set_table_dtypes).with_columns(pl.lit('test').alias('partition'))\n        \n        # concat train and test\n        self.df = pl.concat([train,test],how='diagonal_relaxed')\n        \n        # get all case_ids\n        self.case_ids = self.df.get_column('case_id').to_list()\n        \n        # store base cols\n        self.base_df_cols = self.df.columns\n        \n\n    def read_in_files_with_criteria(self, criteria:str):\n        train_df  = pl.concat([pl.read_parquet(f\"{self.parent_path}/parquet_files/train/{x}\").pipe(self.set_table_dtypes).filter(pl.col('case_id').is_in(self.case_ids))\n                       for x in os.listdir(f\"{self.parent_path}/parquet_files/train\") if (criteria in x)],how='diagonal_relaxed')\n        test_df  =  pl.concat([pl.read_parquet(f\"{self.parent_path}/parquet_files/test/{x}\").pipe(self.set_table_dtypes)\n                       for x in os.listdir(f\"{self.parent_path}/parquet_files/test\") if (criteria in x)],how='diagonal_relaxed')\n        \n        # being in train partition doesnt gaurentee it is in the test partition, so we have to ensure it \n        columns_in_common = list(set(train_df.columns).intersection(test_df.columns))\n        \n        df = pl.concat([train_df.select(columns_in_common),\n                         test_df.select(columns_in_common)],how='diagonal_relaxed')\n        \n        \n        return df\n\n    \n    def process_depth0(self):\n        \"\"\"\n        These files can be used as is except for the dates, so just collect them, do feature engineering on the dates, then \n        throw out the date columns\n        \"\"\"\n        depth0_criterias = [\"static_0\",\"static_cb_0\"]\n        df = self.df[['case_id','target','date_decision']]\n        for criteria in depth0_criterias:\n            df = df.join(self.read_in_files_with_criteria(criteria), on=['case_id'], how='left')\n            \n        df = self.feature_engineer_dates(df)\n        depth0_feats = self.select_features(df,score=\"woe_iv\",top_k=100)\n        self.df = self.df.join(df[['case_id']+depth0_feats], on='case_id', how='left')\n\n\n    \n\n    def process_depth1(self):\n        depth1_criterias = [\"applprev_1\",\"other_1\",\n                            \"tax_registry_a_1\",\"tax_registry_b_1\",\"tax_registry_c_1\",\n                            \"credit_bureau_a_1\",\"credit_bureau_b_1\",\n                            \"deposit_1\",\"person_1\",\"debitcard_1\"]\n        df = self.df[['case_id','target','date_decision']]\n        agg_dt_cols_coll = []\n        for criteria in depth1_criterias:\n            criteria_df = self.read_in_files_with_criteria(criteria)\n            aggr = Aggregator(self.numeric_cols,self.string_cols,self.date_cols,criteria.upper())\n            agg_expr, agg_dt_cols = aggr.get_exprs(criteria_df)\n            agg_dt_cols_coll.extend(agg_dt_cols)\n            criteria_df = criteria_df.group_by(\"case_id\").agg(agg_expr)\n            df = df.join(criteria_df, on=['case_id'], how='inner')\n        \n        df = self.feature_engineer_dates(df,date_cols=agg_dt_cols_coll)\n        depth1_feats = self.select_features(df,score=\"woe_iv\",top_k=100)\n        self.df = self.df.join(df[['case_id']+depth1_feats], on='case_id', how='left')        \n        \n    \n    \n    def evaluate_features(self,df):\n        feats = [x for x in df.columns if x not in self.base_df_cols]\n        \n        # predictive power - woe*iv\n        woeivs  = []\n        for col in feats:\n            if col in self.string_cols:\n                woeiv = calculate_woe_iv_categorical(df[col].to_pandas(), df['target'].to_pandas())\n                woeivs.append(woeiv)\n            else:\n                woeiv = calculate_woe_iv_numeric(df[col].to_pandas(), df['target'].to_pandas())\n                woeivs.append(woeiv)\n\n        \n#         # stability - psi and woe*iv\n#         psi_res = {x:[] for x in feats}\n#         woe_res = {x:[] for x in feats}\n#         for i in range(len(year_months)-1):\n#             psis = []\n#             old = df.filter((pl.col(\"ym_decision\") == year_months[i]))\n#             new = df.filter((pl.col(\"ym_decision\") == year_months[i+1]))\n#             for col in feats:\n#                 if col in self.string_cols:\n#                     psi = calculate_psi_categorical(old[col].to_pandas(),new[col].to_pandas())\n#                 else:\n#                     psi = calculate_psi_numeric(old[col].to_pandas(),new[col].to_pandas())\n                    \n#                 psi_res[col].append(psi)\n                    \n\n\n            \n        feature_scores = pd.DataFrame(feats,columns=['feature'])\n        feature_scores['prop_null'] = feature_scores['feature'].apply(lambda feat: df[feat].to_pandas().isna().sum()) / len(self.df)\n        feature_scores['woe_iv'] = woeivs\n        \n        # lots of these ended up as nulls, will deal with later\n#         feature_scores['eligible_psi'] = feature_scores['feature'].apply(lambda feat: sum([0 if np.isnan(x) else 1 for x in psi_res[feat]]))\n#         feature_scores['avg_psi'] = feature_scores['feature'].apply(lambda feat: np.nanmean(psi_res[feat]))\n#         feature_scores['std_psi'] = feature_scores['feature'].apply(lambda feat: np.nanstd(psi_res[feat]))\n#         feature_scores['max_psi'] = feature_scores['feature'].apply(lambda feat: np.max(psi_res[feat]))\n\n        return feature_scores\n        \n    \n    def select_features(self,df,score=\"woe_iv\",top_k=150):\n        feature_scores = self.evaluate_features(df)\n        top_k = min(top_k,len(feature_scores)-1)\n        chosen_features = feature_scores.sort_values(score,ascending=False).reset_index(drop=True).loc[:top_k,'feature'].to_list()\n        print(f\"selected {top_k}/{len(feature_scores)} features for the model dataset\")\n        return chosen_features\n        \n    def run(self):\n        self.create_base_dataset()\n        self.process_depth0()\n        self.process_depth1()\n\n    \n    def to_pandas(self,df_data):\n        df_data = df_data.to_pandas()\n        df_data[self.string_cols] = df_data[self.string_cols].astype(\"category\")\n        \n#         enc = LabelEncoder()\n#         for col in self.string_cols:\n#             df_data[col] = enc.fit_transform(df_data[col])\n#             df_data[col] = df_data[col].astype(np.int8)\n        return df_data\n    \n    def get_datasets(self):\n        ds = self.to_pandas(self.df)\n        ds = reduce_mem_usage(ds)\n        return {\"train\":ds[ds['partition']=='train'].reset_index(drop=True), \n                \"test\": ds[ds['partition']=='test'].reset_index(drop=True), \n                \"features\": [x for x in ds.columns if x not in self.base_df_cols],\n                \"cat_features\": self.string_cols}","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:32:13.350561Z","iopub.execute_input":"2024-05-10T21:32:13.351252Z","iopub.status.idle":"2024-05-10T21:32:13.385642Z","shell.execute_reply.started":"2024-05-10T21:32:13.351220Z","shell.execute_reply":"2024-05-10T21:32:13.384197Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"DSBuilder = DatasetBuilder()\nds = DSBuilder.get_datasets()\ndel DSBuilder\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:32:13.387522Z","iopub.execute_input":"2024-05-10T21:32:13.388567Z","iopub.status.idle":"2024-05-10T21:36:54.354790Z","shell.execute_reply.started":"2024-05-10T21:32:13.388455Z","shell.execute_reply":"2024-05-10T21:36:54.352258Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"selected 100/219 features for the model dataset\nselected 100/975 features for the model dataset\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m DSBuilder \u001b[38;5;241m=\u001b[39m DatasetBuilder()\n\u001b[0;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mDSBuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m DSBuilder\n\u001b[1;32m      5\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n","Cell \u001b[0;32mIn[5], line 213\u001b[0m, in \u001b[0;36mDatasetBuilder.get_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_datasets\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 213\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     ds \u001b[38;5;241m=\u001b[39m reduce_mem_usage(ds)\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:ds[ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \n\u001b[1;32m    216\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: ds[ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \n\u001b[1;32m    217\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_df_cols],\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring_cols}\n","Cell \u001b[0;32mIn[5], line 204\u001b[0m, in \u001b[0;36mDatasetBuilder.to_pandas\u001b[0;34m(self, df_data)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m,df_data):\n\u001b[1;32m    203\u001b[0m         df_data \u001b[38;5;241m=\u001b[39m df_data\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m--> 204\u001b[0m         df_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring_cols] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m#         enc = LabelEncoder()\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m#         for col in self.string_cols:\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m#             df_data[col] = enc.fit_transform(df_data[col])\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m#             df_data[col] = df_data[col].astype(np.int8)\u001b[39;00m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m df_data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyError\u001b[0m: \"['bankacctype_710L', 'cardtype_51L', 'credtype_322L', 'disbursementtype_67L', 'inittransactioncode_186L', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastrejectcommodtypec_5251769M', 'paytype1st_925L', 'paytype_783L', 'twobodfilling_608L', 'typesuite_864L', 'description_5085714M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'cancelreason_3545846M', 'credacc_status_367L', 'credtype_587L', 'district_544M', 'education_1138M', 'familystate_726L', 'inittransactioncode_279L', 'postype_4733339M', 'profession_152M', 'rejectreason_755M', 'rejectreasonclient_4145042M', 'status_219L', 'name_4527232M', 'name_4917606M', 'employername_160M', 'classificationofcontr_13M', 'classificationofcontr_400M', 'contractst_545M', 'contractst_964M', 'description_351M', 'financialinstitution_382M', 'financialinstitution_591M', 'purposeofcred_426M', 'purposeofcred_874M', 'subjectrole_182M', 'subjectrole_93M', 'classificationofcontr_1114M', 'contractst_516M', 'contracttype_653M', 'credor_3940957M', 'periodicityofpmts_997L', 'periodicityofpmts_997M', 'pmtmethod_731M', 'purposeofcred_722M', 'subjectrole_326M', 'subjectrole_43M', 'contaddr_district_15M', 'contaddr_zipcode_807M', 'education_927M', 'empl_employedtotal_800L', 'empl_industry_691L', 'empladdr_district_926M', 'empladdr_zipcode_114M', 'familystate_447L', 'gender_992L', 'housetype_905L', 'housingtype_772L', 'incometype_1044T', 'language1_981M', 'maritalst_703L', 'registaddr_district_1083M', 'registaddr_zipcode_184M', 'relationshiptoclient_415T', 'relationshiptoclient_642T', 'role_1084L', 'role_993L', 'sex_738L', 'type_25L'] not in index\""],"ename":"KeyError","evalue":"\"['bankacctype_710L', 'cardtype_51L', 'credtype_322L', 'disbursementtype_67L', 'inittransactioncode_186L', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastrejectcommodtypec_5251769M', 'paytype1st_925L', 'paytype_783L', 'twobodfilling_608L', 'typesuite_864L', 'description_5085714M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'cancelreason_3545846M', 'credacc_status_367L', 'credtype_587L', 'district_544M', 'education_1138M', 'familystate_726L', 'inittransactioncode_279L', 'postype_4733339M', 'profession_152M', 'rejectreason_755M', 'rejectreasonclient_4145042M', 'status_219L', 'name_4527232M', 'name_4917606M', 'employername_160M', 'classificationofcontr_13M', 'classificationofcontr_400M', 'contractst_545M', 'contractst_964M', 'description_351M', 'financialinstitution_382M', 'financialinstitution_591M', 'purposeofcred_426M', 'purposeofcred_874M', 'subjectrole_182M', 'subjectrole_93M', 'classificationofcontr_1114M', 'contractst_516M', 'contracttype_653M', 'credor_3940957M', 'periodicityofpmts_997L', 'periodicityofpmts_997M', 'pmtmethod_731M', 'purposeofcred_722M', 'subjectrole_326M', 'subjectrole_43M', 'contaddr_district_15M', 'contaddr_zipcode_807M', 'education_927M', 'empl_employedtotal_800L', 'empl_industry_691L', 'empladdr_district_926M', 'empladdr_zipcode_114M', 'familystate_447L', 'gender_992L', 'housetype_905L', 'housingtype_772L', 'incometype_1044T', 'language1_981M', 'maritalst_703L', 'registaddr_district_1083M', 'registaddr_zipcode_184M', 'relationshiptoclient_415T', 'relationshiptoclient_642T', 'role_1084L', 'role_993L', 'sex_738L', 'type_25L'] not in index\"","output_type":"error"}]},{"cell_type":"markdown","source":"# Training XGBoost","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nimport xgboost as xgb\nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\nfrom hyperopt.pyll import scope\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:36:54.355925Z","iopub.status.idle":"2024-05-10T21:36:54.356359Z","shell.execute_reply.started":"2024-05-10T21:36:54.356147Z","shell.execute_reply":"2024-05-10T21:36:54.356164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:36:54.357642Z","iopub.status.idle":"2024-05-10T21:36:54.358039Z","shell.execute_reply.started":"2024-05-10T21:36:54.357848Z","shell.execute_reply":"2024-05-10T21:36:54.357864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_space = {\n    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),\n    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n    'gamma': hp.loguniform('gamma',np.log(.00001), np.log(100)),\n    'max_depth': scope.int(hp.uniform('max_depth', 5, 50)),\n    'min_child_weight': hp.loguniform('min_child_weight', np.log(.00001), np.log(100)),\n    'reg_alpha': hp.loguniform('reg_alpha', np.log(.00001), np.log(100)),\n    'reg_lambda':hp.loguniform('reg_lambda',np.log(.00001), np.log(100)),\n    'scale_pos_weight': hp.uniform('scale_pos_weight',1,10),\n    'subsample': hp.uniform('subsample', 0.5, 1),\n    'learning_rate' : hp.loguniform('learning_rate', np.log(.00001), np.log(.5)),\n    'n_estimators':scope.int(hp.uniform('n_estimators', 100, 1000)),\n    'max_cat_to_onehot': scope.int(hp.uniform('max_cat_to_onehot', 2, 10)),\n    'tree_method':'hist',\n    'enable_categorical':True,\n    'random_state': 185,\n    'objective': 'binary:logistic',\n#     'device': 'cuda',\n    'n_jobs': 10,\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:36:54.359077Z","iopub.status.idle":"2024-05-10T21:36:54.359477Z","shell.execute_reply.started":"2024-05-10T21:36:54.359263Z","shell.execute_reply":"2024-05-10T21:36:54.359279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trial_fn(params,\n             feats = [],\n             ds = [],\n             k_folds=4):\n    skf = StratifiedKFold(n_splits=k_folds)\n    idx = np.arange(len(ds))\n    ds['score'] = 0.0\n\n    for train_idx, valid_idx in skf.split(idx,ds['target']):\n        mod = xgb.XGBClassifier(**params)\n        mod.fit(ds.loc[train_idx,feats],ds.loc[train_idx,'target'])\n        ds.loc[valid_idx,'score']  = mod.predict_proba(ds.loc[valid_idx,feats])[:,1] # p(Y=1|X)\n\n\n    score = roc_auc_score(ds['target'],ds['score'])\n\n    return {\"status\": STATUS_OK, \"loss\": -score}","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:36:54.360699Z","iopub.status.idle":"2024-05-10T21:36:54.361067Z","shell.execute_reply.started":"2024-05-10T21:36:54.360887Z","shell.execute_reply":"2024-05-10T21:36:54.360904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = fmin(fn=partial(trial_fn, feats = ds['features'], ds = ds['train']),\n                    space=search_space,\n                    algo=tpe.suggest,\n                    max_evals=10,\n                    timeout=60*60 # seconds\n                  )\nint_params = ['max_depth','n_estimators']\nfor k,v in best_params.items():\n    best_params[k] = int(best_params[k])\n\nbest_params","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:36:54.362902Z","iopub.status.idle":"2024-05-10T21:36:54.363318Z","shell.execute_reply.started":"2024-05-10T21:36:54.363097Z","shell.execute_reply":"2024-05-10T21:36:54.363113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod = xgb.XGBClassifier(**best_params)\nmod.fit(ds['train'][ds['features']], ds['train']['target'])","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:36:54.365478Z","iopub.status.idle":"2024-05-10T21:36:54.366707Z","shell.execute_reply.started":"2024-05-10T21:36:54.366513Z","shell.execute_reply":"2024-05-10T21:36:54.366531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\n","metadata":{}},{"cell_type":"code","source":"ds['test']['score'] = mod.predict_proba(ds['test'][ds['features']])[:,1]","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:36:54.367721Z","iopub.status.idle":"2024-05-10T21:36:54.368662Z","shell.execute_reply.started":"2024-05-10T21:36:54.368465Z","shell.execute_reply":"2024-05-10T21:36:54.368483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = ds['test'][['case_id','score']]\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T21:36:54.369802Z","iopub.status.idle":"2024-05-10T21:36:54.370200Z","shell.execute_reply.started":"2024-05-10T21:36:54.370016Z","shell.execute_reply":"2024-05-10T21:36:54.370032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}