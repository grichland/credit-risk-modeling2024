{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 1000)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:40:34.497380Z","iopub.execute_input":"2024-05-04T22:40:34.499422Z","iopub.status.idle":"2024-05-04T22:40:36.227078Z","shell.execute_reply.started":"2024-05-04T22:40:34.499337Z","shell.execute_reply":"2024-05-04T22:40:36.225825Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def calculate_woe_iv_categorical(data, feature, target):\n    # Create a DataFrame for counts per category\n    df = data.groupby([feature])[target].agg(['count', 'sum'])\n    df.columns = ['Total', 'Bad']\n    \n    # Calculate the number of good outcomes\n    df['Good'] = df['Total'] - df['Bad']\n    \n    # Handle cases where the count is 0 to avoid division by zero in WoE calculation\n    df['Bad'] = np.where(df['Bad'] == 0, 0.0001, df['Bad'])\n    df['Good'] = np.where(df['Good'] == 0, 0.0001, df['Good'])\n    \n    # Calculate the percentage of bads and goods\n    df['Distr_Bad'] = df['Bad'] / df['Bad'].sum()\n    df['Distr_Good'] = df['Good'] / df['Good'].sum()\n    \n    # Calculate WoE\n    df['WoE'] = np.log(df['Distr_Good'] / df['Distr_Bad'])\n    \n    # Calculate IV\n    df['IV'] = (df['Distr_Good'] - df['Distr_Bad']) * df['WoE']\n    \n    # Sum the IV values for the feature\n    IV = df['IV'].sum()\n    \n    # Prepare a report\n    report = df.reset_index()[[feature, 'WoE', 'IV']]\n    \n    return IV, report\n\ndef calculate_woe_iv_numeric(data, feature, target, bins=10):\n    # Bin the data\n    data['binned'] = pd.qcut(data[feature], q=bins, duplicates='drop')\n\n    # Group by the binned feature\n    grouped = data.groupby('binned')[target].agg(['count', 'sum'])\n    grouped.columns = ['Total', 'Bad']\n\n    # Calculate the number of good outcomes\n    grouped['Good'] = grouped['Total'] - grouped['Bad']\n    \n    # Handle cases where the count is 0 to avoid division by zero in WoE calculation\n    grouped['Bad'] = np.where(grouped['Bad'] == 0, 0.0001, grouped['Bad'])\n    grouped['Good'] = np.where(grouped['Good'] == 0, 0.0001, grouped['Good'])\n\n    # Calculate the distribution of bads and goods\n    grouped['Distr_Bad'] = grouped['Bad'] / grouped['Bad'].sum()\n    grouped['Distr_Good'] = grouped['Good'] / grouped['Good'].sum()\n\n    # Calculate WoE\n    grouped['WoE'] = np.log(grouped['Distr_Good'] / grouped['Distr_Bad'])\n\n    # Calculate IV\n    grouped['IV'] = (grouped['Distr_Good'] - grouped['Distr_Bad']) * grouped['WoE']\n\n    # Sum the IV values for the feature\n    IV = grouped['IV'].sum()\n\n    # Prepare a report\n    report = grouped.reset_index()[['binned', 'WoE', 'IV']]\n\n    return IV, report","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-04T22:40:36.229342Z","iopub.execute_input":"2024-05-04T22:40:36.229997Z","iopub.status.idle":"2024-05-04T22:40:36.246044Z","shell.execute_reply.started":"2024-05-04T22:40:36.229952Z","shell.execute_reply":"2024-05-04T22:40:36.244635Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"[Data Info](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data)\n\n* depth=0 - These are static features directly tied to a specific case_id.\n* depth=1 - Each case_id has an associated historical record, indexed by num_group1.\n* depth=2 - Each case_id has an associated historical record, indexed by both num_group1 and num_group2.","metadata":{}},{"cell_type":"code","source":"class DatasetBuilder:\n    \"\"\" This class is used to create the dataset \"\"\"\n    def __init__(self, \n                 n_samples = None, \n                 partition = \"train\",\n                 parent_path = \"/kaggle/input/home-credit-credit-risk-model-stability\"):\n        \n        assert partition in [\"train\",\"test\"], \"partition can only be 'train','test' \"\n        \n        self.parent_path = parent_path\n        self.partition = partition\n        self.n_samples = n_samples\n\n        self.feat_info = pd.read_csv(f\"{parent_path}/feature_definitions.csv\")\n        self.date_cols = []\n        self.features = []\n        # run process\n        self.run()\n \n    def explain_feat(self,feat_name:str):\n        assert feat_name in self.feat_info['Variable'].unique(), \"feature not found in feature info dataframe\"\n        return self.feat_info[self.feat_info['Variable']==feat_name]['Description'].values[0]\n\n    def create_base_dataset(self):\n        if self.n_samples:\n            self.df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{self.partition}_base.csv\").sample(n=self.n_samples)\n        else:\n            self.df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{self.partition}_base.csv\")\n        \n        self.df['date_decision'] = pd.to_datetime(self.df['date_decision'])\n        self.df['MONTH'] = pd.to_datetime(self.df['MONTH'].astype(str).str[:4] + '-' + self.df['MONTH'].astype(str).str[-2:] +'-01')\n        self.base_cols = self.df.columns.tolist()\n        self.case_ids = self.df['case_id'].unique().tolist()\n    \n    def read_in_file(self, file_name:str):\n        df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n        df = df[df['case_id'].isin(self.case_ids)]\n        return df\n    \n    def add_df_to_dataset(self,df_to_add: pd.DataFrame,convert_types = True):\n        if convert_types:\n            for col in df_to_add.columns:\n                if (df_to_add[col].dtype == 'object'):\n                    if ('date' in col) or (col in ['dtlastpmtallstes_4499206D','firstclxcampaign_1125D']):\n                        self.date_cols.append(col)\n                        df_to_add[col] = pd.to_datetime(df_to_add[col])\n                    else:\n                        df_to_add[col] = df_to_add[col].astype(\"category\")\n        self.df = self.df.merge(df_to_add,on='case_id',how='left')\n        \n    def find_all_files_that_contain(self,criteria:str):\n        files = [x for x in os.listdir(f\"{self.parent_path}/csv_files/{self.partition}\") if (criteria in x) and (self.partition in x)]\n        return files \n    \n    def add_all_level0_files(self):\n        level0_criterias = [\"static_0\",\"static_cb_0\"]\n        for crit in level0_criterias:\n            df_to_concat = []\n            for file in self.find_all_files_that_contain(crit):\n                print(f\"adding {file}...\")\n                df_to_concat.append(self.read_in_file(file))\n            self.add_df_to_dataset(pd.concat(df_to_concat,axis=0))\n    \n\n    def make_level0_features(self):\n        print(\"making level0 features...\")\n        # features that can be used as is are already in numeric or categorical format\n        provided_feats = [col for col in self.df.columns[6:] if self.df[col].dtype in ['int64','float64','category']]\n            \n        # date transformations\n        ## number of days prior to the decision date, cannot be negative otherwise we wouldnt have this information when the decision was made \n        date_feats = []\n        for dt_col in self.date_cols:\n            new_col = f\"days_since_{dt_col}\"\n            dt_feat_series = (self.df['date_decision'] - self.df[dt_col]).dt.days\n            dt_feat_series = (dt_feat_series).mask(dt_feat_series < 0, np.nan)\n            self.df[new_col] = dt_feat_series\n            date_feats.append(new_col)\n        \n        self.features = provided_feats + date_feats\n\n        \n    def process_level1_files(self):\n        \n        print(\"adding previous applications...\")\n        # previous applications\n        ## gather all files and concat\n        appl_prev = pd.concat([self.read_in_file(file) for file in self.find_all_files_that_contain(\"applprev_1\")],axis=0)\n        ## only focus on the person for group 0 because that is the person we are making the decision on\n        ## additionally, only focus on the most recent application that existed prior to the case's decision date\n        ## it might be worth revisting this to look at all past applications because someone might be approved in a previous application but we are only going to\n        ## look at the most recent application\n        \n        appl_prev['creationdate_885D'] = pd.to_datetime(appl_prev['creationdate_885D'])\n        appl_prev = appl_prev.merge(self.df[['case_id','date_decision']],on='case_id')\n        appl_prev = appl_prev[  (appl_prev['creationdate_885D'] < appl_prev['date_decision']) \n                              & (appl_prev['num_group1'] == 0)].sort_values('creationdate_885D',ascending=False).drop_duplicates(subset=['case_id'])\n        ## convert the date columns to days since\n        for dt_col in [\"creationdate_885D\",\"approvaldate_319D\",\"dateactivated_425D\",\"employedfrom_700D\",\"firstnonzeroinstldate_307D\",\"dtlastpmt_581D\",\"dtlastpmtallstes_3545839D\"]:\n            new_col = f\"days_since_{dt_col}\"\n            dt_feat_series = (appl_prev['date_decision'] - pd.to_datetime(appl_prev[dt_col]) ).dt.days\n            appl_prev[new_col] = dt_feat_series\n            del appl_prev[dt_col]\n        ## change the names so it is clear that this is the most recent application\n        del appl_prev['num_group1']\n        del appl_prev['date_decision']\n        appl_prev.columns = ['case_id'] + [f\"{x}_MOST_RECENT_APPLICATION\" for x in appl_prev.columns[1:]]\n        ## add previous applications to the dataframe\n        self.features.extend(appl_prev.columns.tolist()[1:])\n        self.add_df_to_dataset(appl_prev)\n        ## free up memory\n        del appl_prev\n        \n        print(\"adding other...\")        \n        # other file\n        ## going to keep it real simple, just grab the first record for the person who we are making decision on\n        other = pd.concat([self.read_in_file(file) for file in self.find_all_files_that_contain(\"other_1\")],axis=0)\n        other = other[other['num_group1']==0].drop_duplicates(subset=['case_id'])\n        ## add other to the dataframe\n        self.features.extend(other.columns.tolist()[1:])\n        self.add_df_to_dataset(other)        \n        ## free up memory\n        del other       \n        \n        print(\"adding tax registry a...\")             \n        # tax registry a, look at individual and across all groups\n        tra = pd.concat([self.read_in_file(file) for file in self.find_all_files_that_contain(\"tax_registry_a\")],axis=0)\n        tra = tra.merge(self.df[['case_id','date_decision']],on='case_id')\n        tra['recorddate_4527225D'] = pd.to_datetime(tra['recorddate_4527225D'])\n        tra = tra[tra['recorddate_4527225D']<tra['date_decision']]\n        ## individual\n        individual_tra = tra[tra['num_group1']==0].drop(columns='num_group1')\n        for dt_col in [\"recorddate_4527225D\"]:\n            new_col = f\"days_since_{dt_col}\"\n            dt_feat_series = (individual_tra['date_decision'] - pd.to_datetime(individual_tra[dt_col]) ).dt.days\n            individual_tra[new_col] = dt_feat_series\n            del individual_tra[dt_col]\n        del individual_tra['date_decision']\n        individual_tra.columns = ['case_id'] + [f\"{x}_TRA_INDIV\" for x in individual_tra.columns[1:]]\n        ## add individual_tra to the dataframe\n        self.features.extend(individual_tra.columns.tolist()[1:])\n        self.add_df_to_dataset(individual_tra)        \n        ## free up memory\n        del individual_tra   \n        ## agg\n        agg_tra = tra.groupby('case_id',as_index=False).agg(\n            amount_4527230A_MIN = ('amount_4527230A','min'),\n            amount_4527230A_MAX = ('amount_4527230A','max'),\n            amount_4527230A_STD = ('amount_4527230A','std'),\n            amount_4527230A_AVG = ('amount_4527230A','mean'),\n            amount_4527230A_MEDIAN = ('amount_4527230A','median'),\n            amount_4527230A_SUM = ('amount_4527230A','sum'),    \n        )\n        self.features.extend(agg_tra.columns.tolist()[1:])\n        self.add_df_to_dataset(agg_tra)        \n        ## free up memory\n        del agg_tra        \n        \n        \n        print(\"adding tax registry b...\") \n        # tax registry b, look at individual and across all groups\n        trb = pd.concat([self.read_in_file(file) for file in self.find_all_files_that_contain(\"tax_registry_b\")],axis=0)\n        trb = trb.merge(self.df[['case_id','date_decision']],on='case_id')\n        trb['deductiondate_4917603D'] = pd.to_datetime(trb['deductiondate_4917603D'])\n        trb = trb[trb['deductiondate_4917603D']<trb['date_decision']]\n        ## individual\n        individual_trb = trb[trb['num_group1']==0].drop(columns='num_group1')\n        for dt_col in [\"deductiondate_4917603D\"]:\n            new_col = f\"days_since_{dt_col}\"\n            dt_feat_series = (individual_trb['date_decision'] - pd.to_datetime(individual_trb[dt_col]) ).dt.days\n            individual_trb[new_col] = dt_feat_series\n            del individual_trb[dt_col]\n        del individual_trb['date_decision']\n        individual_trb.columns = ['case_id'] + [f\"{x}_TRB_INDIV\" for x in individual_trb.columns[1:]]\n        ## add individual_trb to the dataframe\n        self.features.extend(individual_trb.columns.tolist()[1:])\n        self.add_df_to_dataset(individual_trb)        \n        ## free up memory\n        del individual_trb   \n        ## agg\n        agg_trb = trb.groupby('case_id',as_index=False).agg(\n            amount_4917619A_MIN = ('amount_4917619A','min'),\n            amount_4917619A_MAX = ('amount_4917619A','max'),\n            amount_4917619A_STD = ('amount_4917619A','std'),\n            amount_4917619A_AVG = ('amount_4917619A','mean'),\n            amount_4917619A_MEDIAN = ('amount_4917619A','median'),\n            amount_4917619A_SUM = ('amount_4917619A','sum'),    \n        )\n        self.features.extend(agg_trb.columns.tolist()[1:])\n        self.add_df_to_dataset(agg_trb)        \n        ## free up memory\n        del agg_trb              \n        \n        print(\"adding tax registry c...\")        \n        # tax registry c, look at individual and across all groups\n        trc = pd.concat([self.read_in_file(file) for file in self.find_all_files_that_contain(\"tax_registry_c\")],axis=0)\n        trc = trc.merge(self.df[['case_id','date_decision']],on='case_id')\n        trc['processingdate_168D'] = pd.to_datetime(trc['processingdate_168D'])\n        trc = trc[trc['processingdate_168D']<trc['date_decision']]\n        ## individual\n        individual_trc = trc[trc['num_group1']==0].drop(columns='num_group1')\n        for dt_col in [\"processingdate_168D\"]:\n            new_col = f\"days_since_{dt_col}\"\n            dt_feat_series = (individual_trc['date_decision'] - pd.to_datetime(individual_trc[dt_col]) ).dt.days\n            individual_trc[new_col] = dt_feat_series\n            del individual_trc[dt_col]\n        del individual_trc['date_decision']\n        individual_trc.columns = ['case_id'] + [f\"{x}_TRC_INDIV\" for x in individual_trc.columns[1:]]\n        ## add individual_trc to the dataframe\n        self.features.extend(individual_trc.columns.tolist()[1:])\n        self.add_df_to_dataset(individual_trc)        \n        ## free up memory\n        del individual_trc   \n        ## agg\n        agg_trc = trc.groupby('case_id',as_index=False).agg(\n            pmtamount_36A_MIN = ('pmtamount_36A','min'),\n            pmtamount_36A_MAX = ('pmtamount_36A','max'),\n            pmtamount_36A_STD = ('pmtamount_36A','std'),\n            pmtamount_36A_AVG = ('pmtamount_36A','mean'),\n            pmtamount_36A_MEDIAN = ('pmtamount_36A','median'),\n            pmtamount_36A_SUM = ('pmtamount_36A','sum'),    \n        )\n        self.features.extend(agg_trc.columns.tolist()[1:])\n        self.add_df_to_dataset(agg_trc)        \n        ## free up memory\n        del agg_trc            \n        \n        \n        \n        \n        \n        \n#         level1_criterias = [,\"other_\",\n#                             \"tax_registry_a\",\"tax_registry_b\",\"tax_registry_c\",\n#                             \"credit_bureau_a_1\",\"credit_bureau_b_1\",\n#                             \"deposit_1\",\"person_1\"]\n\n            \n        \n    def run(self):\n        self.create_base_dataset()\n        \n        self.add_all_level0_files()\n        self.make_level0_features()\n        \n        self.process_level1_files()\n    \n    def get_modeling_dataset(self):\n        return self.df[self.base_cols + self.features]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:40:36.247961Z","iopub.execute_input":"2024-05-04T22:40:36.248902Z","iopub.status.idle":"2024-05-04T22:40:36.296060Z","shell.execute_reply.started":"2024-05-04T22:40:36.248846Z","shell.execute_reply":"2024-05-04T22:40:36.295060Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_ds_builder = DatasetBuilder(n_samples = 10)\ntrain_ds = train_ds_builder.get_modeling_dataset()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:40:36.298237Z","iopub.execute_input":"2024-05-04T22:40:36.298883Z","iopub.status.idle":"2024-05-04T22:42:45.178407Z","shell.execute_reply.started":"2024-05-04T22:40:36.298845Z","shell.execute_reply":"2024-05-04T22:42:45.177046Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"adding train_static_0_0.csv...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_8507/4055596497.py:36: DtypeWarning: Columns (20,45,46,53,57,84,143,146,167) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"adding train_static_0_1.csv...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_8507/4055596497.py:36: DtypeWarning: Columns (20,45,46,56,57,84,143,146,167) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"adding train_static_cb_0.csv...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_8507/4055596497.py:36: DtypeWarning: Columns (1,2,3,4,7,45,46,47,48) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"making level0 features...\nadding previous applications...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_8507/4055596497.py:36: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n/tmp/ipykernel_8507/4055596497.py:36: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"adding other...\nadding tax registry a...\nadding tax registry b...\nadding tax registry c...\n","output_type":"stream"}]},{"cell_type":"code","source":"# test = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_applprev_1_0.csv\")\n# # test.head()\n\n# test ","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:42:45.180331Z","iopub.execute_input":"2024-05-04T22:42:45.180766Z","iopub.status.idle":"2024-05-04T22:42:45.185851Z","shell.execute_reply.started":"2024-05-04T22:42:45.180722Z","shell.execute_reply":"2024-05-04T22:42:45.184806Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Training LightGBM\n\nMinimal example of LightGBM training is shown below.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:42:45.186995Z","iopub.execute_input":"2024-05-04T22:42:45.187375Z","iopub.status.idle":"2024-05-04T22:42:48.048871Z","shell.execute_reply.started":"2024-05-04T22:42:45.187347Z","shell.execute_reply":"2024-05-04T22:42:48.047281Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_ds_builder = DatasetBuilder(partition=\"train\")\ntrain_ds = train_ds_builder.get_modeling_dataset()\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_ds,train_ds['target'],stratify=train_ds['target'],train_size=.8)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:42:48.050434Z","iopub.execute_input":"2024-05-04T22:42:48.050839Z","iopub.status.idle":"2024-05-04T22:45:25.504158Z","shell.execute_reply.started":"2024-05-04T22:42:48.050802Z","shell.execute_reply":"2024-05-04T22:45:25.503193Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"adding train_static_0_0.csv...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_8507/4055596497.py:36: DtypeWarning: Columns (20,45,46,53,57,84,143,146,167) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"adding train_static_0_1.csv...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_8507/4055596497.py:36: DtypeWarning: Columns (20,45,46,56,57,84,143,146,167) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"adding train_static_cb_0.csv...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_8507/4055596497.py:36: DtypeWarning: Columns (1,2,3,4,7,45,46,47,48) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"making level0 features...\nadding previous applications...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_8507/4055596497.py:36: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n/tmp/ipykernel_8507/4055596497.py:36: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"adding other...\nadding tax registry a...\nadding tax registry b...\nadding tax registry c...\n","output_type":"stream"}]},{"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train[train_ds_builder.features], label=y_train)\nlgb_valid = lgb.Dataset(X_valid[train_ds_builder.features], label=y_valid, reference=lgb_train)\n\nparams = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 3,\n    \"num_leaves\": 31,\n    \"learning_rate\": 0.01,\n    \"feature_fraction\": 0.9,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"n_estimators\": 1000,\n    \"verbose\": -1,\n}\n\ngbm = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=lgb_valid,\n    callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:58:38.293595Z","iopub.execute_input":"2024-05-04T22:58:38.294734Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 10 rounds\n[50]\tvalid_0's auc: 0.720335\n[100]\tvalid_0's auc: 0.73812\n[150]\tvalid_0's auc: 0.751975\n[200]\tvalid_0's auc: 0.76271\n[250]\tvalid_0's auc: 0.76905\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Evaluation with AUC and then comparison with the stability metric is shown below.","metadata":{}},{"cell_type":"code","source":"eval_train = X_train.copy()\neval_train['score'] = gbm.predict(X_train[train_ds_builder.features], num_iteration=gbm.best_iteration)\n\neval_valid = X_valid.copy()\neval_valid['score'] = gbm.predict(X_valid[train_ds_builder.features], num_iteration=gbm.best_iteration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score \ndef gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n\nstability_score_train = gini_stability(eval_train)\nstability_score_valid = gini_stability(eval_valid)\n\nprint(f'The stability score on the train set is: {stability_score_train}') \nprint(f'The stability score on the valid set is: {stability_score_valid}') \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\n","metadata":{}},{"cell_type":"code","source":"train_ds_builder = DatasetBuilder(partition=\"train\")\ntrain_ds = train_ds_builder.get_modeling_dataset()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = xgb_classifier.predict_proba(test_data)[:, 1]\ntest_Id = test_base_df[\"case_id\"]\nsubmission = pd.DataFrame({\n    'case_id': test_Id,\n    'score': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T21:27:23.798139Z","iopub.execute_input":"2024-02-07T21:27:23.798639Z","iopub.status.idle":"2024-02-07T21:27:23.946242Z","shell.execute_reply.started":"2024-02-07T21:27:23.798595Z","shell.execute_reply":"2024-02-07T21:27:23.944996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"case_id\": data_submission[\"case_id\"].to_numpy(),\n    \"score\": y_submission_pred\n}).set_index('case_id')\nsubmission.to_csv(\"./submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-07T21:27:23.947771Z","iopub.execute_input":"2024-02-07T21:27:23.948164Z","iopub.status.idle":"2024-02-07T21:27:23.96104Z","shell.execute_reply.started":"2024-02-07T21:27:23.948128Z","shell.execute_reply":"2024-02-07T21:27:23.959969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best of luck, and most importantly, enjoy the process of learning and discovery! \n\n<img src=\"https://i.imgur.com/obVWIBh.png\" alt=\"Image\" width=\"700\"/>","metadata":{}}]}