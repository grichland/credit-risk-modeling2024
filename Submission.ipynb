{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport polars as pl\nimport os, gc\nfrom sklearn.preprocessing import LabelEncoder\n\n\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 1000)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:05:30.111044Z","iopub.execute_input":"2024-05-13T22:05:30.112266Z","iopub.status.idle":"2024-05-13T22:05:30.118175Z","shell.execute_reply.started":"2024-05-13T22:05:30.112227Z","shell.execute_reply":"2024-05-13T22:05:30.117001Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def calculate_woe_iv_categorical(feature, response):\n    # Calculate the total number of events (positive responses) and non-events (negative responses)\n    total_events = response.sum()\n    total_non_events = response.count() - total_events\n    \n    # Create a new DataFrame with the feature and response values\n    df = pd.DataFrame({'bin': feature.fillna('missing'), 'response': response})\n    \n    # Calculate the percentage of events and non-events for each bin of the feature\n    bin_summary = df.groupby('bin')['response'].agg(['sum', 'count']).reset_index()\n    bin_summary.columns = ['bin', 'events', 'total']\n    bin_summary['non-events'] = (bin_summary['total'] - bin_summary['events']) \n    bin_summary['event_rate'] = (bin_summary['events'] / total_events)\n    bin_summary['non-event_rate'] = (bin_summary['non-events'] / total_non_events) + 1e-10 # epsilon so that that the non event rate is not 0\n\n    # Calculate the Weight of Evidence (WOE) and Information Value (IV) for each bin\n    bin_summary['WOE'] = np.log1p(bin_summary['event_rate'] / bin_summary['non-event_rate'])\n    bin_summary['IV'] = (bin_summary['event_rate'] - bin_summary['non-event_rate']) * bin_summary['WOE']\n\n    # # Calculate the total Information Value (IV) for the feature\n    total_IV = bin_summary['IV'].sum()\n    \n    return total_IV\n\ndef calculate_woe_iv_numeric(feature, response,quantiles = 50):\n    # Calculate the total number of events (positive responses) and non-events (negative responses)\n    total_events = response.sum()\n    total_non_events = response.count() - total_events\n    \n    # Create a new DataFrame with the feature and response values\n    df = pd.DataFrame({'feature': feature, 'response': response})\n    \n    # we want to support missing values\n    df['bin'] = -1\n    df.loc[df['feature'].notnull(),'bin'] = pd.qcut(df.loc[df['feature'].notnull(),'feature'], q=quantiles,duplicates='drop',labels=False)\n\n    del df['feature']\n    # Calculate the percentage of events and non-events for each bin of the feature\n    bin_summary = df.groupby('bin')['response'].agg(['sum', 'count']).reset_index()\n    bin_summary.columns = ['bin', 'events', 'total']\n    bin_summary['non-events'] = (bin_summary['total'] - bin_summary['events']) \n    bin_summary['event_rate'] = (bin_summary['events'] / total_events)\n    bin_summary['non-event_rate'] = (bin_summary['non-events'] / total_non_events) + 1e-10 # epsilon so that that the non event rate is not 0\n\n    # Calculate the Weight of Evidence (WOE) and Information Value (IV) for each bin\n    bin_summary['WOE'] = np.log1p(bin_summary['event_rate'] / bin_summary['non-event_rate'])\n    bin_summary['IV'] = (bin_summary['event_rate'] - bin_summary['non-event_rate']) * bin_summary['WOE']\n\n    # # Calculate the total Information Value (IV) for the feature\n    total_IV = bin_summary['IV'].sum()\n    \n    return total_IV\n\ndef calculate_psi_categorical(old,new): \n    old = old.to_frame().fillna('missing')\n    old.columns = ['bin']\n    new = new.to_frame().fillna('missing')\n    new.columns = ['bin']    \n    \n    old = old.groupby('bin').agg(count_old=('bin','count'))\n    new = new.groupby('bin').agg(count_new=('bin','count'))\n    \n    bins = list(set(old['bin'].tolist()+new['bin'].tolist())) \n    bin_summary = pd.DataFrame(bins,columns=['bin'])\n    bin_summary = bin_summary.merge(old[['bin','count_old']],on='bin',how='left')\n    bin_summary = bin_summary.merge(new[['bin','count_new']],on='bin',how='left')\n    bin_summary['prop_old'] = (bin_summary['count_old'].fillna(0) / len(old)) + 1e-10 # epsilon\n    bin_summary['prop_new'] = (bin_summary['count_new'].fillna(0) / len(new)) + 1e-10 # epsilon\n\n    return np.sum((bin_summary['prop_old'] - bin_summary['prop_new']) * np.log(bin_summary['prop_old']/bin_summary['prop_new']))\n\ndef calculate_psi_numeric(old,new,q=10): \n\n    old = old.to_frame()\n    old.columns = ['val']\n    new = new.to_frame()\n    new.columns = ['val']\n    \n    # set up initial bins for missing values\n    old['bin'] = -1\n    new['bin'] = -1\n    \n    \n    # return 0 in the event that theres less than 3 unique bin across both\n    if (old['val'].fillna(-9999).nunique() + new['val'].fillna(-9999).nunique()) <= 2:\n        return np.nan\n    else: \n        # assign each value to a quantile \n        old.loc[old['val'].notnull(),'bin'] = pd.qcut(old.loc[old['val'].notnull(),'val'], q=q,duplicates='drop',labels=False)\n        new.loc[new['val'].notnull(),'bin'] = pd.qcut(new.loc[new['val'].notnull(),'val'], q=q,duplicates='drop',labels=False)\n        \n        old = old.groupby('bin').agg(count_old=('bin','count'))\n        new = new.groupby('bin').agg(count_new=('bin','count'))\n        \n        \n        bins = list(set(old['bin'].tolist()+new['bin'].tolist())) \n        bin_summary = pd.DataFrame(bins,columns=['bin'])\n        bin_summary = bin_summary.merge(old[['bin','count_old']],on='bin',how='left')\n        bin_summary = bin_summary.merge(new[['bin','count_new']],on='bin',how='left')\n        bin_summary['prop_old'] = (bin_summary['count_old'].fillna(0) / len(old)) + 1e-10 # epsilon\n        bin_summary['prop_new'] = (bin_summary['count_new'].fillna(0) / len(new)) + 1e-10 # epsilon\n    \n        return np.sum((bin_summary['prop_old'] - bin_summary['prop_new']) * np.log(bin_summary['prop_old']/bin_summary['prop_new']))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:05:30.120352Z","iopub.execute_input":"2024-05-13T22:05:30.120775Z","iopub.status.idle":"2024-05-13T22:05:30.146556Z","shell.execute_reply.started":"2024-05-13T22:05:30.120737Z","shell.execute_reply":"2024-05-13T22:05:30.145246Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"[Data Info](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data) <br>\n[Discussion on how the data is setup](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/473950) <br>\n[Starter Notebook](https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook)\n* depth=0 - These are static features directly tied to a specific case_id.\n* depth=1 - Each case_id has an associated historical record, indexed by num_group1.\n* depth=2 - Each case_id has an associated historical record, indexed by both num_group1 and num_group2.","metadata":{}},{"cell_type":"code","source":"class Aggregator:\n    def __init__(self,numeric_cols,string_cols,date_cols,criteria):\n        self.numeric_cols = numeric_cols\n        self.string_cols  = string_cols\n        self.date_cols    = date_cols\n        self.criteria = criteria\n        \n    def num_expr(self,col):\n        \n        expr_max    = [pl.max(col).cast(pl.Float32).alias(f\"{col}_MAX_{self.criteria}\")]\n        expr_min    = [pl.min(col).cast(pl.Float32).alias(f\"{col}_MIN_{self.criteria}\")]\n        expr_last   = [pl.last(col).cast(pl.Float32).alias(f\"{col}_LAST_{self.criteria}\")]\n        expr_mean   = [pl.mean(col).cast(pl.Float32).alias(f\"{col}_MEAN_{self.criteria}\")]\n        expr_median = [pl.median(col).cast(pl.Float32).alias(f\"{col}_MEDIAN_{self.criteria}\")]\n        expr_var    = [pl.var(col).cast(pl.Float32).alias(f\"{col}_VAR_{self.criteria}\")]\n\n        return expr_max + expr_last + expr_mean + expr_median + expr_var + expr_min\n\n    def date_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST_{self.criteria}\")]\n        expr_mean   = [pl.mean(col).alias(f\"{col}_MEAN_{self.criteria}\")]\n\n        return expr_max + expr_last + expr_mean \n\n    def str_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n        expr_last   = [pl.last(col).alias(f\"{col}_LAST_{self.criteria}\")]\n        return expr_max + expr_last \n\n    def count_expr(self,col):\n        expr_max    = [pl.max(col).alias(f\"{col}_MAX_{self.criteria}\")]\n\n        return expr_max\n\n    def get_exprs(self,df):\n        expr = []\n        new_date_cols = []\n        new_str_cols = []\n        for col in df.columns:\n            if 'num_group' in col:\n                expr.extend(self.count_expr(col))\n            elif col in self.numeric_cols:\n                expr.extend(self.num_expr(col))\n            elif col in self.string_cols:\n                new_str_cols.extend([f\"{col}_MAX_{self.criteria}\",f\"{col}_LAST_{self.criteria}\"])\n                expr.extend(self.str_expr(col))\n            elif col in self.date_cols:\n                new_date_cols.extend([f\"{col}_MAX_{self.criteria}\",f\"{col}_LAST_{self.criteria}\",f\"{col}_MEAN_{self.criteria}\"])\n                expr.extend(self.date_expr(col))\n        \n        return expr, new_date_cols, new_str_cols","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:05:30.148576Z","iopub.execute_input":"2024-05-13T22:05:30.149181Z","iopub.status.idle":"2024-05-13T22:05:30.166323Z","shell.execute_reply.started":"2024-05-13T22:05:30.149141Z","shell.execute_reply":"2024-05-13T22:05:30.165097Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def filter_cols(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Filters columns in the DataFrame based on null percentage and unique values for string columns.\n\n    Args:\n    - df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n    - pl.DataFrame: DataFrame with filtered columns.\n    \"\"\"\n    for col in df.columns:\n        if col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]:\n            null_pct = df[col].is_null().mean()\n\n            if null_pct > 0.95:\n                df = df.drop(col)\n                print(f\"dropped column {col} because too many nulls\")\n    for col in df.columns:\n        if (col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]) & (\n            df[col].dtype == pl.String\n        ):\n            freq = df[col].n_unique()\n\n            if (freq > 200) | (freq == 1):\n                df = df.drop(col)\n                print(f\"dropped column {col} because of category size\")\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:05:30.213088Z","iopub.execute_input":"2024-05-13T22:05:30.213471Z","iopub.status.idle":"2024-05-13T22:05:30.220933Z","shell.execute_reply.started":"2024-05-13T22:05:30.213434Z","shell.execute_reply":"2024-05-13T22:05:30.220024Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def reduce_polars_memory_usage(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Reduces memory usage of a DataFrame by converting column types.\n\n    Args:\n    - df (pl.DataFrame): DataFrame to optimize.\n    - name (str): Name of the DataFrame.\n\n    Returns:\n    - pl.DataFrame: Optimized DataFrame.\n    \"\"\"\n    og_mem = round(df.estimated_size('mb'), 4)\n\n    int_types = [\n        pl.Int8,\n        pl.Int16,\n        pl.Int32,\n        pl.Int64,\n        pl.UInt8,\n        pl.UInt16,\n        pl.UInt32,\n        pl.UInt64,\n    ]\n    float_types = [pl.Float32, pl.Float64]\n\n    for col in df.columns:\n        if col == 'case_id':\n            continue\n        col_type = df[col].dtype\n        if col_type in int_types + float_types:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if c_min is not None and c_max is not None:\n                if col_type in int_types:\n                    if c_min >= 0:\n                        if (\n                            c_min >= np.iinfo(np.uint8).min\n                            and c_max <= np.iinfo(np.uint8).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt8))\n                        elif (\n                            c_min >= np.iinfo(np.uint16).min\n                            and c_max <= np.iinfo(np.uint16).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt16))\n                        elif (\n                            c_min >= np.iinfo(np.uint32).min\n                            and c_max <= np.iinfo(np.uint32).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt32))\n                        elif (\n                            c_min >= np.iinfo(np.uint64).min\n                            and c_max <= np.iinfo(np.uint64).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.UInt64))\n                    else:\n                        if (\n                            c_min >= np.iinfo(np.int8).min\n                            and c_max <= np.iinfo(np.int8).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int8))\n                        elif (\n                            c_min >= np.iinfo(np.int16).min\n                            and c_max <= np.iinfo(np.int16).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int16))\n                        elif (\n                            c_min >= np.iinfo(np.int32).min\n                            and c_max <= np.iinfo(np.int32).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int32))\n                        elif (\n                            c_min >= np.iinfo(np.int64).min\n                            and c_max <= np.iinfo(np.int64).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Int64))\n                elif col_type in float_types:\n                    if (\n                        c_min > np.finfo(np.float32).min\n                        and c_max < np.finfo(np.float32).max\n                    ):\n                        df = df.with_columns(df[col].cast(pl.Float32))\n\n    print(\n        f\"Memory of polars dataframe went from {og_mem}MB to {round(df.estimated_size('mb'), 4)}MB.\"\n    )\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:05:30.222695Z","iopub.execute_input":"2024-05-13T22:05:30.223220Z","iopub.status.idle":"2024-05-13T22:05:30.240058Z","shell.execute_reply.started":"2024-05-13T22:05:30.223188Z","shell.execute_reply":"2024-05-13T22:05:30.238874Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def extract_lowercase(s):\n    # Initialize an empty result string\n    result = \"\"\n\n    # Loop through each character in the string\n    for char in s:\n        # Check if the character is lowercase\n        if char.islower() or char == '_' or char.isnumeric():\n            result += char\n        # Break the loop if a non-lowercase character is encountered (if desired)\n        elif result:\n            break\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:05:30.241461Z","iopub.execute_input":"2024-05-13T22:05:30.242048Z","iopub.status.idle":"2024-05-13T22:05:30.257235Z","shell.execute_reply.started":"2024-05-13T22:05:30.242017Z","shell.execute_reply":"2024-05-13T22:05:30.256085Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class DatasetBuilder:\n    \"\"\" This class is used to create the dataset \"\"\"\n    def __init__(self, \n                 n_samples   = None, \n                 parent_path = \"/kaggle/input/home-credit-credit-risk-model-stability\",\n                ):\n        \n\n\n        self.parent_path = parent_path\n        self.n_samples = n_samples\n\n        self.feat_info = pd.read_csv(f\"{parent_path}/feature_definitions.csv\")\n        self.date_cols = []\n        self.string_cols = []\n        \n        self.run()\n\n    def explain_feat(self,feat_name:str):\n        assert feat_name in self.feat_info['Variable'].unique(), \"feature not found in feature info dataframe\"\n        return self.feat_info[self.feat_info['Variable']==feat_name]['Description'].values[0]\n\n    def set_table_dtypes(self,df):\n        for col in df.columns:\n                    \n            if col in [\"case_id\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int32))\n            elif col in  [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"target\"]:\n                df = df.with_columns(pl.col(col).cast(pl.UInt16))            \n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))                \n            elif (col[-1] in (\"M\",)) or (col in self.string_cols):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n                if col not in self.string_cols:\n                    self.string_cols.append(col)\n            elif col[-1] in (\"L\",\"T\"): # we dont know the transform needed, just going to assume its either float and if not, then string\n                try:\n                    df = df.with_columns(pl.col(col).cast(pl.Float64))\n                except:\n                    df = df.with_columns(pl.col(col).cast(pl.String))\n                    if col not in self.string_cols:\n                        self.string_cols.append(col) \n                    continue\n                \n            elif col[-1] in (\"D\",) or (col in self.date_cols):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n                if col not in self.date_cols:\n                    self.date_cols.append(col)\n        return df\n\n    def feature_engineer_dates(self,df,date_cols=None):\n        if date_cols is None:\n            date_cols = self.date_cols\n        if 'date_decision' not in df.columns:\n            df = df.join(self.df[['case_id','date_decision']],on='case_id')\n        for col in date_cols:\n            if col in df.columns:\n                df = df.with_columns((pl.col(\"date_decision\") - pl.col(col)).dt.total_days().alias(f'{col}_DAYS_SINCE'))\n                df = df.drop(col)\n\n        if 'date_decision' in df.columns:\n            df = df.drop('date_decision')\n            \n        return df\n  \n    \n    def create_base_dataset(self):\n        \n        # load in the training dataset \n        if self.n_samples is not None:\n            train = pl.read_parquet(f\"{self.parent_path}/parquet_files/train/train_base.parquet\") \\\n            .sample(n=self.n_samples).with_columns(pl.lit('train').alias('partition'))\n        else:\n            train = pl.read_parquet(f\"{self.parent_path}/parquet_files/train/train_base.parquet\") \\\n            .with_columns(pl.lit('train').alias('partition'))\n        \n        # load in the test dataset\n        test =  pl.read_parquet(f\"{self.parent_path}/parquet_files/test/test_base.parquet\")\\\n                .with_columns(pl.lit(0).alias('target'))\\\n                .with_columns(pl.lit('test').alias('partition'))        \n        \n        \n        \n        \n        \n        # concat train and test\n        self.df = reduce_polars_memory_usage(pl.concat([train,test],how='vertical_relaxed').pipe(self.set_table_dtypes))\n        \n        # get all case_ids\n        self.train_case_ids = train.get_column('case_id').to_list()\n        self.test_case_ids  = test.get_column('case_id').to_list()\n        \n        # store base cols\n        self.base_df_cols = self.df.columns\n        \n        del train\n        del test\n        gc.collect()\n\n    def read_in_files_with_criteria(self, criteria:str):\n        print(f\"processing criteria {criteria}...\")\n        train_df  = pl.concat([pl.scan_parquet(f\"{self.parent_path}/parquet_files/train/{x}\", low_memory=True, rechunk=True)\n                       for x in os.listdir(f\"{self.parent_path}/parquet_files/train\") if (criteria in x)],how='vertical_relaxed')\n        test_df  =  pl.concat([pl.scan_parquet(f\"{self.parent_path}/parquet_files/test/{x}\", low_memory=True, rechunk=True)\n                       for x in os.listdir(f\"{self.parent_path}/parquet_files/test\") if (criteria in x)],how='vertical_relaxed')\n        \n\n        # being in train partition doesnt gaurentee it is in the test partition, so we have to ensure it \n        columns_in_common = list(set(train_df.columns).intersection(set(test_df.columns)))\n        \n        df = pl.concat([train_df.select(columns_in_common),\n                         test_df.select(columns_in_common)],how='vertical_relaxed') \\\n            .collect().pipe(self.set_table_dtypes).filter(pl.col('case_id').is_in(self.train_case_ids+self.test_case_ids))\n        \n        del train_df\n        del test_df\n        gc.collect()\n        return df\n        \n    def optimize_polars_df(self,df):\n        return reduce_polars_memory_usage(filter_cols(df))\n       \n        \n\n    def evaluate_features(self,df:pl.DataFrame,\n                          stability_scoring=False):\n        \"\"\"\n        1) calculates weight of evidence * information value for measuring predictive power\n        \n        \"\"\"\n        feats = [x for x in df.columns if x not in self.base_df_cols]\n\n        df = df.filter(pl.col(\"case_id\").is_in(self.train_case_ids))\n        n_row = len(self.df)\n        if 'target' not in df.columns:\n            df = df.join(self.df[['case_id','target']],on='case_id')\n        \n        # predictive power - woe*iv\n        woeivs  = []\n        for col in feats:\n            if df[col].dtype == pl.String:\n                woeiv = calculate_woe_iv_categorical(df[col].to_pandas(), df['target'].to_pandas())\n                woeivs.append(woeiv)\n            else:\n                woeiv = calculate_woe_iv_numeric(df[col].to_pandas(), df['target'].to_pandas())\n                woeivs.append(woeiv)\n        \n\n        feature_scores = pd.DataFrame(feats,columns=['feature'])\n        feature_scores['categorical'] = feature_scores['feature'].isin(self.string_cols)\n        feature_scores['prop_null'] = feature_scores['feature'].apply(lambda feat: df[feat].to_pandas().isna().sum()) / n_row\n        feature_scores['woe_iv'] = woeivs       \n    \n        return feature_scores\n        \n    \n    def select_features(self,df,score=\"woe_iv\",threshold=0.01,dedup_agg=False):\n        feature_scores = self.evaluate_features(df)\n        start_n = len(feature_scores)\n        \n        if dedup_agg:\n            feature_scores['feature_base_col'] = feature_scores['feature'].apply(extract_lowercase)\n            feature_scores = feature_scores.sort_values(['feature_base_col',score],ascending=[True,False]).drop_duplicates(subset=['feature_base_col'])\n        \n        chosen_features = feature_scores[feature_scores[score]>=threshold]['feature'].unique().tolist()\n        print(f\"selected {len(chosen_features)}/{start_n} features for the model dataset\")\n        del feature_scores\n        return chosen_features\n\n    \n    def to_pandas(self,df_data):\n        df_data = df_data.to_pandas()\n        cat_cols = [x for x in df_data.columns if (x in self.string_cols) and (x not in self.base_df_cols)]\n        df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n        df_data = self.reduce_pandas_mem_usage(df_data)\n        return df_data, cat_cols\n\n    def reduce_pandas_mem_usage(self,df):\n        \"\"\" iterate through all the columns of a dataframe and modify the data type\n            to reduce memory usage.        \n        \"\"\"\n        start_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage of pandas dataframe is {:.2f} MB'.format(start_mem))\n\n        for col in [x for x in df.columns if x not in self.base_df_cols]:\n            col_type = df[col].dtype\n            if str(col_type)==\"category\":\n                continue\n                \n            else:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n\n\n        end_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n        return df\n     \n    def process_depth0(self):\n        \"\"\"\n        These files can be used as is except for the dates, so just collect them, do feature engineering on the dates, then \n        throw out the date columns, grab top k features, join back to base\n        \"\"\"\n        depth0_criterias = [\"static_0\",\"static_cb_0\"]\n\n        for criteria in depth0_criterias:\n            df = self.read_in_files_with_criteria(criteria)\n            df = self.optimize_polars_df(df)\n            df = self.feature_engineer_dates(df)\n            depth0_feats = self.select_features(df,score=\"woe_iv\")\n            self.df = self.df.join(df[['case_id']+depth0_feats], on='case_id', how='left')   \n        \n        del df\n        gc.collect()\n\n    def process_depth1(self):\n        \"\"\"\n        These files have one group; collect them, auto aggregate, do feature engineering on the dates,\n        throw out the date columns, grab top k features, join back to base\n        \"\"\"\n        depth1_criterias = [\"applprev_1\",\"other_1\",\n                            \"tax_registry_a_1\",\"tax_registry_b_1\",\"tax_registry_c_1\",\n                            \"credit_bureau_a_1\",\"credit_bureau_b_1\",\n                            \"deposit_1\",\"person_1\",\"debitcard_1\"]\n        \n        # all groups\n        for criteria in depth1_criterias:\n            df = self.df[['case_id','target','date_decision']]\n            \n            criteria_df = self.read_in_files_with_criteria(criteria)\n            criteria_df = self.optimize_polars_df(criteria_df)\n            aggr = Aggregator([x for x in criteria_df.columns if x not in self.string_cols+self.date_cols+self.base_df_cols],\n                              self.string_cols,self.date_cols,\n                              f\"{criteria.upper()}_DEPTH1_ALL\")\n            agg_expr, agg_dt_cols, agg_str_cols = aggr.get_exprs(criteria_df)\n \n            criteria_df = criteria_df.group_by(\"case_id\").agg(agg_expr)\n            df = df.join(criteria_df, on=['case_id'], how='left')\n            df = self.feature_engineer_dates(df,date_cols=agg_dt_cols)    \n\n            feats = self.select_features(df,score=\"woe_iv\",dedup_agg=True)\n            \n            if len(feats)>0:\n                self.string_cols.extend([x for x in agg_str_cols if x in feats])\n                self.df = self.df.join(df[['case_id']+feats], on='case_id', how='left') \n \n            del criteria_df\n            del df\n            gc.collect()\n\n    def process_depth2(self):\n        \"\"\"\n        For now, just approach it like depth 2\n        \"\"\"\n        depth2_criterias = [\"applprev_2\",\"person_2\",\"credit_bureau_b_2\"] # \"credit_bureau_a_2\",\n        \n\n        for criteria in depth2_criterias:\n            print(f\"processing criteria {criteria}...\")\n            df = self.df[['case_id','target','date_decision']]\n            # all groups\n            criteria_df = self.read_in_files_with_criteria(criteria)\n            criteria_df = self.optimize_polars_df(criteria_df)\n            aggr = Aggregator([x for x in criteria_df.columns if x not in self.string_cols+self.date_cols+self.base_df_cols],\n                              self.string_cols,self.date_cols,\n                              f\"{criteria.upper()}_DEPTH2_ALL\")\n            agg_expr, agg_dt_cols, agg_str_cols = aggr.get_exprs(criteria_df)\n \n            criteria_df = criteria_df.group_by(\"case_id\").agg(agg_expr)\n            df = df.join(criteria_df, on=['case_id'], how='left')\n            df = self.feature_engineer_dates(df,date_cols=agg_dt_cols)    \n            feats = self.select_features(df,score=\"woe_iv\",dedup_agg=True)\n            if len(feats)>0:\n                self.string_cols.extend([x for x in agg_str_cols if x in feats])\n                self.df = self.df.join(df[['case_id']+feats], on='case_id', how='left') \n \n            del criteria_df\n            del df\n            gc.collect()            \n            \n\n    def run(self):\n        self.create_base_dataset()\n        self.process_depth0()\n        self.process_depth1()\n        self.process_depth2()        \n        \n    def get_datasets(self):\n        df,cat_cols = self.to_pandas(self.df)\n\n        del self.df\n        gc.collect()\n        \n        return {\"train\":df[df['partition']=='train'].reset_index(drop=True), \n                \"test\": df[df['partition']=='test'].reset_index(drop=True), \n                \"features\": [x for x in df.columns if x not in self.base_df_cols],\n                \"cat_features\": cat_cols}\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:05:30.260158Z","iopub.execute_input":"2024-05-13T22:05:30.261021Z","iopub.status.idle":"2024-05-13T22:05:30.314800Z","shell.execute_reply.started":"2024-05-13T22:05:30.260956Z","shell.execute_reply":"2024-05-13T22:05:30.313395Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"ds = DatasetBuilder().get_datasets()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:05:30.316020Z","iopub.execute_input":"2024-05-13T22:05:30.316352Z","iopub.status.idle":"2024-05-13T22:13:42.900780Z","shell.execute_reply.started":"2024-05-13T22:05:30.316326Z","shell.execute_reply":"2024-05-13T22:13:42.899180Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Memory of polars dataframe went from 36.3986MB to 27.6629MB.\nprocessing criteria static_0...\ndropped column equalityempfrom_62L because too many nulls\ndropped column lastdependentsnum_448L because too many nulls\ndropped column payvacationpostpone_4187118D because too many nulls\ndropped column lastrepayingdate_696D because too many nulls\ndropped column isbidproductrequest_292L because too many nulls\ndropped column lastotherinc_902A because too many nulls\ndropped column maxannuity_4075009A because too many nulls\ndropped column interestrategrace_34L because too many nulls\ndropped column clientscnt_136L because too many nulls\ndropped column lastotherlnsexpense_631A because too many nulls\ndropped column previouscontdistrict_112M because of category size\ndropped column lastapprcommoditytypec_5251766M because of category size\nMemory of polars dataframe went from 1677.9578MB to 944.1615MB.\nselected 84/155 features for the model dataset\nprocessing criteria static_cb_0...\ndropped column foryear_850L because too many nulls\ndropped column foryear_818L because too many nulls\ndropped column forweek_528L because too many nulls\ndropped column foryear_618L because too many nulls\ndropped column for3years_504L because too many nulls\ndropped column for3years_584L because too many nulls\ndropped column forquarter_634L because too many nulls\ndropped column riskassesment_302T because too many nulls\ndropped column formonth_118L because too many nulls\ndropped column forquarter_462L because too many nulls\ndropped column dateofbirth_342D because too many nulls\ndropped column assignmentdate_4955616D because too many nulls\ndropped column formonth_206L because too many nulls\ndropped column pmtaverage_4955615A because too many nulls\ndropped column for3years_128L because too many nulls\ndropped column forquarter_1017L because too many nulls\ndropped column fortoday_1092L because too many nulls\ndropped column forweek_1077L because too many nulls\ndropped column forweek_601L because too many nulls\ndropped column pmtcount_4955617L because too many nulls\ndropped column formonth_535L because too many nulls\ndropped column riskassesment_940T because too many nulls\nMemory of polars dataframe went from 308.7586MB to 211.4523MB.\nselected 20/30 features for the model dataset\nprocessing criteria applprev_1...\ndropped column credacc_actualbalance_314A because too many nulls\ndropped column credacc_minhisbal_90A because too many nulls\ndropped column credacc_transactions_402L because too many nulls\ndropped column revolvingaccount_394A because too many nulls\ndropped column credacc_maxhisbal_375A because too many nulls\ndropped column credacc_status_367L because too many nulls\ndropped column profession_152M because of category size\ndropped column district_544M because of category size\nMemory of polars dataframe went from 1327.5042MB to 947.8599MB.\nselected 29/130 features for the model dataset\nprocessing criteria other_1...\nMemory of polars dataframe went from 2.2422MB to 1.2186MB.\nselected 0/31 features for the model dataset\nprocessing criteria tax_registry_a_1...\ndropped column name_4527232M because of category size\nMemory of polars dataframe went from 56.2323MB to 40.6122MB.\nselected 3/10 features for the model dataset\nprocessing criteria tax_registry_b_1...\ndropped column name_4917606M because of category size\nMemory of polars dataframe went from 19.0191MB to 13.736MB.\nselected 0/10 features for the model dataset\nprocessing criteria tax_registry_c_1...\ndropped column employername_160M because of category size\nMemory of polars dataframe went from 57.4001MB to 41.4557MB.\nselected 2/10 features for the model dataset\nprocessing criteria credit_bureau_a_1...\ndropped column instlamount_852A because too many nulls\ndropped column overdueamountmax2date_1142D because too many nulls\ndropped column numberofoverdueinstlmaxdat_641D because too many nulls\ndropped column annualeffectiverate_199L because too many nulls\ndropped column contractsum_5085717L because too many nulls\ndropped column prolongationcount_599L because too many nulls\ndropped column prolongationcount_1120L because too many nulls\ndropped column annualeffectiverate_63L because too many nulls\ndropped column interestrate_508L because too many nulls\ndropped column financialinstitution_591M because of category size\ndropped column classificationofcontr_400M because of category size\ndropped column financialinstitution_382M because of category size\ndropped column contractst_964M because of category size\nMemory of polars dataframe went from 7374.9528MB to 4516.9545MB.\nselected 42/327 features for the model dataset\nprocessing criteria credit_bureau_b_1...\ndropped column periodicityofpmts_997L because too many nulls\nMemory of polars dataframe went from 27.3313MB to 17.4315MB.\nselected 30/208 features for the model dataset\nprocessing criteria deposit_1...\nMemory of polars dataframe went from 3.0613MB to 2.3695MB.\nselected 0/13 features for the model dataset\nprocessing criteria person_1...\ndropped column childnum_185L because too many nulls\ndropped column gender_992L because too many nulls\ndropped column housetype_905L because too many nulls\ndropped column role_993L because too many nulls\ndropped column housingtype_772L because too many nulls\ndropped column birthdate_87D because too many nulls\ndropped column isreference_387L because too many nulls\ndropped column maritalst_703L because too many nulls\ndropped column empladdr_zipcode_114M because of category size\ndropped column registaddr_zipcode_184M because of category size\ndropped column empladdr_district_926M because of category size\ndropped column contaddr_district_15M because of category size\ndropped column registaddr_district_1083M because of category size\ndropped column contaddr_zipcode_807M because of category size\nMemory of polars dataframe went from 359.5869MB to 265.9914MB.\nselected 13/77 features for the model dataset\nprocessing criteria debitcard_1...\nMemory of polars dataframe went from 5.1755MB to 3.2253MB.\nselected 0/22 features for the model dataset\nprocessing criteria applprev_2...\nprocessing criteria applprev_2...\ndropped column credacc_cards_status_52L because too many nulls\nMemory of polars dataframe went from 355.6499MB to 328.8031MB.\nselected 3/6 features for the model dataset\nprocessing criteria person_2...\nprocessing criteria person_2...\ndropped column addres_role_871L because too many nulls\ndropped column relatedpersons_role_762T because too many nulls\ndropped column empls_employedfrom_796D because too many nulls\ndropped column addres_zip_823M because of category size\ndropped column addres_district_368M because of category size\ndropped column empls_employer_name_740M because of category size\nMemory of polars dataframe went from 37.8015MB to 34.6669MB.\nselected 0/6 features for the model dataset\nprocessing criteria credit_bureau_b_2...\nprocessing criteria credit_bureau_b_2...\nMemory of polars dataframe went from 34.6669MB to 22.3954MB.\nselected 5/17 features for the model dataset\nMemory usage of pandas dataframe is 1514.20 MB\nMemory usage after optimization is: 800.79 MB\nDecreased by 47.1%\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ds['train'].shape)\nds['train']['target'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:13:42.903550Z","iopub.execute_input":"2024-05-13T22:13:42.903914Z","iopub.status.idle":"2024-05-13T22:13:42.926253Z","shell.execute_reply.started":"2024-05-13T22:13:42.903887Z","shell.execute_reply":"2024-05-13T22:13:42.925426Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"(1526659, 237)\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"target\n0    0.968563\n1    0.031437\nName: proportion, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# del DSBuilder\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:13:42.927538Z","iopub.execute_input":"2024-05-13T22:13:42.928436Z","iopub.status.idle":"2024-05-13T22:13:42.940568Z","shell.execute_reply.started":"2024-05-13T22:13:42.928405Z","shell.execute_reply":"2024-05-13T22:13:42.939363Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Training XGBoost","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split,StratifiedGroupKFold\nimport xgboost as xgb\nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\nfrom hyperopt.pyll import scope\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:13:42.941967Z","iopub.execute_input":"2024-05-13T22:13:42.942335Z","iopub.status.idle":"2024-05-13T22:13:44.304260Z","shell.execute_reply.started":"2024-05-13T22:13:42.942304Z","shell.execute_reply":"2024-05-13T22:13:44.303029Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:13:44.305564Z","iopub.execute_input":"2024-05-13T22:13:44.306100Z","iopub.status.idle":"2024-05-13T22:13:44.314353Z","shell.execute_reply.started":"2024-05-13T22:13:44.305872Z","shell.execute_reply":"2024-05-13T22:13:44.313115Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def get_base_params():\n    base_params = {\n        'max_cat_to_onehot': 4,\n#         'max_delta_step':1,\n        'random_state': 117,\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n\n        # turn on when gpu \n        'device': 'cuda',   \n\n        # turn off when gpu\n    #     'n_jobs': 10,\n    #     'tree_method':'hist',\n    }\n    return base_params","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:13:44.317562Z","iopub.execute_input":"2024-05-13T22:13:44.317952Z","iopub.status.idle":"2024-05-13T22:13:44.328731Z","shell.execute_reply.started":"2024-05-13T22:13:44.317921Z","shell.execute_reply":"2024-05-13T22:13:44.327526Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"search_space_setup = {\n    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.6, 0.9), # col_sample reduces correlation b/c cols to a limit and reduces computation\n    'colsample_bynode': hp.uniform('colsample_bynode', 0.6, 0.9),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 0.9),\n#     'gamma': hp.loguniform('gamma',np.log(0.00001), np.log(100)),\n    'max_depth': scope.int(hp.uniform('max_depth', 2, 20)),\n#     'min_child_weight': hp.loguniform('min_child_weight', np.log(0.00001), np.log(100)),\n    'reg_alpha': hp.loguniform('reg_alpha', np.log(.00001), np.log(100)),\n    'reg_lambda':hp.loguniform('reg_lambda',np.log(.00001), np.log(100)),\n    'scale_pos_weight': hp.uniform('scale_pos_weight',1, 20),\n    'subsample': hp.uniform('subsample', 0.6, 0.9),\n    'learning_rate' : hp.loguniform('learning_rate', np.log(0.00001), np.log(.5)),\n    'n_estimators':scope.int(hp.uniform('n_estimators', 500, 1500)),\n\n}\nsearch_space = get_base_params()\nfor k,v in search_space_setup.items():\n    search_space[k] = v","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:13:44.330254Z","iopub.execute_input":"2024-05-13T22:13:44.330955Z","iopub.status.idle":"2024-05-13T22:13:44.340927Z","shell.execute_reply.started":"2024-05-13T22:13:44.330925Z","shell.execute_reply":"2024-05-13T22:13:44.339700Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# do splits ahead of time to improve trial speed\nk = 5\n\n\n# strat group k\nskf = StratifiedGroupKFold(n_splits=k)\nidx = np.arange(len(ds['train']))\nds_train_dmatrix_splits = []\nfor train_idx, valid_idx in skf.split(idx,ds['train']['target'],groups = ds['train']['WEEK_NUM']): \n    dtrain = xgb.DMatrix(ds['train'].loc[train_idx,ds['features']], label=ds['train'].loc[train_idx,'target'],enable_categorical=True)\n    dvalid = xgb.DMatrix(ds['train'].loc[valid_idx,ds['features']], label=ds['train'].loc[valid_idx,'target'],enable_categorical=True)\n    ds_train_dmatrix_splits.append((dtrain,dvalid))\n    \n    \n    \n# week num\n# ds_train_dmatrix_splits = []\n# unique_weeknums = ds['train']['WEEK_NUM'].sort_values().unique()\n# for i in range(1,k+1): \n#     train_idx, valid_idx = ds['train']['WEEK_NUM'] <= unique_weeknums[-int(i*4)]\n#     dtrain = xgb.DMatrix(ds['train'].loc[train_idx,ds['features']], label=ds['train'].loc[train_idx,'target'],enable_categorical=True)\n#     dvalid = xgb.DMatrix(ds['train'].loc[valid_idx,ds['features']], label=ds['train'].loc[valid_idx,'target'],enable_categorical=True)\n#     ds_train_dmatrix_splits.append((dtrain,dvalid))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:13:44.341864Z","iopub.execute_input":"2024-05-13T22:13:44.342187Z","iopub.status.idle":"2024-05-13T22:14:28.388136Z","shell.execute_reply.started":"2024-05-13T22:13:44.342162Z","shell.execute_reply":"2024-05-13T22:14:28.386524Z"},"trusted":true},"execution_count":24,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m ds_train_dmatrix_splits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, valid_idx \u001b[38;5;129;01min\u001b[39;00m skf\u001b[38;5;241m.\u001b[39msplit(idx,ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m],groups \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWEEK_NUM\u001b[39m\u001b[38;5;124m'\u001b[39m]): \n\u001b[0;32m---> 10\u001b[0m     dtrain \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     dvalid \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[valid_idx,ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]], label\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[valid_idx,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m],enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m     ds_train_dmatrix_splits\u001b[38;5;241m.\u001b[39mappend((dtrain,dvalid))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:857\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m handle, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_data_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnthread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/data.py:1089\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[0;32m-> 1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_cudf_df(data) \u001b[38;5;129;01mor\u001b[39;00m _is_cudf_ser(data):\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_cudf_df(\n\u001b[1;32m   1094\u001b[0m         data, missing, threads, feature_names, feature_types, enable_categorical\n\u001b[1;32m   1095\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/data.py:522\u001b[0m, in \u001b[0;36m_from_pandas_df\u001b[0;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_pandas_df\u001b[39m(\n\u001b[1;32m    515\u001b[0m     data: DataFrame,\n\u001b[1;32m    516\u001b[0m     enable_categorical: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    521\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DispatchedDataBackendReturnType:\n\u001b[0;32m--> 522\u001b[0m     data, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_numpy_array(data, missing, nthread, feature_names, feature_types)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/data.py:498\u001b[0m, in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, enable_categorical, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[1;32m    492\u001b[0m         pyarrow_extension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    494\u001b[0m feature_names, feature_types \u001b[38;5;241m=\u001b[39m pandas_feature_info(\n\u001b[1;32m    495\u001b[0m     data, meta, feature_names, feature_types, enable_categorical\n\u001b[1;32m    496\u001b[0m )\n\u001b[0;32m--> 498\u001b[0m transformed \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_cat_null\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pyarrow_extension:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transformed \u001b[38;5;129;01mis\u001b[39;00m data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/data.py:436\u001b[0m, in \u001b[0;36mpandas_cat_null\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ser\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39m__arrow_array__()\u001b[38;5;241m.\u001b[39mcombine_chunks()\u001b[38;5;241m.\u001b[39mdictionary_encode()\u001b[38;5;241m.\u001b[39mindices\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cat_columns:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# DF doesn't have the cat attribute, as a result, we use apply here\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     transformed[cat_columns] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 436\u001b[0m         \u001b[43mtransformed\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcat_columns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;241m.\u001b[39mapply(cat_codes)\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, np\u001b[38;5;241m.\u001b[39mNaN)\n\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nul_columns:\n\u001b[1;32m    442\u001b[0m     transformed[nul_columns] \u001b[38;5;241m=\u001b[39m transformed[nul_columns]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4117\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   4115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 4117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[1;32m   4120\u001b[0m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[1;32m   4121\u001b[0m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[1;32m   4122\u001b[0m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[1;32m   4123\u001b[0m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[1;32m   4124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n\u001b[1;32m   4125\u001b[0m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[1;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[1;32m   4131\u001b[0m     )\n\u001b[0;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4140\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[1;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    689\u001b[0m             indexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    696\u001b[0m     ]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:818\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    816\u001b[0m deep \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m (only_slice \u001b[38;5;129;01mor\u001b[39;00m using_copy_on_write())\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mgr_loc \u001b[38;5;129;01min\u001b[39;00m mgr_locs:\n\u001b[0;32m--> 818\u001b[0m     newblk \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m     newblk\u001b[38;5;241m.\u001b[39mmgr_locs \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(mgr_loc, mgr_loc \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    820\u001b[0m     blocks\u001b[38;5;241m.\u001b[39mappend(newblk)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:796\u001b[0m, in \u001b[0;36mBlock.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    794\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 796\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"def trial_fn(params,\n             splits = []):\n\n\n    n_estimators = params.pop('n_estimators')\n    scores = [] \n    for dtrain, dvalid in splits: \n        mod = xgb.train(params,dtrain, n_estimators)\n        score = roc_auc_score(dvalid.get_label(),mod.predict(dvalid))\n        scores.append(score)\n    \n    score = np.mean(scores) \n\n    return {\"status\": STATUS_OK, \"loss\": -score} # always minimizes","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:14:28.389550Z","iopub.status.idle":"2024-05-13T22:14:28.390355Z","shell.execute_reply.started":"2024-05-13T22:14:28.390114Z","shell.execute_reply":"2024-05-13T22:14:28.390134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = fmin(fn=partial(trial_fn, splits = ds_train_dmatrix_splits),\n                    space=search_space,\n                    algo=tpe.suggest,\n                    max_evals=100,\n                    timeout=60*60 # seconds\n                  )\nint_params = ['max_depth','n_estimators','max_cat_to_onehot']\nbestp = get_base_params()\nfor k,v in best_params.items():\n    if k in int_params:\n        bestp[k] = int(v)\n    else:\n        bestp[k] = v\nbestp","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:14:28.391733Z","iopub.status.idle":"2024-05-13T22:14:28.392488Z","shell.execute_reply.started":"2024-05-13T22:14:28.392266Z","shell.execute_reply":"2024-05-13T22:14:28.392286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_estimators = bestp.pop('n_estimators')\ndtrain = xgb.DMatrix(ds['train'][ds['features']], label=ds['train']['target'],enable_categorical=True)\nmod = xgb.train(bestp,dtrain, n_estimators)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:14:28.393582Z","iopub.status.idle":"2024-05-13T22:14:28.394508Z","shell.execute_reply.started":"2024-05-13T22:14:28.394302Z","shell.execute_reply":"2024-05-13T22:14:28.394319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n","metadata":{}},{"cell_type":"code","source":"del ds['train']\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:14:28.395819Z","iopub.status.idle":"2024-05-13T22:14:28.396537Z","shell.execute_reply.started":"2024-05-13T22:14:28.396327Z","shell.execute_reply":"2024-05-13T22:14:28.396345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtest = xgb.DMatrix(ds['test'][ds['features']], enable_categorical=True)\nds['test']['score'] = mod.predict(dtest)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:14:28.397898Z","iopub.status.idle":"2024-05-13T22:14:28.398574Z","shell.execute_reply.started":"2024-05-13T22:14:28.398366Z","shell.execute_reply":"2024-05-13T22:14:28.398384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = ds['test'][['case_id','score']]\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:14:28.399794Z","iopub.status.idle":"2024-05-13T22:14:28.400435Z","shell.execute_reply.started":"2024-05-13T22:14:28.400246Z","shell.execute_reply":"2024-05-13T22:14:28.400264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}