{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 1000)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:04:40.645778Z","iopub.execute_input":"2024-05-04T20:04:40.646190Z","iopub.status.idle":"2024-05-04T20:04:40.652696Z","shell.execute_reply.started":"2024-05-04T20:04:40.646155Z","shell.execute_reply":"2024-05-04T20:04:40.651166Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"# Load Datasets","metadata":{}},{"cell_type":"markdown","source":"[Data Info](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data)\n\n* depth=0 - These are static features directly tied to a specific case_id.\n* depth=1 - Each case_id has an associated historical record, indexed by num_group1.\n* depth=2 - Each case_id has an associated historical record, indexed by both num_group1 and num_group2.","metadata":{}},{"cell_type":"code","source":"class DatasetBuilder:\n    \"\"\" This class is used to create the dataset \"\"\"\n    def __init__(self, \n                 n_samples = None, \n                 partition = \"train\",\n                 parent_path = \"/kaggle/input/home-credit-credit-risk-model-stability\"):\n        \n        assert partition in [\"train\",\"test\"], \"partition can only be 'train','test' \"\n        \n        self.parent_path = parent_path\n        self.partition = partition\n        self.n_samples = n_samples\n\n        self.feat_info = pd.read_csv(f\"{parent_path}/feature_definitions.csv\")\n        self.date_cols = []\n        self.features = []\n        # run process\n        self.run()\n \n    def explain_feat(self,feat_name:str):\n        assert feat_name in self.feat_info['Variable'].unique(), \"feature not found in feature info dataframe\"\n        return self.feat_info[self.feat_info['Variable']==feat_name]['Description'].values[0]\n\n    def create_base_dataset(self):\n        if self.n_samples:\n            self.df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{self.partition}_base.csv\").sample(n=self.n_samples)\n        else:\n            self.df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{self.partition}_base.csv\")\n        \n        self.df['date_decision'] = pd.to_datetime(self.df['date_decision'])\n        self.df['MONTH'] = pd.to_datetime(self.df['MONTH'].astype(str).str[:4] + '-' + self.df['MONTH'].astype(str).str[-2:] +'-01')\n        self.base_cols = self.df.columns.tolist()\n        self.case_ids = self.df['case_id'].unique().tolist()\n    \n    def read_in_file(self, file_name:str):\n        df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n        df = df[df['case_id'].isin(self.case_ids)]\n        return df\n    \n    def add_df_to_dataset(self,df_to_add: pd.DataFrame,convert_types = True):\n        if convert_types:\n            for col in df_to_add.columns:\n                if (df_to_add[col].dtype == 'object'):\n                    if ('date' in col) or (col in ['dtlastpmtallstes_4499206D','firstclxcampaign_1125D']):\n                        self.date_cols.append(col)\n                        df_to_add[col] = pd.to_datetime(df_to_add[col])\n                    else:\n                        df_to_add[col] = df_to_add[col].astype(\"category\")\n        self.df = self.df.merge(df_to_add,on='case_id',how='left')\n        \n    def find_all_files_that_contain(self,criteria:str):\n        files = [x for x in os.listdir(f\"{self.parent_path}/csv_files/{self.partition}\") if (criteria in x) and (self.partition in x)]\n        return files \n    \n    def add_all_level0_files(self):\n        level0_criterias = [\"static_0\",\"static_cb_0\"]\n        for crit in level0_criterias:\n            df_to_concat = []\n            for file in self.find_all_files_that_contain(crit):\n                print(f\"adding {file}...\")\n                df_to_concat.append(self.read_in_file(file))\n            self.add_df_to_dataset(pd.concat(df_to_concat,axis=0))\n    \n\n    def make_level0_features(self):\n        print(\"making level0 features...\")\n        # features that can be used as is are already in numeric or categorical format\n        provided_feats = [col for col in self.df.columns[6:] if self.df[col].dtype in ['int64','float64','category']]\n            \n        # date transformations\n        ## number of days prior to the decision date, cannot be negative otherwise we wouldnt have this information when the decision was made \n        date_feats = []\n        for dt_col in self.date_cols:\n            new_col = f\"days_since_{dt_col}\"\n            dt_feat_series = (self.df['date_decision'] - self.df[dt_col]).dt.days\n            dt_feat_series = (dt_feat_series).mask(dt_feat_series < 0, np.nan)\n            self.df[new_col] = dt_feat_series\n            date_feats.append(new_col)\n        \n        self.features = provided_feats + date_feats\n\n        \n    def process_level1_files(self):\n        \n        print(\"adding previous applications...\")\n        # previous applications\n        ## gather all files and concat\n        appl_prev = pd.concat([self.read_in_file(file) for file in self.find_all_files_that_contain(\"applprev_1\")],axis=0)\n        ## only focus on the person for group 0 because that is the person we are making the decision on\n        ## additionally, only focus on the most recent application that existed prior to the case's decision date\n        ## it might be worth revisting this to look at all past applications because someone might be approved in a previous application but we are only going to\n        ## look at the most recent application\n        \n        appl_prev['creationdate_885D'] = pd.to_datetime(appl_prev['creationdate_885D'])\n        appl_prev = appl_prev.merge(self.df[['case_id','date_decision']],on='case_id')\n        appl_prev = appl_prev[  (appl_prev['creationdate_885D'] < appl_prev['date_decision']) \n                              & (appl_prev['num_group1'] == 0)].sort_values('creationdate_885D',ascending=False).drop_duplicates(subset=['case_id'])\n        ## convert the date columns to days since\n        for dt_col in [\"creationdate_885D\",\"approvaldate_319D\",\"dateactivated_425D\",\"employedfrom_700D\",\"firstnonzeroinstldate_307D\",\"dtlastpmt_581D\",\"dtlastpmtallstes_3545839D\"]:\n            new_col = f\"days_since_{dt_col}\"\n            dt_feat_series = (appl_prev['date_decision'] - pd.to_datetime(appl_prev[dt_col]) ).dt.days\n            appl_prev[new_col] = dt_feat_series\n            del appl_prev[dt_col]\n        ## change the names so it is clear that this is the most recent application\n        del appl_prev['num_group1']\n        del appl_prev['date_decision']\n        appl_prev.columns = ['case_id'] + [f\"{x}_MOST_RECENT_APPLICATION\" for x in appl_prev.columns[1:]]\n        ## add previous applications to the dataframe\n        self.features.extend(appl_prev.columns.tolist()[1:])\n        self.add_df_to_dataset(appl_prev)\n        ## free up memory\n        del appl_prev\n        \n        print(\"adding other...\")        \n        # other file\n        ## going to keep it real simple, just grab the first record for the person who we are making decision on\n        other = pd.concat([self.read_in_file(file) for file in self.find_all_files_that_contain(\"other_1\")],axis=0)\n        other = other[other['num_group1']==0].drop_duplicates(subset=['case_id'])\n        ## add other to the dataframe\n        self.features.extend(other.columns.tolist()[1:])\n        self.add_df_to_dataset(other)        \n        ## free up memory\n        del other       \n        \n        print(\"adding tax registry a...\")             \n        # tax registry a, look at individual and across all groups\n        tra = pd.concat([self.read_in_file(file) for file in self.find_all_files_that_contain(\"tax_registry_a\")],axis=0)\n        tra = tra.merge(self.df[['case_id','date_decision']],on='case_id')\n        tra['recorddate_4527225D'] = pd.to_datetime(tra['recorddate_4527225D'])\n        tra = tra[tra['recorddate_4527225D']<tra['date_decision']]\n        ## individual\n        individual_tra = tra[tra['num_group1']==0].drop(columns='num_group1')\n        for dt_col in [\"recorddate_4527225D\"]:\n            new_col = f\"days_since_{dt_col}\"\n            dt_feat_series = (individual_tra['date_decision'] - pd.to_datetime(individual_tra[dt_col]) ).dt.days\n            individual_tra[new_col] = dt_feat_series\n            del individual_tra[dt_col]\n        del individual_tra['date_decision']\n        individual_tra.columns = ['case_id'] + [f\"{x}_TRA_INDIV\" for x in individual_tra.columns[1:]]\n        ## add individual_tra to the dataframe\n        self.features.extend(individual_tra.columns.tolist()[1:])\n        self.add_df_to_dataset(individual_tra)        \n        ## free up memory\n        del individual_tra   \n        ## agg\n        agg_tra = tra.groupby('case_id',as_index=False).agg(\n            amount_4527230A_MIN = ('amount_4527230A','min'),\n            amount_4527230A_MAX = ('amount_4527230A','max'),\n            amount_4527230A_STD = ('amount_4527230A','std'),\n            amount_4527230A_AVG = ('amount_4527230A','mean'),\n            amount_4527230A_MEDIAN = ('amount_4527230A','median'),\n            amount_4527230A_SUM = ('amount_4527230A','sum'),    \n        )\n        self.features.extend(agg_tra.columns.tolist()[1:])\n        self.add_df_to_dataset(agg_tra)        \n        ## free up memory\n        del agg_tra        \n        \n        \n        print(\"adding tax registry b...\") \n        # tax registry b, look at individual and across all groups\n        trb = pd.concat([self.read_in_file(file) for file in self.find_all_files_that_contain(\"tax_registry_b\")],axis=0)\n        trb = trb.merge(self.df[['case_id','date_decision']],on='case_id')\n        trb['deductiondate_4917603D'] = pd.to_datetime(trb['deductiondate_4917603D'])\n        trb = trb[trb['deductiondate_4917603D']<trb['date_decision']]\n        ## individual\n        individual_trb = trb[trb['num_group1']==0].drop(columns='num_group1')\n        for dt_col in [\"deductiondate_4917603D\"]:\n            new_col = f\"days_since_{dt_col}\"\n            dt_feat_series = (individual_trb['date_decision'] - pd.to_datetime(individual_trb[dt_col]) ).dt.days\n            individual_trb[new_col] = dt_feat_series\n            del individual_trb[dt_col]\n        del individual_trb['date_decision']\n        individual_trb.columns = ['case_id'] + [f\"{x}_TRB_INDIV\" for x in individual_trb.columns[1:]]\n        ## add individual_trb to the dataframe\n        self.features.extend(individual_trb.columns.tolist()[1:])\n        self.add_df_to_dataset(individual_trb)        \n        ## free up memory\n        del individual_trb   \n        ## agg\n        agg_trb = trb.groupby('case_id',as_index=False).agg(\n            amount_4917619A_MIN = ('amount_4917619A','min'),\n            amount_4917619A_MAX = ('amount_4917619A','max'),\n            amount_4917619A_STD = ('amount_4917619A','std'),\n            amount_4917619A_AVG = ('amount_4917619A','mean'),\n            amount_4917619A_MEDIAN = ('amount_4917619A','median'),\n            amount_4917619A_SUM = ('amount_4917619A','sum'),    \n        )\n        self.features.extend(agg_trb.columns.tolist()[1:])\n        self.add_df_to_dataset(agg_trb)        \n        ## free up memory\n        del agg_trb              \n        \n        print(\"adding tax registry c...\")        \n        # tax registry c, look at individual and across all groups\n        trc = pd.concat([self.read_in_file(file) for file in self.find_all_files_that_contain(\"tax_registry_c\")],axis=0)\n        trc = trc.merge(self.df[['case_id','date_decision']],on='case_id')\n        trc['processingdate_168D'] = pd.to_datetime(trc['processingdate_168D'])\n        trc = trc[trc['processingdate_168D']<trc['date_decision']]\n        ## individual\n        individual_trc = trc[trc['num_group1']==0].drop(columns='num_group1')\n        for dt_col in [\"processingdate_168D\"]:\n            new_col = f\"days_since_{dt_col}\"\n            dt_feat_series = (individual_trc['date_decision'] - pd.to_datetime(individual_trc[dt_col]) ).dt.days\n            individual_trc[new_col] = dt_feat_series\n            del individual_trc[dt_col]\n        del individual_trc['date_decision']\n        individual_trc.columns = ['case_id'] + [f\"{x}_TRC_INDIV\" for x in individual_trc.columns[1:]]\n        ## add individual_trc to the dataframe\n        self.features.extend(individual_trc.columns.tolist()[1:])\n        self.add_df_to_dataset(individual_trc)        \n        ## free up memory\n        del individual_trc   \n        ## agg\n        agg_trc = trc.groupby('case_id',as_index=False).agg(\n            pmtamount_36A_MIN = ('pmtamount_36A','min'),\n            pmtamount_36A_MAX = ('pmtamount_36A','max'),\n            pmtamount_36A_STD = ('pmtamount_36A','std'),\n            pmtamount_36A_AVG = ('pmtamount_36A','mean'),\n            pmtamount_36A_MEDIAN = ('pmtamount_36A','median'),\n            pmtamount_36A_SUM = ('pmtamount_36A','sum'),    \n        )\n        self.features.extend(agg_trc.columns.tolist()[1:])\n        self.add_df_to_dataset(agg_trc)        \n        ## free up memory\n        del agg_trc            \n        \n        \n        \n        \n        \n        \n#         level1_criterias = [,\"other_\",\n#                             \"tax_registry_a\",\"tax_registry_b\",\"tax_registry_c\",\n#                             \"credit_bureau_a_1\",\"credit_bureau_b_1\",\n#                             \"deposit_1\",\"person_1\"]\n\n            \n        \n    def run(self):\n        self.create_base_dataset()\n        \n        self.add_all_level0_files()\n        self.make_level0_features()\n        \n        self.process_level1_files()\n    \n    def get_modeling_dataset(self):\n        return self.df[self.base_cols + self.features]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:00:20.081624Z","iopub.execute_input":"2024-05-04T22:00:20.082248Z","iopub.status.idle":"2024-05-04T22:00:20.125031Z","shell.execute_reply.started":"2024-05-04T22:00:20.082199Z","shell.execute_reply":"2024-05-04T22:00:20.123671Z"},"trusted":true},"execution_count":188,"outputs":[]},{"cell_type":"code","source":"train_ds_builder = DatasetBuilder(n_samples = 10)\ntrain_ds = train_ds_builder.get_modeling_dataset()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:00:21.061628Z","iopub.execute_input":"2024-05-04T22:00:21.062077Z","iopub.status.idle":"2024-05-04T22:01:51.295293Z","shell.execute_reply.started":"2024-05-04T22:00:21.062038Z","shell.execute_reply":"2024-05-04T22:01:51.294143Z"},"trusted":true},"execution_count":189,"outputs":[{"name":"stdout","text":"adding train_static_0_0.csv...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_42/4055596497.py:36: DtypeWarning: Columns (20,45,46,53,57,84,143,146,167) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"adding train_static_0_1.csv...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_42/4055596497.py:36: DtypeWarning: Columns (20,45,46,56,57,84,143,146,167) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"adding train_static_cb_0.csv...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_42/4055596497.py:36: DtypeWarning: Columns (1,2,3,4,7,45,46,47,48) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"making level0 features...\nadding previous applications...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_42/4055596497.py:36: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n/tmp/ipykernel_42/4055596497.py:36: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"{self.parent_path}/csv_files/{self.partition}/{file_name}\")\n","output_type":"stream"},{"name":"stdout","text":"adding other...\nadding tax registry a...\nadding tax registry b...\nadding tax registry c...\n","output_type":"stream"}]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_applprev_1_0.csv\")\n# test.head()\n\ntest ","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:41:40.621410Z","iopub.execute_input":"2024-05-04T21:41:40.621944Z","iopub.status.idle":"2024-05-04T21:42:04.894201Z","shell.execute_reply.started":"2024-05-04T21:41:40.621904Z","shell.execute_reply":"2024-05-04T21:42:04.893059Z"},"trusted":true},"execution_count":170,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_42/3430821530.py:1: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n  test = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_applprev_1_0.csv\")\n","output_type":"stream"},{"execution_count":170,"output_type":"execute_result","data":{"text/plain":"         case_id  actualdpd_943P  annuity_853A approvaldate_319D  \\\n0              2             0.0         640.2               NaN   \n1              2             0.0        1682.4               NaN   \n2              3             0.0        6140.0               NaN   \n3              4             0.0        2556.6               NaN   \n4              5             0.0           NaN               NaN   \n...          ...             ...           ...               ...   \n3887679  2651092             0.0        3000.0        2019-12-30   \n3887680  2651092             0.0        3119.2        2012-12-12   \n3887681  2651092             0.0        4366.0        2017-11-09   \n3887682  2651092             0.0        4496.6        2019-02-17   \n3887683  2651092             0.0           NaN        2014-02-15   \n\n         byoccupationinc_3656910L cancelreason_3545846M  childnum_21L  \\\n0                             NaN              a55475b1           0.0   \n1                             NaN              a55475b1           0.0   \n2                             NaN           P94_109_143           NaN   \n3                             NaN             P24_27_36           NaN   \n4                             NaN           P85_114_140           NaN   \n...                           ...                   ...           ...   \n3887679                       NaN              a55475b1           NaN   \n3887680                   25000.0              a55475b1           1.0   \n3887681                       NaN              a55475b1           NaN   \n3887682                       NaN              a55475b1           NaN   \n3887683                   60000.0              a55475b1           1.0   \n\n        creationdate_885D  credacc_actualbalance_314A  credacc_credlmt_575A  \\\n0              2013-04-03                         NaN                   0.0   \n1              2013-04-03                         NaN                   0.0   \n2              2019-01-07                         NaN                   0.0   \n3              2019-01-08                         NaN                   0.0   \n4              2019-01-16                         NaN                   NaN   \n...                   ...                         ...                   ...   \n3887679        2019-12-30                     53300.0               53300.0   \n3887680        2012-12-12                         NaN                   0.0   \n3887681        2017-11-09                         NaN                   0.0   \n3887682        2019-02-17                         NaN                   0.0   \n3887683        2014-02-15                         NaN               53300.0   \n\n         credacc_maxhisbal_375A  credacc_minhisbal_90A credacc_status_367L  \\\n0                           NaN                    NaN                 NaN   \n1                           NaN                    NaN                 NaN   \n2                           NaN                    NaN                 NaN   \n3                           NaN                    NaN                 NaN   \n4                           NaN                    NaN                 NaN   \n...                         ...                    ...                 ...   \n3887679                     0.0                    0.0                  CL   \n3887680                     NaN                    NaN                 NaN   \n3887681                     NaN                    NaN                 NaN   \n3887682                     NaN                    NaN                 NaN   \n3887683                     NaN                    NaN                 NaN   \n\n         credacc_transactions_402L  credamount_590A credtype_587L  \\\n0                              NaN          10000.0           CAL   \n1                              NaN          16000.0           CAL   \n2                              NaN          59999.8           CAL   \n3                              NaN          40000.0           CAL   \n4                              NaN              NaN           NaN   \n...                            ...              ...           ...   \n3887679                        0.0          53998.0           COL   \n3887680                        NaN          25740.0           COL   \n3887681                        NaN          19638.0           COL   \n3887682                        NaN          20840.0           COL   \n3887683                        NaN          53300.0           REL   \n\n         currdebt_94A dateactivated_425D district_544M  downpmt_134A  \\\n0                 NaN                NaN  P136_108_173           0.0   \n1                 NaN                NaN  P136_108_173           0.0   \n2                 NaN                NaN   P131_33_167           0.0   \n3                 NaN                NaN   P194_82_174           0.0   \n4                 NaN                NaN    P54_133_26           NaN   \n...               ...                ...           ...           ...   \n3887679       53998.0                NaN   P147_21_170           0.0   \n3887680           0.0         2012-12-20   P147_21_170           0.0   \n3887681           0.0         2017-11-15   P147_21_170           0.0   \n3887682           0.0         2019-02-18   P147_21_170           0.0   \n3887683           0.0         2014-02-17   P147_21_170           0.0   \n\n        dtlastpmt_581D dtlastpmtallstes_3545839D education_1138M  \\\n0                  NaN                       NaN      P97_36_170   \n1                  NaN                       NaN      P97_36_170   \n2                  NaN                       NaN      P97_36_170   \n3                  NaN                       NaN        a55475b1   \n4                  NaN                       NaN        a55475b1   \n...                ...                       ...             ...   \n3887679            NaN                       NaN        a55475b1   \n3887680            NaN                       NaN      P97_36_170   \n3887681     2018-04-03                2018-04-03        a55475b1   \n3887682     2019-07-31                2019-07-31        a55475b1   \n3887683            NaN                       NaN      P97_36_170   \n\n        employedfrom_700D     familystate_726L firstnonzeroinstldate_307D  \\\n0              2010-02-15               SINGLE                 2013-05-04   \n1              2010-02-15               SINGLE                 2013-05-04   \n2              2018-05-15              MARRIED                 2019-02-07   \n3                     NaN                  NaN                 2019-02-08   \n4                     NaN                  NaN                        NaN   \n...                   ...                  ...                        ...   \n3887679               NaN                  NaN                 2020-01-30   \n3887680        2004-10-15              MARRIED                 2013-01-11   \n3887681               NaN  LIVING_WITH_PARTNER                 2017-12-10   \n3887682               NaN                  NaN                 2019-03-17   \n3887683        2004-09-15  LIVING_WITH_PARTNER                 2014-03-11   \n\n        inittransactioncode_279L isbidproduct_390L isdebitcard_527L  \\\n0                           CASH             False              NaN   \n1                           CASH             False              NaN   \n2                           CASH             False              NaN   \n3                           CASH             False              NaN   \n4                            NaN             False              NaN   \n...                          ...               ...              ...   \n3887679                      POS             False              NaN   \n3887680                      POS             False              NaN   \n3887681                      POS             False              NaN   \n3887682                      POS             False              NaN   \n3887683                     CASH             False            False   \n\n         mainoccupationinc_437A  maxdpdtolerance_577P  num_group1  \\\n0                        8200.0                   NaN           0   \n1                        8200.0                   NaN           1   \n2                       11000.0                   NaN           0   \n3                       16000.0                   NaN           0   \n4                       62000.0                   NaN           0   \n...                         ...                   ...         ...   \n3887679                 60000.0                   NaN           0   \n3887680                 10000.0                   0.0           7   \n3887681                 40000.0                   0.0           4   \n3887682                 60000.0                   0.0           2   \n3887683                 15000.0                   0.0           6   \n\n         outstandingdebt_522A  pmtnum_8L postype_4733339M profession_152M  \\\n0                         NaN       24.0         a55475b1        a55475b1   \n1                         NaN       12.0         a55475b1        a55475b1   \n2                         NaN       12.0         a55475b1        a55475b1   \n3                         NaN       24.0         a55475b1        a55475b1   \n4                         NaN        NaN         a55475b1        a55475b1   \n...                       ...        ...              ...             ...   \n3887679               53998.0       18.0     P177_117_192        a55475b1   \n3887680                   0.0       12.0      P149_40_170        a55475b1   \n3887681                   0.0        5.0      P60_146_156        a55475b1   \n3887682                   0.0        6.0      P149_40_170        a55475b1   \n3887683                   0.0        NaN       P46_145_78        a55475b1   \n\n        rejectreason_755M rejectreasonclient_4145042M  revolvingaccount_394A  \\\n0                a55475b1                    a55475b1                    NaN   \n1                a55475b1                    a55475b1                    NaN   \n2             P94_109_143                    a55475b1                    NaN   \n3                a55475b1                    a55475b1                    NaN   \n4                a55475b1                    a55475b1                    NaN   \n...                   ...                         ...                    ...   \n3887679          a55475b1                    a55475b1                    NaN   \n3887680          a55475b1                    a55475b1                    NaN   \n3887681          a55475b1                    a55475b1                    NaN   \n3887682          a55475b1                    a55475b1                    NaN   \n3887683          a55475b1                    a55475b1            680531000.0   \n\n        status_219L  tenor_203L  \n0                 D        24.0  \n1                 D        12.0  \n2                 D        12.0  \n3                 T        24.0  \n4                 T         NaN  \n...             ...         ...  \n3887679           N        18.0  \n3887680           K        12.0  \n3887681           K         5.0  \n3887682           K         6.0  \n3887683           K         NaN  \n\n[3887684 rows x 41 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_id</th>\n      <th>actualdpd_943P</th>\n      <th>annuity_853A</th>\n      <th>approvaldate_319D</th>\n      <th>byoccupationinc_3656910L</th>\n      <th>cancelreason_3545846M</th>\n      <th>childnum_21L</th>\n      <th>creationdate_885D</th>\n      <th>credacc_actualbalance_314A</th>\n      <th>credacc_credlmt_575A</th>\n      <th>credacc_maxhisbal_375A</th>\n      <th>credacc_minhisbal_90A</th>\n      <th>credacc_status_367L</th>\n      <th>credacc_transactions_402L</th>\n      <th>credamount_590A</th>\n      <th>credtype_587L</th>\n      <th>currdebt_94A</th>\n      <th>dateactivated_425D</th>\n      <th>district_544M</th>\n      <th>downpmt_134A</th>\n      <th>dtlastpmt_581D</th>\n      <th>dtlastpmtallstes_3545839D</th>\n      <th>education_1138M</th>\n      <th>employedfrom_700D</th>\n      <th>familystate_726L</th>\n      <th>firstnonzeroinstldate_307D</th>\n      <th>inittransactioncode_279L</th>\n      <th>isbidproduct_390L</th>\n      <th>isdebitcard_527L</th>\n      <th>mainoccupationinc_437A</th>\n      <th>maxdpdtolerance_577P</th>\n      <th>num_group1</th>\n      <th>outstandingdebt_522A</th>\n      <th>pmtnum_8L</th>\n      <th>postype_4733339M</th>\n      <th>profession_152M</th>\n      <th>rejectreason_755M</th>\n      <th>rejectreasonclient_4145042M</th>\n      <th>revolvingaccount_394A</th>\n      <th>status_219L</th>\n      <th>tenor_203L</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>640.2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>a55475b1</td>\n      <td>0.0</td>\n      <td>2013-04-03</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10000.0</td>\n      <td>CAL</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P136_108_173</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P97_36_170</td>\n      <td>2010-02-15</td>\n      <td>SINGLE</td>\n      <td>2013-05-04</td>\n      <td>CASH</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>8200.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>24.0</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>D</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1682.4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>a55475b1</td>\n      <td>0.0</td>\n      <td>2013-04-03</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16000.0</td>\n      <td>CAL</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P136_108_173</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P97_36_170</td>\n      <td>2010-02-15</td>\n      <td>SINGLE</td>\n      <td>2013-05-04</td>\n      <td>CASH</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>8200.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>12.0</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>D</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>6140.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P94_109_143</td>\n      <td>NaN</td>\n      <td>2019-01-07</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>59999.8</td>\n      <td>CAL</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P131_33_167</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P97_36_170</td>\n      <td>2018-05-15</td>\n      <td>MARRIED</td>\n      <td>2019-02-07</td>\n      <td>CASH</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>11000.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>12.0</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>P94_109_143</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>D</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.0</td>\n      <td>2556.6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P24_27_36</td>\n      <td>NaN</td>\n      <td>2019-01-08</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>40000.0</td>\n      <td>CAL</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P194_82_174</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2019-02-08</td>\n      <td>CASH</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>16000.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>24.0</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>T</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P85_114_140</td>\n      <td>NaN</td>\n      <td>2019-01-16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P54_133_26</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>62000.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>T</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3887679</th>\n      <td>2651092</td>\n      <td>0.0</td>\n      <td>3000.0</td>\n      <td>2019-12-30</td>\n      <td>NaN</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>2019-12-30</td>\n      <td>53300.0</td>\n      <td>53300.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>CL</td>\n      <td>0.0</td>\n      <td>53998.0</td>\n      <td>COL</td>\n      <td>53998.0</td>\n      <td>NaN</td>\n      <td>P147_21_170</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2020-01-30</td>\n      <td>POS</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>60000.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>53998.0</td>\n      <td>18.0</td>\n      <td>P177_117_192</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>N</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>3887680</th>\n      <td>2651092</td>\n      <td>0.0</td>\n      <td>3119.2</td>\n      <td>2012-12-12</td>\n      <td>25000.0</td>\n      <td>a55475b1</td>\n      <td>1.0</td>\n      <td>2012-12-12</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>25740.0</td>\n      <td>COL</td>\n      <td>0.0</td>\n      <td>2012-12-20</td>\n      <td>P147_21_170</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P97_36_170</td>\n      <td>2004-10-15</td>\n      <td>MARRIED</td>\n      <td>2013-01-11</td>\n      <td>POS</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>10000.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>P149_40_170</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>K</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>3887681</th>\n      <td>2651092</td>\n      <td>0.0</td>\n      <td>4366.0</td>\n      <td>2017-11-09</td>\n      <td>NaN</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>2017-11-09</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>19638.0</td>\n      <td>COL</td>\n      <td>0.0</td>\n      <td>2017-11-15</td>\n      <td>P147_21_170</td>\n      <td>0.0</td>\n      <td>2018-04-03</td>\n      <td>2018-04-03</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>LIVING_WITH_PARTNER</td>\n      <td>2017-12-10</td>\n      <td>POS</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>40000.0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>P60_146_156</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>K</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3887682</th>\n      <td>2651092</td>\n      <td>0.0</td>\n      <td>4496.6</td>\n      <td>2019-02-17</td>\n      <td>NaN</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>2019-02-17</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20840.0</td>\n      <td>COL</td>\n      <td>0.0</td>\n      <td>2019-02-18</td>\n      <td>P147_21_170</td>\n      <td>0.0</td>\n      <td>2019-07-31</td>\n      <td>2019-07-31</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2019-03-17</td>\n      <td>POS</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>60000.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>P149_40_170</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>NaN</td>\n      <td>K</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>3887683</th>\n      <td>2651092</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>2014-02-15</td>\n      <td>60000.0</td>\n      <td>a55475b1</td>\n      <td>1.0</td>\n      <td>2014-02-15</td>\n      <td>NaN</td>\n      <td>53300.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>53300.0</td>\n      <td>REL</td>\n      <td>0.0</td>\n      <td>2014-02-17</td>\n      <td>P147_21_170</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>P97_36_170</td>\n      <td>2004-09-15</td>\n      <td>LIVING_WITH_PARTNER</td>\n      <td>2014-03-11</td>\n      <td>CASH</td>\n      <td>False</td>\n      <td>False</td>\n      <td>15000.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>P46_145_78</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>a55475b1</td>\n      <td>680531000.0</td>\n      <td>K</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>3887684 rows Ã— 41 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"[\n    \n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training LightGBM\n\nMinimal example of LightGBM training is shown below.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:02:52.514496Z","iopub.execute_input":"2024-05-04T22:02:52.515695Z","iopub.status.idle":"2024-05-04T22:02:52.520499Z","shell.execute_reply.started":"2024-05-04T22:02:52.515632Z","shell.execute_reply":"2024-05-04T22:02:52.519505Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"train_ds_builder = DatasetBuilder(partition=\"train\")\ntrain_ds = train_ds_builder.get_modeling_dataset()\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_ds[train_ds_builder.features],train_ds['target'],stratify=train_ds['target'],train_size=.8)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:17:30.168558Z","iopub.execute_input":"2024-05-04T22:17:30.169069Z","iopub.status.idle":"2024-05-04T22:17:30.174720Z","shell.execute_reply.started":"2024-05-04T22:17:30.169029Z","shell.execute_reply":"2024-05-04T22:17:30.173049Z"},"trusted":true},"execution_count":207,"outputs":[]},{"cell_type":"code","source":"def calculate_woe_iv_categorical(data, feature, target):\n    # Create a DataFrame for counts per category\n    df = data.groupby([feature])[target].agg(['count', 'sum'])\n    df.columns = ['Total', 'Bad']\n    \n    # Calculate the number of good outcomes\n    df['Good'] = df['Total'] - df['Bad']\n    \n    # Handle cases where the count is 0 to avoid division by zero in WoE calculation\n    df['Bad'] = np.where(df['Bad'] == 0, 0.0001, df['Bad'])\n    df['Good'] = np.where(df['Good'] == 0, 0.0001, df['Good'])\n    \n    # Calculate the percentage of bads and goods\n    df['Distr_Bad'] = df['Bad'] / df['Bad'].sum()\n    df['Distr_Good'] = df['Good'] / df['Good'].sum()\n    \n    # Calculate WoE\n    df['WoE'] = np.log(df['Distr_Good'] / df['Distr_Bad'])\n    \n    # Calculate IV\n    df['IV'] = (df['Distr_Good'] - df['Distr_Bad']) * df['WoE']\n    \n    # Sum the IV values for the feature\n    IV = df['IV'].sum()\n    \n    # Prepare a report\n    report = df.reset_index()[[feature, 'WoE', 'IV']]\n    \n    return IV, report\n\ndef calculate_woe_iv_numeric(data, feature, target, bins=10):\n    # Bin the data\n    data['binned'] = pd.qcut(data[feature], q=bins, duplicates='drop')\n\n    # Group by the binned feature\n    grouped = data.groupby('binned')[target].agg(['count', 'sum'])\n    grouped.columns = ['Total', 'Bad']\n\n    # Calculate the number of good outcomes\n    grouped['Good'] = grouped['Total'] - grouped['Bad']\n    \n    # Handle cases where the count is 0 to avoid division by zero in WoE calculation\n    grouped['Bad'] = np.where(grouped['Bad'] == 0, 0.0001, grouped['Bad'])\n    grouped['Good'] = np.where(grouped['Good'] == 0, 0.0001, grouped['Good'])\n\n    # Calculate the distribution of bads and goods\n    grouped['Distr_Bad'] = grouped['Bad'] / grouped['Bad'].sum()\n    grouped['Distr_Good'] = grouped['Good'] / grouped['Good'].sum()\n\n    # Calculate WoE\n    grouped['WoE'] = np.log(grouped['Distr_Good'] / grouped['Distr_Bad'])\n\n    # Calculate IV\n    grouped['IV'] = (grouped['Distr_Good'] - grouped['Distr_Bad']) * grouped['WoE']\n\n    # Sum the IV values for the feature\n    IV = grouped['IV'].sum()\n\n    # Prepare a report\n    report = grouped.reset_index()[['binned', 'WoE', 'IV']]\n\n    return IV, report","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:13:53.161074Z","iopub.execute_input":"2024-05-04T22:13:53.161523Z","iopub.status.idle":"2024-05-04T22:13:53.179876Z","shell.execute_reply.started":"2024-05-04T22:13:53.161481Z","shell.execute_reply":"2024-05-04T22:13:53.178572Z"},"trusted":true},"execution_count":201,"outputs":[]},{"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n\nparams = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 3,\n    \"num_leaves\": 31,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.9,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"n_estimators\": 1000,\n    \"verbose\": -1,\n}\n\ngbm = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=lgb_valid,\n    callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:10:22.646113Z","iopub.execute_input":"2024-05-04T22:10:22.646558Z","iopub.status.idle":"2024-05-04T22:13:37.448967Z","shell.execute_reply.started":"2024-05-04T22:10:22.646519Z","shell.execute_reply":"2024-05-04T22:13:37.447881Z"},"trusted":true},"execution_count":200,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 10 rounds\n[50]\tvalid_0's auc: 0.77198\n[100]\tvalid_0's auc: 0.787867\n[150]\tvalid_0's auc: 0.794406\n[200]\tvalid_0's auc: 0.797841\n[250]\tvalid_0's auc: 0.800297\n[300]\tvalid_0's auc: 0.801641\n[350]\tvalid_0's auc: 0.802647\n[400]\tvalid_0's auc: 0.803571\n[450]\tvalid_0's auc: 0.804014\n[500]\tvalid_0's auc: 0.804618\n[550]\tvalid_0's auc: 0.805147\n[600]\tvalid_0's auc: 0.805546\nEarly stopping, best iteration is:\n[608]\tvalid_0's auc: 0.805619\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Evaluation with AUC and then comparison with the stability metric is shown below.","metadata":{}},{"cell_type":"code","source":"test_ds_builder = DatasetBuilder(partition=\"test\")\ntest_ds = test_ds_builder.get_modeling_dataset()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:18:11.284005Z","iopub.execute_input":"2024-05-04T22:18:11.284453Z","iopub.status.idle":"2024-05-04T22:18:11.611980Z","shell.execute_reply.started":"2024-05-04T22:18:11.284418Z","shell.execute_reply":"2024-05-04T22:18:11.610416Z"},"trusted":true},"execution_count":208,"outputs":[{"name":"stdout","text":"adding test_static_0_0.csv...\nadding test_static_0_2.csv...\nadding test_static_0_1.csv...\nadding test_static_cb_0.csv...\nmaking level0 features...\nadding previous applications...\nadding other...\nadding tax registry a...\nadding tax registry b...\nadding tax registry c...\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.read_csv('/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_base.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:19:27.639521Z","iopub.execute_input":"2024-05-04T22:19:27.640134Z","iopub.status.idle":"2024-05-04T22:19:27.659019Z","shell.execute_reply.started":"2024-05-04T22:19:27.640090Z","shell.execute_reply":"2024-05-04T22:19:27.658049Z"},"trusted":true},"execution_count":209,"outputs":[{"execution_count":209,"output_type":"execute_result","data":{"text/plain":"   case_id date_decision   MONTH  WEEK_NUM\n0    57543    2021-05-14  202201       100\n1    57549    2022-01-17  202201       100\n2    57551    2020-11-27  202201       100\n3    57552    2020-11-27  202201       100\n4    57569    2021-12-20  202201       100\n5    57630    2021-03-16  202201       100\n6    57631    2022-06-04  202201       100\n7    57632    2022-02-05  202201       100\n8    57633    2022-01-25  202201       100\n9    57634    2021-01-27  202201       100","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_id</th>\n      <th>date_decision</th>\n      <th>MONTH</th>\n      <th>WEEK_NUM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>57543</td>\n      <td>2021-05-14</td>\n      <td>202201</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>57549</td>\n      <td>2022-01-17</td>\n      <td>202201</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57551</td>\n      <td>2020-11-27</td>\n      <td>202201</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>57552</td>\n      <td>2020-11-27</td>\n      <td>202201</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57569</td>\n      <td>2021-12-20</td>\n      <td>202201</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>57630</td>\n      <td>2021-03-16</td>\n      <td>202201</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>57631</td>\n      <td>2022-06-04</td>\n      <td>202201</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>57632</td>\n      <td>2022-02-05</td>\n      <td>202201</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>57633</td>\n      <td>2022-01-25</td>\n      <td>202201</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>57634</td>\n      <td>2021-01-27</td>\n      <td>202201</td>\n      <td>100</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n    y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n    base[\"score\"] = y_pred\n\nprint(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}') \nprint(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}') ","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:47:48.919371Z","iopub.status.idle":"2024-05-04T21:47:48.919811Z","shell.execute_reply.started":"2024-05-04T21:47:48.919590Z","shell.execute_reply":"2024-05-04T21:47:48.919610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n\nstability_score_train = gini_stability(base_train)\nstability_score_valid = gini_stability(base_valid)\nstability_score_test = gini_stability(base_test)\n\nprint(f'The stability score on the train set is: {stability_score_train}') \nprint(f'The stability score on the valid set is: {stability_score_valid}') \nprint(f'The stability score on the test set is: {stability_score_test}')  ","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:47:48.921645Z","iopub.status.idle":"2024-05-04T21:47:48.922060Z","shell.execute_reply.started":"2024-05-04T21:47:48.921875Z","shell.execute_reply":"2024-05-04T21:47:48.921895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\n\nScoring the submission dataset is below, we need to take care of new categories. Then we save the score as a last step. ","metadata":{}},{"cell_type":"code","source":"X_submission = data_submission[cols_pred].to_pandas()\nX_submission = convert_strings(X_submission)\ncategorical_cols = X_train.select_dtypes(include=['category']).columns\n\nfor col in categorical_cols:\n    train_categories = set(X_train[col].cat.categories)\n    submission_categories = set(X_submission[col].cat.categories)\n    new_categories = submission_categories - train_categories\n    X_submission.loc[X_submission[col].isin(new_categories), col] = \"Unknown\"\n    new_dtype = pd.CategoricalDtype(categories=train_categories, ordered=True)\n    X_train[col] = X_train[col].astype(new_dtype)\n    X_submission[col] = X_submission[col].astype(new_dtype)\n\ny_submission_pred = gbm.predict(X_submission, num_iteration=gbm.best_iteration)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T21:27:23.798139Z","iopub.execute_input":"2024-02-07T21:27:23.798639Z","iopub.status.idle":"2024-02-07T21:27:23.946242Z","shell.execute_reply.started":"2024-02-07T21:27:23.798595Z","shell.execute_reply":"2024-02-07T21:27:23.944996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"case_id\": data_submission[\"case_id\"].to_numpy(),\n    \"score\": y_submission_pred\n}).set_index('case_id')\nsubmission.to_csv(\"./submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-07T21:27:23.947771Z","iopub.execute_input":"2024-02-07T21:27:23.948164Z","iopub.status.idle":"2024-02-07T21:27:23.96104Z","shell.execute_reply.started":"2024-02-07T21:27:23.948128Z","shell.execute_reply":"2024-02-07T21:27:23.959969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best of luck, and most importantly, enjoy the process of learning and discovery! \n\n<img src=\"https://i.imgur.com/obVWIBh.png\" alt=\"Image\" width=\"700\"/>","metadata":{}}]}